{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0hK4nU35JVaA",
        "N4dDTs-rJL_h",
        "TycHgKTZJ6a6"
      ],
      "authorship_tag": "ABX9TyO8aivXcDQYfvBMGRI9QOaP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/nlp-research-and-practice/blob/main/text-similarity-works/icd10-code-highlighting/13_2_icd_10_code_keyword_and_impairment_highlighting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "u6vazPC0Ja4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "!pip install pillow\n",
        "\n",
        "!sudo apt install build-essential libpoppler-cpp-dev pkg-config python3-dev\n",
        "!pip install -U pdftotext\n",
        "!pip install PyPDF2\n",
        "!pip install fitz\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "gdwgVvXuJdz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "id": "4QPcnYT5AKpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "YyREdYtyqNMK",
        "outputId": "afe4f8cd-b474-454f-a66d-80886094833a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/csv_files/\n",
            "  inflating: data/csv_files/icd_10_code_and_keywords_v2.csv  \n",
            "  inflating: data/csv_files/icd_10_code_and_keywords_v3.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v1.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v2.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v3.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v4.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v5.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v6.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v7.csv  \n",
            "   creating: data/pattern_files/\n",
            "  inflating: data/pattern_files/date_regex-v0.json  \n",
            "  inflating: data/pattern_files/date_regex-v1.json  \n",
            "  inflating: data/pattern_files/icd10_code_patterns-v6.jsonl  \n",
            "   creating: data/pkl_files/\n",
            "  inflating: data/pkl_files/dict_words.pkl  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v0.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v1.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v2.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v3.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v4.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v5.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v6.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v7.pickle  \n",
            "   creating: data/txt_files/\n",
            "  inflating: data/txt_files/kcaps_v1.txt  \n",
            "  inflating: data/txt_files/kcaps_v2.txt  \n",
            "  inflating: data/txt_files/kcaps_v3.txt  \n",
            "  inflating: data/txt_files/kcaps_v4.txt  \n",
            "  inflating: data/txt_files/kcaps_v5.txt  \n",
            "  inflating: data/txt_files/kcaps_v6.txt  \n",
            "  inflating: data/txt_files/kcaps_v7.txt  \n",
            "  inflating: data/txt_files/knocaps_v0.txt  \n",
            "  inflating: data/txt_files/knocaps_v1.txt  \n",
            "  inflating: data/txt_files/knocaps_v2.txt  \n",
            "  inflating: data/txt_files/knocaps_v3.txt  \n",
            "  inflating: data/txt_files/knocaps_v4.txt  \n",
            "  inflating: data/txt_files/knocaps_v5.txt  \n",
            "  inflating: data/txt_files/knocaps_v6.txt  \n",
            "  inflating: data/txt_files/knocaps_v7.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import"
      ],
      "metadata": {
        "id": "L0vdWKmkdeL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import fitz\n",
        "\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from array import *\n",
        "\n",
        "from concurrent import futures\n",
        "from keyword_matcher import KeywordMatcher\n",
        "from sentence_extractor import SentenceExtractor\n",
        "from generic_matcher import GenericMatcher\n",
        "import config as cfg\n",
        "from utils import *"
      ],
      "metadata": {
        "id": "gKbd7WsfyYX3",
        "outputId": "814e2fc6-0c2e-4671-f884-04f6ee9c22d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Imv6fB5gPBe2",
        "outputId": "2c79e6da-e2d7-488a-903a-19a5a64505a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Core Classes"
      ],
      "metadata": {
        "id": "9oNqjgIsRpNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matches = len(re.findall(r\"^\\d+$\", \"/2020\".strip())) > 0\n",
        "matches"
      ],
      "metadata": {
        "id": "q8KzT6qB2Rum",
        "outputId": "6f93b846-403f-4cd4-c66b-b66f77ce525f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(238/255, 210/255, 2/255)"
      ],
      "metadata": {
        "id": "aQJui1vVhbad",
        "outputId": "ad293c6c-6e5d-4f34-f0d2-39fa9e4bb4e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9333333333333333, 0.8235294117647058, 0.00784313725490196)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Highlighter:\n",
        "    def __init__(self, match_threshold=30):\n",
        "        # loading and updating patterns for ICD-10 code\n",
        "        self.nlp_code10 = English()\n",
        "        self.nlp_code10.add_pipe(\"entity_ruler\").from_disk(cfg.pattern_files[\"jsonl\"])\n",
        "\n",
        "        self.match_threshold = match_threshold\n",
        "        self.text_list = None\n",
        "\n",
        "        # define icd-10 code dataset\n",
        "        core_df = pd.read_csv(cfg.csv_files[\"CODE_CSV\"])\n",
        "        self.code_df = core_df.where(pd.notnull(core_df), None)\n",
        "        self.synid_df = pd.read_csv(cfg.csv_files[\"IMP_CSV\"])\n",
        "\n",
        "        # define required directory path\n",
        "        self.PDF_FILES_PATH = cfg.file_path[\"TMP_PDF_FILES_PATH\"]\n",
        "        self.TXT_FILES_PATH = cfg.file_path[\"TMP_TXT_FILES_PATH\"]\n",
        "        self.OUTPUT_FILES_PATH = cfg.file_path[\"OUTPUT_PATH\"]\n",
        "\n",
        "    def set_impairment_keyword_dict(self, imp_keyword_dict):\n",
        "        self.impairment_keyword_dict = imp_keyword_dict\n",
        "\n",
        "    def set_page_matched_date_dict(self, matched_date_dict):\n",
        "        self.page_matched_date_dict = matched_date_dict\n",
        "\n",
        "    def set_match_threshold(self, match_val):\n",
        "        self.match_threshold = match_val\n",
        "\n",
        "    def set_text_list(self, p_text_list):\n",
        "        self.text_list = p_text_list\n",
        "\n",
        "    def highlight_all(self, icd10_code_dict, pdf_file_name=None, highlight_all=False):\n",
        "        pdf_file = fitz.open(pdf_file_name)\n",
        "        json_data = []\n",
        "        json_imp_data = []\n",
        "        json_date_data = []\n",
        "\n",
        "        for page_num, page in enumerate(pdf_file):\n",
        "            already_done_page_list = []\n",
        "            self.code_y2_coords_list = []\n",
        "            self.already_found_word_list = []\n",
        "            self.highlighted_coord_list = []\n",
        "            code_highlight_dict = {}\n",
        "\n",
        "            # highlight ICD-10 code\n",
        "            if page_num in icd10_code_dict:\n",
        "                num_page = page_num + 1\n",
        "\n",
        "                # let's store code y2 coords\n",
        "                for code in icd10_code_dict[page_num]:\n",
        "                    highlight_list = page.search_for(code)\n",
        "                    #print(f\"Page-{num_page}-{code}, Coordinate: {highlight_list}\")\n",
        "                    if len(highlight_list) > 0:\n",
        "                        code_highlight_dict[code] = highlight_list\n",
        "                        self.code_y2_coords_list.extend([highlight[3] for highlight in highlight_list])\n",
        "                    else:\n",
        "                        for alt_code in get_alternate_code_pattern(code):\n",
        "                            highlight_list = page.search_for(alt_code)\n",
        "                            for highlight in highlight_list:\n",
        "                                if len(highlight) > 0:\n",
        "                                    code_highlight_dict[code] = [highlight]\n",
        "                                    # print(f\"alternate-code:{code}, Coordinate: {highlight}\")\n",
        "                                    self.code_y2_coords_list.extend([highlight[3]])\n",
        "                                    # print(f\"[highlight[3]]: {[highlight[3]]}\")\n",
        "\n",
        "                filtered_code_highlight_dict = filter_duplicate_coordinate(code_highlight_dict)\n",
        "                # now, let highlight every code and its common words\n",
        "                for code, highlight_list in filtered_code_highlight_dict.items():\n",
        "                    prev_highlight = fitz.Rect(0, 0, 0, 0)\n",
        "                    for highlight in highlight_list:\n",
        "                        keyword = get_keyword(code, self.code_df)\n",
        "                        # get match score and common words coordinate\n",
        "                        if not highlight_all:\n",
        "                          all_match_keyword_list, all_match_imp_keyword_list = self.get_best_token_match(page_num,\n",
        "                                                                                                        code,\n",
        "                                                                                                        page,\n",
        "                                                                                                        highlight[3],\n",
        "                                                                                                        self.match_threshold)\n",
        "                        else:\n",
        "                          all_match_keyword_list, all_match_imp_keyword_list = self.get_best_all_token_match(page_num,\n",
        "                                                                                                        code,\n",
        "                                                                                                        page,\n",
        "                                                                                                        highlight[3],\n",
        "                                                                                                        self.match_threshold)\n",
        "\n",
        "                        # Step-1: highlight and set color coding dont have common words\n",
        "                        if not all_match_keyword_list:\n",
        "                            highlight_code(page, highlight)\n",
        "                            # prepare json object\n",
        "                            json_data.append(prepare_json_object(num_page, code, self.code_df, highlight, keyword))\n",
        "\n",
        "                        # Step-2: highlight and set color coding that have common words\n",
        "                        for match_keyword_dict in all_match_keyword_list:\n",
        "                            # highlight ICD-10 code\n",
        "                            if not is_coord_equal(prev_highlight, highlight):\n",
        "                              highlight_code(page, highlight)\n",
        "                              prev_highlight = highlight\n",
        "                            # highlight common words\n",
        "                            highlight_coords_dict = {}\n",
        "                            highlight_coords_dict, highlighted_word_list = self.highlight_common_words(page, match_keyword_dict[\"common_words_coords\"])\n",
        "                            # prepare json object\n",
        "                            json_data.append(prepare_json_object(num_page, code, self.code_df, highlight, keyword, match_keyword_dict, highlight_coords_dict))\n",
        "\n",
        "                        # Step-3: highlight and set color coding for impairment keyword common words\n",
        "                        json_imp_data_list = []\n",
        "                        for match_imp_keyword_dict in all_match_imp_keyword_list:\n",
        "                            json_imp_data_object = {}\n",
        "                            # highlight common words\n",
        "                            highlight_coords_dict = {}\n",
        "                            if \"imp_common_keyword_coords\" in match_imp_keyword_dict:\n",
        "                              if match_imp_keyword_dict[\"imp_common_keyword_coords\"]:\n",
        "                                highlight_coords_list = self.highlight_impairment(page, num_page, match_imp_keyword_dict[\"imp_common_keyword_coords\"])\n",
        "                                already_done_page_list.append(page_num)\n",
        "                                # prepare json object\n",
        "                                if highlight_coords_list:\n",
        "                                  json_imp_data_list.extend(highlight_coords_list)\n",
        "                        if len(json_imp_data_list) > 0:\n",
        "                            json_imp_data.extend(json_imp_data_list)\n",
        "\n",
        "            # highlight impairment key phrase\n",
        "            if page_num in self.impairment_keyword_dict and page_num not in already_done_page_list:\n",
        "              json_imp_data_list = self.highlight_keyword_impairment_data(page, page_num)\n",
        "              if len(json_imp_data_list) > 0:\n",
        "                json_imp_data.extend(json_imp_data_list)\n",
        "\n",
        "            # highlight date and time in PDF\n",
        "            if page_num in self.page_matched_date_dict:\n",
        "                json_date_data_list = self.highlight_date_and_time(page, page_num)\n",
        "                if len(json_date_data_list) > 0:\n",
        "                  json_date_data.extend(json_date_data_list)\n",
        "\n",
        "        # save PDF and JSON output file\n",
        "        pdf_output_file_name, json_data_dump_file = save_output_file(pdf_object=pdf_file, cfg=cfg,\n",
        "                                                                     output_type=2 if highlight_all else 1,\n",
        "                                                                     output_path=self.OUTPUT_FILES_PATH,\n",
        "                                                                     output_file_name=pdf_file_name,\n",
        "                                                                     json_data=json_data,\n",
        "                                                                     json_imp_data=json_imp_data,\n",
        "                                                                     json_date_data=json_date_data)\n",
        "        return pdf_output_file_name, json_data_dump_file\n",
        "\n",
        "    def highlight_icd_code_and_common_words(self, icd10_code_dict, pdf_file_name=None):\n",
        "        pdf_file = fitz.open(pdf_file_name)\n",
        "        json_data = []\n",
        "\n",
        "        for page_num, page in enumerate(pdf_file):\n",
        "            self.code_y2_coords_list = []\n",
        "            self.already_found_word_list = []\n",
        "            code_highlight_dict = {}\n",
        "\n",
        "            # highlight ICD-10 code\n",
        "            if page_num in icd10_code_dict:\n",
        "                num_page = page_num + 1\n",
        "                # let's store code y2 coords\n",
        "                for code in icd10_code_dict[page_num]:\n",
        "                    highlight_list = page.search_for(code)\n",
        "                    #print(f\"Page-{num_page}-{code}, Coordinate: {highlight_list}\")\n",
        "                    if len(highlight_list) > 0:\n",
        "                        code_highlight_dict[code] = highlight_list\n",
        "                        self.code_y2_coords_list.extend([highlight[3] for highlight in highlight_list])\n",
        "                    else:\n",
        "                        for alt_code in get_alternate_code_pattern(code):\n",
        "                            highlight_list = page.search_for(alt_code)\n",
        "                            for highlight in highlight_list:\n",
        "                                if len(highlight) > 0:\n",
        "                                    code_highlight_dict[code] = [highlight]\n",
        "                                    #print(f\"alternate-code:{code}, Coordinate: {highlight}\")\n",
        "                                    self.code_y2_coords_list.extend([highlight[3]])\n",
        "\n",
        "                filtered_code_highlight_dict = filter_duplicate_coordinate(code_highlight_dict)\n",
        "                # now, let highlight every code and its common words\n",
        "                for code, highlight_list in filtered_code_highlight_dict.items():\n",
        "                    prev_highlight = fitz.Rect(0, 0, 0, 0)\n",
        "                    for highlight in highlight_list:\n",
        "                        keyword = get_keyword(code, self.code_df)\n",
        "                        # get match score and common words coordinate\n",
        "                        all_match_keyword_list, all_match_imp_keyword_list = self.get_best_token_match(page_num, code,\n",
        "                                                                                                       page,\n",
        "                                                                                                       highlight[3],\n",
        "                                                                                                       self.match_threshold)\n",
        "\n",
        "                        # Step-1: highlight and set color coding dont have common words\n",
        "                        if not all_match_keyword_list:\n",
        "                            page_highlight = page.add_highlight_annot(highlight)\n",
        "                            page_highlight.set_colors(stroke=[0.92, 0.59, 0.48])  # dark salmon\n",
        "                            page_highlight.update()\n",
        "                            # prepare json object\n",
        "                            json_data.append(prepare_json_object(num_page, code, self.code_df, highlight, keyword))\n",
        "\n",
        "                        # Step-2: highlight and set color coding that have common words\n",
        "                        for match_keyword_dict in all_match_keyword_list:\n",
        "                            # highlight ICD-10 code\n",
        "                            if not self.is_coord_equal(prev_highlight, highlight):\n",
        "                              highlight_code(page, highlight)\n",
        "                              prev_highlight = highlight\n",
        "                            # highlight common words\n",
        "                            highlight_coords_dict = {}\n",
        "                            highlight_coords_dict, highlighted_word_list = self.highlight_common_words(page, match_keyword_dict[\"common_words_coords\"])\n",
        "\n",
        "                            # prepare json object\n",
        "                            json_data.append(prepare_json_object(num_page, code, self.code_df, highlight, keyword, match_keyword_dict, highlight_coords_dict))\n",
        "\n",
        "        # save PDF and JSON output file\n",
        "        pdf_output_file_name, json_data_dump_file = save_output_file(pdf_file, cfg, 3, self.OUTPUT_FILES_PATH, pdf_file_name, json_data)\n",
        "        return pdf_output_file_name, json_data_dump_file\n",
        "\n",
        "    def highlight_icd_code_and_keyword_impairment(self, icd10_code_dict, pdf_file_name=None):\n",
        "      pdf_file = fitz.open(pdf_file_name)\n",
        "      json_data = []\n",
        "      json_imp_data = []\n",
        "      json_date_data = []\n",
        "\n",
        "      for page_num, page in enumerate(pdf_file):\n",
        "        already_done_page_list = []\n",
        "        self.code_y2_coords_list = []\n",
        "        self.already_found_word_list = []\n",
        "        self.highlighted_coord_list = []\n",
        "        self.highlighted_term_list = []\n",
        "        code_highlight_dict = {}\n",
        "\n",
        "        # highlight ICD-10 code\n",
        "        if page_num in icd10_code_dict:\n",
        "            num_page = page_num + 1\n",
        "\n",
        "            # let's store code y2 coords\n",
        "            for code in icd10_code_dict[page_num]:\n",
        "                highlight_list = page.search_for(code)\n",
        "                #print(f\"Page-{num_page}-{code}, Coordinate: {highlight_list}\")\n",
        "                if len(highlight_list) > 0:\n",
        "                    code_highlight_dict[code] = highlight_list\n",
        "                    self.code_y2_coords_list.extend([highlight[3] for highlight in highlight_list])\n",
        "                else:\n",
        "                    for alt_code in get_alternate_code_pattern(code):\n",
        "                        highlight_list = page.search_for(alt_code)\n",
        "                        for highlight in highlight_list:\n",
        "                            if len(highlight) > 0:\n",
        "                                code_highlight_dict[code] = [highlight]\n",
        "                                # print(f\"alternate-code:{code}, Coordinate: {highlight}\")\n",
        "                                self.code_y2_coords_list.extend([highlight[3]])\n",
        "                                # print(f\"[highlight[3]]: {[highlight[3]]}\")\n",
        "\n",
        "            filtered_code_highlight_dict = filter_duplicate_coordinate(code_highlight_dict)\n",
        "            # now, let highlight every code and its common words\n",
        "            for code, highlight_list in filtered_code_highlight_dict.items():\n",
        "                prev_highlight = fitz.Rect(0, 0, 0, 0)\n",
        "                for highlight in highlight_list:\n",
        "                    all_match_keyword_list, all_match_imp_keyword_list = self.get_best_all_token_match(page_num,\n",
        "                                                                                                    code,\n",
        "                                                                                                    page,\n",
        "                                                                                                    highlight[3],\n",
        "                                                                                                    self.match_threshold)\n",
        "                    # Step-1: highlight ICD-10 code\n",
        "                    #for match_keyword_dict in all_match_keyword_list:\n",
        "                    #print(f\"prev_highlight: {prev_highlight}\")\n",
        "                    if not is_coord_equal(prev_highlight, highlight):\n",
        "                      #print(f\"highlight: {highlight}\")\n",
        "                      highlight_code(page, highlight)\n",
        "                      prev_highlight = highlight\n",
        "                    # prepare json object\n",
        "                    json_data.append(prepare_code_json_object(num_page, code, highlight))\n",
        "\n",
        "                    # Step-2: highlight and set color coding for impairment keyword common words\n",
        "                    json_imp_data_list = []\n",
        "                    for match_imp_keyword_dict in all_match_imp_keyword_list:\n",
        "                        if \"imp_common_keyword_coords\" in match_imp_keyword_dict:\n",
        "                            #print(f\"match_imp_keyword_dict: {match_imp_keyword_dict}\")\n",
        "                            highlight_coords_list = self.highlight_impairment(page, num_page, match_imp_keyword_dict[\"imp_common_keyword_coords\"])\n",
        "                            already_done_page_list.append(page_num)\n",
        "                            # prepare json object\n",
        "                            if highlight_coords_list:\n",
        "                                json_imp_data_list.extend(highlight_coords_list)\n",
        "                    if len(json_imp_data_list) > 0:\n",
        "                        json_imp_data.extend(json_imp_data_list)\n",
        "\n",
        "        # highlight ICD key phrase\n",
        "        if page_num in self.impairment_keyword_dict and page_num not in already_done_page_list:\n",
        "          json_imp_data_list = self.highlight_keyword_impairment_data(page, page_num)\n",
        "          if len(json_imp_data_list) > 0:\n",
        "            json_imp_data.extend(json_imp_data_list)\n",
        "\n",
        "        # highlight date and time in PDF\n",
        "        if page_num in self.page_matched_date_dict:\n",
        "          json_date_data_list = self.highlight_date_and_time(page, page_num)\n",
        "          if len(json_date_data_list) > 0:\n",
        "            json_date_data.extend(json_date_data_list)\n",
        "\n",
        "      # save PDF and JSON output file\n",
        "      pdf_output_file_name, json_data_dump_file = save_output_file(pdf_object=pdf_file, cfg=cfg,\n",
        "                                                                   output_type=4,\n",
        "                                                                   output_path=self.OUTPUT_FILES_PATH,\n",
        "                                                                   output_file_name=pdf_file_name,\n",
        "                                                                   json_data=json_data,\n",
        "                                                                   json_imp_data=json_imp_data,\n",
        "                                                                   json_date_data=json_date_data)\n",
        "      return pdf_output_file_name, json_data_dump_file\n",
        "\n",
        "    def highlight_keyword_impairment(self, pdf_file_name=None):\n",
        "        pdf_file = fitz.open(pdf_file_name)\n",
        "        json_imp_data = []\n",
        "        for page_num, page in enumerate(pdf_file):\n",
        "          self.highlighted_coord_list = []\n",
        "          # highlight ICD key phrase\n",
        "          if page_num in self.impairment_keyword_dict:\n",
        "            json_imp_data_list = []\n",
        "            num_page = page_num + 1\n",
        "            imp_keywords_coord_dict = {}\n",
        "            key_phrase_sents = self.impairment_keyword_dict[page_num]\n",
        "            for key_phrase_sent in key_phrase_sents:\n",
        "              #print(f\"key_phrase_sent: {key_phrase_sent}\")\n",
        "              coordinates = page.search_for(key_phrase_sent)\n",
        "              imp_keywords_coord_dict[key_phrase_sent] = coordinates\n",
        "            highlight_coords_list = self.highlight_impairment(page, num_page, imp_keywords_coord_dict)\n",
        "            if len(highlight_coords_list) > 0:\n",
        "              json_imp_data_list.extend(highlight_coords_list)\n",
        "            if len(json_imp_data_list) > 0:\n",
        "              json_imp_data.extend(json_imp_data_list)\n",
        "\n",
        "        # save PDF and JSON output file\n",
        "        pdf_output_file_name, json_data_dump_file = save_output_file(pdf_file, cfg, 5, self.OUTPUT_FILES_PATH, pdf_file_name, json_imp_data=json_imp_data)\n",
        "        return pdf_output_file_name, json_data_dump_file\n",
        "\n",
        "    def highlight_keyword_impairment_data(self, p_page, page_num):\n",
        "        json_imp_data_list = []\n",
        "        num_page = page_num + 1\n",
        "        imp_keywords_coord_dict = {}\n",
        "        key_phrase_sents = self.impairment_keyword_dict[page_num]\n",
        "        for key_phrase_sent in key_phrase_sents:\n",
        "            coordinates = p_page.search_for(key_phrase_sent)\n",
        "            imp_keywords_coord_dict[key_phrase_sent] = coordinates\n",
        "        highlight_coords_list = self.highlight_impairment(p_page, num_page, imp_keywords_coord_dict)\n",
        "        if len(highlight_coords_list) > 0:\n",
        "            json_imp_data_list.extend(highlight_coords_list)\n",
        "        return json_imp_data_list\n",
        "\n",
        "    def highlight_date_and_time(self, p_page, page_num):\n",
        "        num_page = page_num + 1\n",
        "        json_date_data_list = []\n",
        "        highlight_coords_list = []\n",
        "        page_date_coord_dict = {}\n",
        "\n",
        "        def highlight_date(p_coordinate, p_date):\n",
        "          page_highlight = p_page.add_highlight_annot(p_coordinate)\n",
        "          page_highlight.set_colors(stroke=[0.529, 0.807, 0.921])  # sky blue\n",
        "          page_highlight.update()\n",
        "          page_date_coord_dict[p_date] = [p_coordinate[0], p_coordinate[1], p_coordinate[2], p_coordinate[3]]\n",
        "          self.highlighted_term_list.append(p_date)\n",
        "\n",
        "        page_date_list = self.page_matched_date_dict[page_num]\n",
        "        for page_date in page_date_list:\n",
        "            coordinates = p_page.search_for(page_date)\n",
        "            for coordinate in coordinates:\n",
        "              if len(re.findall(r\"^\\d+$\", page_date.strip())) > 0:\n",
        "                if is_exact_year_match(p_page, page_date, coordinate, self.highlighted_term_list):\n",
        "                  highlight_date(coordinate, page_date)\n",
        "              else:\n",
        "                highlight_date(coordinate, page_date)\n",
        "\n",
        "        if page_date_coord_dict:\n",
        "            highlight_coords_list.append(prepare_page_date_json_object(num_page, page_date_coord_dict))\n",
        "        if len(highlight_coords_list) > 0:\n",
        "            json_date_data_list.extend(highlight_coords_list)\n",
        "        return json_date_data_list\n",
        "\n",
        "    def highlight_impairment(self, page_obj, num_page, imp_keywords_coord_dict):\n",
        "        highlight_coords_list = []\n",
        "\n",
        "        for keyword_impairment, imp_keywords_coords in imp_keywords_coord_dict.items():\n",
        "          #print(f\"keyword_impairment: {keyword_impairment}\")\n",
        "          highlight_coords_dict = {}\n",
        "          key_phrase_list = keyword_impairment.split()\n",
        "          for key_phrase in key_phrase_list:\n",
        "            coords_list = []\n",
        "            for imp_keywords_coord in imp_keywords_coords:\n",
        "              if is_exact_match(page_obj, key_phrase, imp_keywords_coord, full_match=True, case_sensitive=True) and not is_coord_already_highlighted(self.highlighted_coord_list, imp_keywords_coord):\n",
        "                page_highlight = page_obj.add_highlight_annot(imp_keywords_coord)\n",
        "                page_highlight.update()\n",
        "                coords_list.append((imp_keywords_coord[0], imp_keywords_coord[1], imp_keywords_coord[2], imp_keywords_coord[3]))\n",
        "                self.highlighted_coord_list.append(imp_keywords_coord)\n",
        "                # print(f\"key_phrase: {key_phrase}, keywords_coord: {imp_keywords_coord}\")\n",
        "            # print(f\"coords_list: {coords_list}\")\n",
        "            highlight_coords_dict[key_phrase] = coords_list\n",
        "          if highlight_coords_dict:\n",
        "            highlight_coords_list.append(prepare_imp_json_object(num_page, self.synid_df, keyword_impairment, highlight_coords_dict))\n",
        "        return highlight_coords_list\n",
        "\n",
        "    def highlight_common_words(self, page_obj, common_words_coord_dict):\n",
        "        highlight_coords_dict = {}\n",
        "        highlighted_word_list = []\n",
        "\n",
        "        prev_common_word = \"\"\n",
        "        for common_word, common_word_coord in common_words_coord_dict.items():\n",
        "            highlight = page_obj.add_highlight_annot(common_word_coord)\n",
        "            highlight.update()\n",
        "            if prev_common_word.lower() != common_word.lower():\n",
        "                highlight_coords_dict[common_word] = common_word_coord\n",
        "                highlighted_word_list.append(common_word)\n",
        "                prev_common_word = common_word\n",
        "        return highlight_coords_dict, highlighted_word_list\n",
        "\n",
        "    def get_best_token_match(self, page_num, p_code, page_obj, code_y2_coord, match_threshold):\n",
        "        all_match_keyword_list = []\n",
        "        all_match_imp_keyword_list = []\n",
        "\n",
        "        # Step 1: reverse code pattern\n",
        "        reversed_icd_code = reverse_code_pattern(p_code)\n",
        "        # Step 2: fetch keyword based on code\n",
        "        keyword = get_keyword(reversed_icd_code, self.code_df)\n",
        "        # Step 3: get code paragraph\n",
        "        code_paragraph_list, non_code_paragraph_list = get_paragraph(page_obj, code_y2_coord)\n",
        "        # print(f\"code_paragraph_list: {non_code_paragraph_list}\")\n",
        "\n",
        "        # Step 4: prepare code_paragraph for common words\n",
        "        for code_paragraph in code_paragraph_list:\n",
        "            match_keyword_dict = {}\n",
        "            common_words = get_common_words(keyword, code_paragraph)\n",
        "            if len(common_words) > 0:\n",
        "                match_keyword_dict[\"common_words\"] = common_words\n",
        "                # Step 4: get best token match ratio\n",
        "                clean_paragraph = \" \".join(clean_text(code_paragraph))\n",
        "                match_keyword_dict[\"paragraph\"] = clean_paragraph\n",
        "                # match_score = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "                # if fuzz.token_set_ratio(keyword, clean_paragraph) >= match_threshold else 0\n",
        "                match_keyword_dict[\"score\"] = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "                # Step 5: build common words coordinate dict\n",
        "                common_words_coord_dict = {}\n",
        "                #print(f\"code_y2_coord: {code_y2_coord}\")\n",
        "                for common_word in common_words:\n",
        "                    highlight_list = page_obj.search_for(common_word)\n",
        "                    #print(f\"highlight_list: {highlight_list}\")\n",
        "                    found_coord = False\n",
        "                    for highlight in highlight_list:\n",
        "                        # get common word y2 coord value\n",
        "                        common_word_y2_coords = highlight[3]\n",
        "                        common_word_y1_coords = highlight[1]\n",
        "                        if (code_y2_coord - 2) <= common_word_y2_coords <= (code_y2_coord + 2):\n",
        "                          common_words_coord_dict[common_word] = highlight\n",
        "                          found_coord = True\n",
        "                        if not found_coord:\n",
        "                          if (code_y2_coord - 20) <= common_word_y2_coords <= (code_y2_coord + 20):\n",
        "                            common_words_coord_dict[common_word] = highlight\n",
        "                            # self.already_found_word_list.append(common_word)\n",
        "                match_keyword_dict[\"common_words_coords\"] = common_words_coord_dict\n",
        "                all_match_keyword_list.append(match_keyword_dict)\n",
        "\n",
        "        def get_keyword_impairment_list(p_paragraph):\n",
        "            keyword_impairments = []\n",
        "            for keyword_impairment in self.impairment_keyword_dict[page_num]:\n",
        "                keyword_impairment_found = get_common_words(keyword_impairment, p_paragraph)\n",
        "                if len(keyword_impairment_found) > 0:\n",
        "                    keyword_impairments.append(keyword_impairment)\n",
        "            return keyword_impairments\n",
        "\n",
        "        # Step 5: prepare non code_paragraph for common words\n",
        "        for non_code_paragraph in non_code_paragraph_list:\n",
        "            match_imp_keyword_dict = {}\n",
        "            imp_common_words_coord_dict = {}\n",
        "            keyword_impairment_list = get_keyword_impairment_list(non_code_paragraph)\n",
        "            for keyword_impairment in keyword_impairment_list:\n",
        "                highlight_list = page_obj.search_for(keyword_impairment)\n",
        "                highlight_list = [highlight for highlight in highlight_list if highlight[3] not in self.code_y2_coords_list]\n",
        "                if keyword_impairment not in self.already_found_word_list and len(highlight_list) > 0:\n",
        "                    self.already_found_word_list.append(keyword_impairment)\n",
        "                    imp_common_words_coord_dict[f\"{keyword_impairment}\"] = highlight_list\n",
        "                    continue\n",
        "            match_imp_keyword_dict[\"imp_common_keyword_coords\"] = imp_common_words_coord_dict\n",
        "            all_match_imp_keyword_list.append(match_imp_keyword_dict)\n",
        "\n",
        "        return all_match_keyword_list, all_match_imp_keyword_list\n",
        "\n",
        "    def get_best_all_token_match(self, page_num, p_code, page_obj, code_y2_coord, match_threshold):\n",
        "        all_match_keyword_list = []\n",
        "        all_match_imp_keyword_list = []\n",
        "\n",
        "        # Step 1: reverse code pattern\n",
        "        reversed_icd_code = reverse_code_pattern(p_code)\n",
        "        # Step 2: fetch keyword based on code\n",
        "        keyword = get_keyword(reversed_icd_code, self.code_df)\n",
        "        # Step 3: get all paragraph\n",
        "        all_paragraph_list = get_all_paragraph(page_obj)\n",
        "        # print(f\"code_paragraph_list: {non_code_paragraph_list}\")\n",
        "\n",
        "        def get_keyword_impairment_list(p_paragraph):\n",
        "          keyword_impairments = []\n",
        "          for keyword_impairment in self.impairment_keyword_dict[page_num]:\n",
        "              keyword_impairment_found = get_common_words(keyword_impairment, p_paragraph)\n",
        "              if len(keyword_impairment_found) > 0:\n",
        "                  keyword_impairments.append(keyword_impairment)\n",
        "          return keyword_impairments\n",
        "\n",
        "        # Step 4: prepare all_paragraph for common words and impairment\n",
        "        for all_paragraph in all_paragraph_list:\n",
        "          match_keyword_dict = {}\n",
        "          common_words = get_common_words(keyword, all_paragraph)\n",
        "          if len(common_words) > 0:\n",
        "              match_keyword_dict[\"common_words\"] = common_words\n",
        "              # Step 4: get best token match ratio\n",
        "              clean_paragraph = \" \".join(clean_text(all_paragraph))\n",
        "              match_keyword_dict[\"paragraph\"] = clean_paragraph\n",
        "              # match_score = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "              # if fuzz.token_set_ratio(keyword, clean_paragraph) >= match_threshold else 0\n",
        "              match_keyword_dict[\"score\"] = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "              # Step 5: build common words coordinate dict\n",
        "              common_words_coord_dict = {}\n",
        "              for common_word in common_words:\n",
        "                  highlight_list = page_obj.search_for(common_word)\n",
        "                  found_coord = False\n",
        "                  for highlight in highlight_list:\n",
        "                      # get common word y2 coord value\n",
        "                      common_word_y2_coords = highlight[3]\n",
        "                      if (code_y2_coord - 2) <= common_word_y2_coords <= (code_y2_coord + 2):\n",
        "                        common_words_coord_dict[common_word] = highlight\n",
        "                        found_coord = True\n",
        "                      if not found_coord:\n",
        "                        if (code_y2_coord - 20) <= common_word_y2_coords <= (code_y2_coord + 20):\n",
        "                          common_words_coord_dict[common_word] = highlight\n",
        "                          # self.already_found_word_list.append(common_word)\n",
        "              match_keyword_dict[\"common_words_coords\"] = common_words_coord_dict\n",
        "              all_match_keyword_list.append(match_keyword_dict)\n",
        "          else:\n",
        "            # Step 5: prepare non code_paragraph for common words\n",
        "            match_imp_keyword_dict = {}\n",
        "            imp_common_words_coord_dict = {}\n",
        "            keyword_impairment_list = get_keyword_impairment_list(all_paragraph)\n",
        "            for keyword_impairment in keyword_impairment_list:\n",
        "              #print(f\"keyword_impairment11: {keyword_impairment}\")\n",
        "              highlight_list = page_obj.search_for(keyword_impairment)\n",
        "              if keyword_impairment not in self.already_found_word_list and len(highlight_list) > 0:\n",
        "                self.already_found_word_list.append(keyword_impairment)\n",
        "                imp_common_words_coord_dict[f\"{keyword_impairment}\"] = highlight_list\n",
        "                #print(f\"keyword_impairment22: {keyword_impairment}, coords: {highlight_list}\")\n",
        "                #continue\n",
        "            match_imp_keyword_dict[\"imp_common_keyword_coords\"] = imp_common_words_coord_dict\n",
        "            all_match_imp_keyword_list.append(match_imp_keyword_dict)\n",
        "\n",
        "        return all_match_keyword_list, all_match_imp_keyword_list\n",
        "\n",
        "\n",
        "    def search_icd_code(self, txt_list):\n",
        "      pdf_page_vocab = {}\n",
        "      for txt_file in txt_list:\n",
        "        with open(txt_file, \"r\") as f:\n",
        "          page_txt = f.read()\n",
        "          # check the page that have line number instead of code\n",
        "          index_page = False\n",
        "          if re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "            index_page = True\n",
        "\n",
        "          doc = self.nlp_code10(page_txt)\n",
        "          code_list = []\n",
        "          if index_page:\n",
        "            # check the code contain letter \"L\"\n",
        "            code_list = [ent.text for ent in doc.ents if not re.search(\"(L[0-9]+)\", ent.text)]\n",
        "          else:\n",
        "            code_list = [ent.text for ent in doc.ents]\n",
        "\n",
        "          if len(code_list) != 0:\n",
        "            # page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "            page_number = int(txt_file.split(\"/\")[-1].split(\".\")[0].split(\"-\")[1])\n",
        "            pdf_page_vocab[page_number] = list(set(code_list))\n",
        "            # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "      return pdf_page_vocab"
      ],
      "metadata": {
        "id": "jRYlndgDUtds"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PDF Highlighting"
      ],
      "metadata": {
        "id": "7s7ZxYMUBAol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /home/ocreng\n",
        "!mkdir -p /home/ocreng/ocrhigh\n",
        "!mkdir -p /home/ocreng/ocrhigh/input\n",
        "!mkdir -p /home/ocreng/ocrhigh/output\n",
        "!mkdir -p /home/ocreng/ocrhigh/processed\n",
        "!mkdir -p /home/ocreng/ocrhigh/pdf-files\n",
        "!mkdir -p /home/ocreng/ocrhigh/txt-files"
      ],
      "metadata": {
        "id": "ZWvB_40a8SNJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def purge(file_path):\n",
        "  for f in glob.glob(file_path):\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "xQe5Xfp9AWLn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Step-0: create highlighter instance\n",
        "INPUT_PDF_FILES_PATH = cfg.file_path[\"INPUT_PATH\"]\n",
        "\n",
        "highlighter = Highlighter(match_threshold=35)\n",
        "sent_extractor = SentenceExtractor()\n",
        "generic_matcher = GenericMatcher()"
      ],
      "metadata": {
        "id": "MkoXzNA1AWu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63dde92-4ce7-4780-9f62-ac13838ab2dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 16s, sys: 14.5 s, total: 1min 31s\n",
            "Wall time: 1min 38s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```log\n",
        "CPU times: user 1min 7s, sys: 6.54 s, total: 1min 14s\n",
        "Wall time: 1min 15s\n",
        "```"
      ],
      "metadata": {
        "id": "ftvHz73FCCke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test"
      ],
      "metadata": {
        "id": "S-EA40XdAcCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /home/ocreng/ocrhigh/input\n",
        "!mkdir -p /home/ocreng/ocrhigh/input"
      ],
      "metadata": {
        "id": "wfJwndx9vcTP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp page-0.pdf /home/ocreng/ocrhigh/input"
      ],
      "metadata": {
        "id": "j6YcCHZ_9OMw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /home/ocreng/ocrhigh/output\n",
        "!mkdir -p /home/ocreng/ocrhigh/output\n",
        "!rm -rf output*.zip"
      ],
      "metadata": {
        "id": "3ArIraAD0zx3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_type = 4"
      ],
      "metadata": {
        "id": "uyQTNjI_gXJJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "txt_list = None\n",
        "icd10_code_dict1 = None\n",
        "wrong_keyword_dict = None\n",
        "for pdf_file in os.listdir(INPUT_PDF_FILES_PATH):\n",
        "  pdf_file_name = f\"{INPUT_PDF_FILES_PATH}/{pdf_file}\"\n",
        "\n",
        "  # Step-1: splitting pdf file\n",
        "  print(\"Step-1: Splitting pdf file............\")\n",
        "  pdf_list = split_pdf(highlighter.PDF_FILES_PATH, pdf_file_name)\n",
        "\n",
        "  # Step-2: Extracting text from pdf\n",
        "  print(\"Step-2: Extracting text from pdf............\")\n",
        "  txt_list = extract_text_from_pdf(highlighter.PDF_FILES_PATH, highlighter.TXT_FILES_PATH, pdf_list)\n",
        "  highlighter.set_text_list(txt_list)\n",
        "\n",
        "  # Step-3: Searching ICD-10 code\n",
        "  print(\"Step-3: Searching ICD-10 code into text file..........\")\n",
        "  icd10_code_dict = highlighter.search_icd_code(txt_list)\n",
        "  icd10_code_dict1 = icd10_code_dict\n",
        "\n",
        "  # Step-4: Get closet match of ICD-10 keyword\n",
        "  print(\"Step-4: Get closet match of ICD-10 keyword..........\")\n",
        "  matched_keyword_dict = sent_extractor.get_matched_keyword_dict(txt_list)\n",
        "  highlighter.set_impairment_keyword_dict(matched_keyword_dict)\n",
        "  wrong_keyword_dict = matched_keyword_dict\n",
        "\n",
        "  # Step-5: Get date time list from text file\n",
        "  print(\"Step-5: Get date time list from text file..........\")\n",
        "  matched_date_dict = generic_matcher.get_match_date_dict(txt_list)\n",
        "  highlighter.set_page_matched_date_dict(matched_date_dict)\n",
        "\n",
        "  if output_type == 1:\n",
        "    # Step-6: Highlighting ICD-10 code, its description and keyword impairment into PDF file\n",
        "    print(\"Step-6: Highlighting ICD-10 code, its description and keyword impairment into PDF file............\")\n",
        "    pdf_output_file, json_code_output_file = highlighter.highlight_all(icd10_code_dict, pdf_file_name=pdf_file_name)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "    print(f\"Highlighted code and impairment coordinates are saved into [{json_code_output_file}] file.\")\n",
        "  elif output_type == 2:\n",
        "    # Step-7: Highlighting ICD-10 code, its description and all keyword impairment into pdf\n",
        "    print(\"Step-7: Highlighting ICD-10 code, its description and all keyword impairment into pdf............\")\n",
        "    pdf_output_file, json_code_output_file = highlighter.highlight_all(icd10_code_dict, pdf_file_name=pdf_file_name, highlight_all=True)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "    print(f\"Highlighted code and impairment coordinates are saved into [{json_code_output_file}] file.\")\n",
        "  elif output_type == 3:\n",
        "    # Step-8: Highlighting ICD-10 code and its description into pdf\n",
        "    print(\"Step-8: Highlighting ICD-10 code and its description into pdf............\")\n",
        "    pdf_output_file, json_output_file = highlighter.highlight_icd_code_and_common_words(icd10_code_dict, pdf_file_name=pdf_file_name)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "    print(f\"Highlighted coordinates are saved into [{json_output_file}] file.\")\n",
        "  elif output_type == 4:\n",
        "    # Step-9: Highlighting ICD-10 code and all keyword impairment into pdf\n",
        "    print(\"Step-9: Highlighting ICD-10 code and all keyword impairment into pdf............\")\n",
        "    pdf_output_file, json_code_output_file = highlighter.highlight_icd_code_and_keyword_impairment(icd10_code_dict, pdf_file_name=pdf_file_name)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code and all keyword impairment \")\n",
        "    print(f\"Highlighted code and impairment coordinates are saved into [{json_code_output_file}] file.\")\n",
        "  elif output_type == 5:\n",
        "    # Step-10: Highlighting keyword impairment into pdf\n",
        "    print(\"Step-10: Highlighting keyword impairment into pdf............\")\n",
        "    pdf_output_file, json_output_file = highlighter.highlight_keyword_impairment(pdf_file_name=pdf_file_name)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting keyword impairment.\")\n",
        "    print(f\"Highlighted coordinates are saved into [{json_output_file}] file.\")\n",
        "  else:\n",
        "    print(\"Please pass value 1, 2 and 3 for output type.\")\n",
        "\n",
        "  # Step-11: Clean up: move the current file into processed folder\n",
        "  if output_type in [1, 2, 3, 4, 5]:\n",
        "    if Path(f\"{cfg.file_path['PROCESSED_PATH']}/{pdf_file}\").exists():\n",
        "      # take backup of existing file\n",
        "      shutil.move(f\"{cfg.file_path['PROCESSED_PATH']}/{pdf_file}\", f\"{cfg.file_path['PROCESSED_PATH']}/{pdf_file}_bkp\")\n",
        "      # then move it\n",
        "      shutil.move(pdf_file_name, cfg.file_path[\"PROCESSED_PATH\"])\n",
        "    else:\n",
        "      shutil.move(pdf_file_name, cfg.file_path[\"PROCESSED_PATH\"])\n",
        "\n",
        "  # remove all pdf and text files\n",
        "  purge(f\"{cfg.file_path['TMP_PDF_FILES_PATH']}/*.pdf\")\n",
        "  purge(f\"{cfg.file_path['TMP_TXT_FILES_PATH']}/*.txt\")\n",
        "  pdf_list = []\n",
        "  txt_list = []"
      ],
      "metadata": {
        "id": "hiOT-aTfedMT",
        "outputId": "b4a4ec64-b805-4e7c-fa85-c4beda8c5bcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-1: Splitting pdf file............\n",
            "Step-2: Extracting text from pdf............\n",
            "Step-3: Searching ICD-10 code into text file..........\n",
            "Step-4: Get closet match of ICD-10 keyword..........\n",
            "Step-5: Get date time list from text file..........\n",
            "Step-9: Highlighting ICD-10 code and all keyword impairment into pdf............\n",
            "term: 3, found_text: 23\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023']\n",
            "matches: True\n",
            "term: 3, found_text: 23\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3']\n",
            "matches: False\n",
            "term: 3, found_text: ID3\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3']\n",
            "matches: False\n",
            "term: 02360, found_text:  02360\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3']\n",
            "matches: True\n",
            "term: 2, found_text:  2\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360']\n",
            "matches: True\n",
            "term: 2, found_text:  2\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360', '2']\n",
            "matches: False\n",
            "term: 2, found_text: 02\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360', '2']\n",
            "matches: False\n",
            "term: 2, found_text: 02\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360', '2']\n",
            "matches: False\n",
            "term: 2, found_text: 7/2\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360', '2']\n",
            "matches: False\n",
            "term: 2, found_text: 02\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360', '2']\n",
            "matches: False\n",
            "term: 2, found_text:  2\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360', '2']\n",
            "matches: False\n",
            "term: 2, found_text:  2\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360', '2']\n",
            "matches: False\n",
            "term: 2, found_text: ID2\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360', '2']\n",
            "matches: False\n",
            "term: 6, found_text: 6\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360', '2']\n",
            "matches: True\n",
            "term: 6, found_text: 36\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360', '2', '6']\n",
            "matches: False\n",
            "term: 6, found_text: 6\n",
            "\n",
            "highlighted_list: ['02/17/2018', 'February 24, 2023', 'February 24, 2023', 'February 24, 2023', '3', '02360', '2', '6']\n",
            "matches: False\n",
            "File[/home/ocreng/ocrhigh/output/page-0_output_4.pdf] is saved after highlighting ICD-10 code and all keyword impairment \n",
            "Highlighted code and impairment coordinates are saved into [/home/ocreng/ocrhigh/output/page-0_output_4.json] file.\n",
            "CPU times: user 199 ms, sys: 11.1 ms, total: 210 ms\n",
            "Wall time: 215 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matched_date_dict[0]"
      ],
      "metadata": {
        "id": "F69FGJa3tuD1",
        "outputId": "61536019-2788-4d42-819a-c07c989387c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['02/17/2018', 'February 24, 2023', '3', '02360', '2', '6']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_keyword_dict[0]"
      ],
      "metadata": {
        "id": "Tg9Anb2fgHW-",
        "outputId": "9fd49505-b6f9-45cb-f7d6-d66a07f86b90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DOB', 'Name', 'Patient'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip output{output_type}.zip /home/ocreng/ocrhigh/output/*.*"
      ],
      "metadata": {
        "id": "MeS8LHhjjfzU",
        "outputId": "040c83f8-d2cf-4c69-df93-927890866346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: home/ocreng/ocrhigh/output/page-0_output_4.json (deflated 58%)\n",
            "  adding: home/ocreng/ocrhigh/output/page-0_output_4.pdf (deflated 27%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip text_files.zip /home/ocreng/ocrhigh/txt-files/*.*\n",
        "#!zip pdf_files.zip /home/ocreng/ocrhigh/pdf-files/*.*"
      ],
      "metadata": {
        "id": "sf5eIhLWp6UE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wrong_keyword_dict1[28]"
      ],
      "metadata": {
        "id": "abzlswRvhDZf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```log\n",
        "{'ANION GAP',\n",
        " 'BASIC METABOLIC PANEL',\n",
        " 'BLOOD UREA NITROGEN',\n",
        " 'CALCIUM',\n",
        " 'CHLORIDE',\n",
        " 'CREATININE',\n",
        " 'GLUCOSE',\n",
        " 'POTASSIUM',\n",
        " 'SODIUM',\n",
        " 'UREA',\n",
        " 'UREA NITROGEN'}\n",
        " ```"
      ],
      "metadata": {
        "id": "p75Glq5HLuWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exact Match"
      ],
      "metadata": {
        "id": "vIep4k_1UBSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mytext = \"\"\"\n",
        "  Encounter #16\n",
        "History & Physical Report\n",
        "1/25/2021: Lab Orders - Hypertriglyceridemia,                 sporadic (E78.3) (Marina Dobricic)\n",
        "   ppointment:    1/25/2021   9:00 AM\n",
        "                                  / Race: White\n",
        "Male\n",
        " The patient is a 41 year old male.\n",
        " Assessment & Plan (Angela Sabbagh; 1/23/2021            11:36 AM)\n",
        " Hypertriglyceridemia, sporadic (E78.3)\n",
        " Impression: stopped will restart and check labs\n",
        "Current Plans\n",
        "       @   Comprehensive Metabolic Panel (CMP) (80053)\n",
        "       @   Complete Blood Count with Differential (CBC) (85025)\n",
        "       @   Lipid Panel (LP) (80061)\n",
        "Signed by Marina Dobricic (1/27/2021        9:54 AM)\n",
        " Comprehensive Metabolic Panel (CMP) (80053) Final, Reviewed (Collected: 01/25/2021)\n",
        " Diagnosis: Hypertriglyceridemia, sporadic (E78.3)\n",
        " Note: Testing done at Silver Pine Medical         Group unless otherwise specified. 43455 Schoenherr Road, Suite 19, Sterling Heights, MI 48313\n",
        "      Sodium                                  141 mmol/L              (Normal Range: 135-145 mmol/L)         Result Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Potassium                               4.1  mmol/L             (Normal Range: 3.5-5.2 mmol/L)        Result    Note:\n",
        "                                                                    Result Annotation:\n",
        "      Chloride                                103   mmol/L            (Normal Range: 98-107 mmol/L)         Result    Note:\n",
        "                                                                    Result Annotation:\n",
        "      Carbon    Dioxide (CO2)                 30 mmol/L               (Normal Range: 21-31    mmol/L)     Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Anion Gap                               8                       (Normal Range: 5-17)      Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Glucose                                 89 mg/dL                (Normal Range: 60-99 mg/dL)        Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Blood Urea Nitrogen (BUN)               17 mg/dLl               (Normal Range: 8-23 mg/dL)        Result    Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Creatinine                              1.03   mg/dL            (Normal Range: 0.80-1.40 mg/dL)        Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      GFR                                     79 mL/min/1.73m2        (Normal Range: >59 mL/min/1.73m2)          Reeult      Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      GFR   African American                  96 mL/min/1.73m2        (Normal Range: >59 mL/min/1.73m2)          Result      Note:\n",
        "                                                                    Result Annotation:\n",
        "      Calcium                                 9.0 mg/dL               (Normal Range: 8.5-11.0 mg/dL)        Result     Note;:\n",
        "                                                                    Result Annotation:\n",
        "      Protein Total                           7.8 g/dL                (Normal Range: 6.4-8.2 g/dL)      Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Albumin                                 4.4 g/dL                (Normal Range: 3.4-5.0 g/dL)      Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Globulin                                3.40                    (Normal Range: 2.20-4.00)       Result    Wote:\n",
        "                                                                    Resutt Annotation:\n",
        "      Album   in/Globulin Ratio               1.3                         Result Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Alkaline Phosphatase (ALP)              64 U/L                  (Normal Range: 50-116 U/L)       Result    Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Aspartate Aminotransferase (AST)        25  U/L                 (Normal Range: 15-37 U/L)      Result    Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Alanine Aminotransferase (ALT)          33 U/L                  (Normal Range: 16-63 U/L)      Reeult    Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Bilirubin Total                         0.5 mg/dL               (Normal Range:   0.3-1.2 mg/dL)     Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        " Complete Blood Count with Differential (CBC) (85025) Final, Reviewed (Collected: 01/25/2021)\n",
        " Diagnosis: Hypertriglyceridemia, sporadic (E78.3)\n",
        " Note: Testina done at Silver Pine Medical Group unless otherwise specified. 43455 Schoenherr Road. Suite 19. Sterlina Heiahts. Ml 48313\n",
        "06/28/2022 12:27 pm                                              s                                                                          Page 11/144\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "T6du52kDm4mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term = \"MI\"\n",
        "#matches = len(re.findall(r'(?i)\\bAlbumin\\b', mytext)) > 0\n",
        "# matches = len(re.findall(f'(?i)\\\\b{term}\\\\b', mytext)) > 0\n",
        "#if len(re.findall(f'(?i)\\\\b{term}\\\\b', mytext)) > 0:\n",
        "matches = re.findall(r'\\bMI\\b', \"mi mister miss\")\n",
        "matches"
      ],
      "metadata": {
        "id": "4CxtWaFymzks",
        "outputId": "64302d54-1514-4ffe-fb7b-d4efa0824974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-30.txt\", \"r\") as f:\n",
        "  mytext = f.read()\n",
        "if len(re.findall(f'(?i)\\\\b{term}\\\\b', mytext)) > 0:\n",
        "  print(\"Found\")"
      ],
      "metadata": {
        "id": "reFqGUwgvkJX",
        "outputId": "f503bc29-dcf0-48ef-ec5c-16c3ef2a9a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-29aebb679170>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"page-30.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mmytext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'(?i)\\\\b{term}\\\\b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmytext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'page-30.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_keyword_dict1[0]"
      ],
      "metadata": {
        "id": "oy3VrsX1vpuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "icd10_code_dict1"
      ],
      "metadata": {
        "id": "-PfeXsii2ZGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "text_list = split_pdf(highlighter.PDF_FILES_PATH, \"APS_38600000R_final.pdf\")\n",
        "len(text_list)"
      ],
      "metadata": {
        "id": "CJw-u86V4uF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Highlight Test"
      ],
      "metadata": {
        "id": "4HjWgUOqYgot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file_name = \"page-30.pdf\"\n",
        "pdf_file = fitz.open(pdf_file_name)\n",
        "for page_num, page in enumerate(pdf_file):\n",
        "  for keyword_impairment in wrong_keyword_dict1[0]:\n",
        "    # print(f\"keyword_impairment11: {keyword_impairment}\")\n",
        "    highlight = page.search_for(keyword_impairment)\n",
        "    print(f\"keyword_impairment: {keyword_impairment}, coords: {highlight}\")\n",
        "    highlight = page.add_highlight_annot(highlight)\n",
        "    highlight.update()\n",
        "output_pdf_file_name = f\"{pdf_file_name.split('.')[0]}_output.pdf\"\n",
        "pdf_file.save(output_pdf_file_name, garbage=4, deflate=True, clean=True)"
      ],
      "metadata": {
        "id": "AZKQdR3oAzOx",
        "outputId": "1d508899-4a61-4ec1-ecee-953a755bfba7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keyword_impairment: Urea, coords: [Rect(70.31328582763672, 435.0400390625, 83.75199890136719, 443.0439453125)]\n",
            "keyword_impairment: Comprehensive Metabolic Panel, coords: [Rect(60.47419357299805, 207.52001953125, 118.06912994384766, 215.52392578125), Rect(121.66831970214844, 207.52001953125, 156.94491577148438, 215.52392578125), Rect(159.82464599609375, 207.52001953125, 176.2230682373047, 215.52392578125), Rect(29.997119903564453, 282.8800048828125, 95.99125671386719, 290.8839111328125), Rect(99.83040618896484, 282.8800048828125, 141.58639526367188, 290.8839111328125), Rect(144.46612548828125, 282.8800048828125, 163.6642608642578, 290.8839111328125)]\n",
            "keyword_impairment: ALT, coords: [Rect(148.30589294433594, 665.4400634765625, 158.38491821289062, 673.4439697265625)]\n",
            "keyword_impairment: GFR, coords: [Rect(45.355674743652344, 477.0400390625, 57.77448272705078, 485.0439453125), Rect(45.35564422607422, 498.1600341796875, 53.8148307800293, 506.1639404296875)]\n",
            "keyword_impairment: CBC, coords: [Rect(214.13943481445312, 216.1600341796875, 225.17835998535156, 224.1639404296875), Rect(206.74017333984375, 717.280029296875, 218.61904907226562, 725.283935546875)]\n",
            "keyword_impairment: Encounter, coords: [Rect(31.916934967041016, 27.03997802734375, 74.24887084960938, 35.04388427734375)]\n",
            "keyword_impairment: Anion Gap, coords: [Rect(44.87572479248047, 393.5200500488281, 65.99369812011719, 401.5239562988281), Rect(69.83332824707031, 393.5200500488281, 81.1722412109375, 401.5239562988281)]\n",
            "keyword_impairment: Sodium, coords: [Rect(45.35564422607422, 310.0, 69.01065063476562, 318.00390625)]\n",
            "keyword_impairment: CO2, coords: [Rect(109.46951293945312, 372.6400451660156, 120.38845825195312, 380.6439514160156)]\n",
            "keyword_impairment: CMP, coords: [Rect(186.74208068847656, 207.52001953125, 198.38096618652344, 215.52392578125), Rect(174.7432098388672, 282.8800048828125, 187.82196044921875, 290.8839111328125)]\n",
            "keyword_impairment: Alkaline Phosphatase, coords: [Rect(44.875816345214844, 623.6800537109375, 74.8731689453125, 631.6839599609375), Rect(77.99263000488281, 623.6800537109375, 120.8885269165039, 631.6839599609375)]\n",
            "keyword_impairment: BUN, coords: [Rect(129.18763732910156, 435.0400390625, 140.2265625, 443.0439453125)]\n",
            "keyword_impairment: Complete Blood Count, coords: [Rect(60.47420883178711, 216.1600341796875, 95.99054718017578, 224.1639404296875), Rect(99.35047149658203, 216.1600341796875, 119.26856231689453, 224.1639404296875), Rect(123.10819244384766, 216.1600341796875, 140.70652770996094, 224.1639404296875), Rect(29.997119903564453, 717.280029296875, 70.07352447509766, 725.283935546875), Rect(73.91290283203125, 717.280029296875, 97.67060852050781, 725.283935546875), Rect(101.0302963256836, 717.280029296875, 121.4283447265625, 725.283935546875)]\n",
            "keyword_impairment: Blood Urea Nitrogen, coords: [Rect(45.59565734863281, 435.0400390625, 65.99370574951172, 443.0439453125), Rect(70.31328582763672, 435.0400390625, 87.11167907714844, 443.0439453125), Rect(90.47135162353516, 435.0400390625, 118.41556549072266, 443.0439453125)]\n",
            "keyword_impairment: Glucose, coords: [Rect(45.355674743652344, 414.4000549316406, 71.18318176269531, 422.4039611816406)]\n",
            "keyword_impairment: Lipid Panel, coords: [Rect(60.95414733886719, 223.56005859375, 77.51274108886719, 232.564453125), Rect(81.1122055053711, 223.56005859375, 97.51043701171875, 232.564453125)]\n",
            "keyword_impairment: MI, coords: [Rect(210.2085723876953, 61.5999755859375, 218.56410217285156, 69.6038818359375), Rect(90.52633666992188, 171.280029296875, 97.7038803100586, 179.283935546875), Rect(136.0669708251953, 291.52001953125, 143.2662811279297, 299.52392578125), Rect(521.4700317382812, 299.9200134277344, 526.5894775390625, 307.9239196777344), Rect(207.80300903320312, 477.0400390625, 216.27084350585938, 485.0439453125), Rect(360.8774108886719, 477.0400390625, 369.1646423339844, 485.0439453125), Rect(207.802978515625, 498.1600341796875, 216.27081298828125, 506.1639404296875), Rect(361.0693054199219, 498.1600341796875, 369.3244934082031, 506.1639404296875), Rect(60.2342529296875, 561.0400390625, 67.91350555419922, 569.0439453125), Rect(88.38217163085938, 644.56005859375, 96.2025375366211, 652.56396484375), Rect(79.72894287109375, 665.4400634765625, 87.52114868164062, 673.4439697265625), Rect(136.0669403076172, 725.6799926757812, 143.26625061035156, 733.6838989257812)]\n",
            "keyword_impairment: Albumin, coords: [Rect(44.87572479248047, 561.0400390625, 71.75313568115234, 569.0439453125)]\n",
            "keyword_impairment: Aspartate, coords: [Rect(44.87578582763672, 644.56005859375, 77.48865509033203, 652.56396484375)]\n",
            "keyword_impairment: Urea Nitrogen, coords: [Rect(70.31328582763672, 435.0400390625, 87.11167907714844, 443.0439453125), Rect(90.47135162353516, 435.0400390625, 118.41556549072266, 443.0439453125)]\n",
            "keyword_impairment: Creatinine, coords: [Rect(45.355674743652344, 456.1600341796875, 80.04353332519531, 464.1639404296875)]\n",
            "keyword_impairment: Hypertriglyceridemia, coords: [Rect(139.1866455078125, 61.5999755859375, 222.74185180664062, 69.6038818359375), Rect(29.517169952392578, 171.280029296875, 101.29265594482422, 179.283935546875), Rect(74.87284851074219, 291.52001953125, 146.86593627929688, 299.52392578125), Rect(74.87281799316406, 725.6799926757812, 146.86590576171875, 733.6838989257812)]\n",
            "keyword_impairment: Calcium, coords: [Rect(44.395713806152344, 519.0400390625, 70.8531723022461, 527.0439453125)]\n",
            "keyword_impairment: ppointment, coords: [Rect(34.07672882080078, 79.1199951171875, 70.67321014404297, 87.1239013671875)]\n",
            "keyword_impairment: Chloride, coords: [Rect(45.355674743652344, 351.52001953125, 72.8730239868164, 359.52392578125)]\n",
            "keyword_impairment: AST, coords: [Rect(156.6650390625, 644.56005859375, 167.34400939941406, 652.56396484375)]\n",
            "keyword_impairment: Potassium, coords: [Rect(45.59565734863281, 330.6400146484375, 79.2884292602539, 338.6439208984375)]\n",
            "keyword_impairment: ALP, coords: [Rect(131.42750549316406, 623.6800537109375, 141.98651123046875, 631.6839599609375)]\n",
            "keyword_impairment: Bilirubin Total, coords: [Rect(45.59574890136719, 686.56005859375, 75.59286499023438, 694.56396484375), Rect(78.95254516601562, 686.56005859375, 94.35107421875, 694.56396484375)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ICD 10 Code"
      ],
      "metadata": {
        "id": "RIRH21XK258d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p pdf-files\n",
        "!mkdir -p txt-files"
      ],
      "metadata": {
        "id": "Ebs7E8X23lri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define directory path after creating it\n",
        "pdf_files_path = \"pdf-files\"\n",
        "txt_files_path = \"txt-files\"\n",
        "\n",
        "# create nlp instance\n",
        "nlp = English()\n",
        "\n",
        "\n",
        "def split_pdf(pdf_path):\n",
        "  pdf_in_file = open(pdf_path, \"rb\")\n",
        "  pdf = PdfReader(pdf_in_file)\n",
        "  pdf_list = []\n",
        "  for page in range(len(pdf.pages)):\n",
        "      inputpdf = PdfReader(pdf_in_file)\n",
        "      output = PdfWriter()\n",
        "      output.add_page(inputpdf.pages[page])\n",
        "      with open(f\"{pdf_files_path}/page-{page}.pdf\", \"wb\") as outputStream:\n",
        "          output.write(outputStream)\n",
        "          pdf_list.append(f\"page-{page}.pdf\")\n",
        "  return pdf_list\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_list):\n",
        "    txt_file_list = []\n",
        "    i = 0\n",
        "    for pdf_file in pdf_list:\n",
        "        with open(os.path.join(pdf_files_path, pdf_file), \"rb\") as f:\n",
        "            pdf = pdftotext.PDF(f)\n",
        "\n",
        "        # Read all the text into one string\n",
        "        pdf_text = \"\\n\\n\".join(pdf)\n",
        "\n",
        "        # write text into file\n",
        "        with open(f\"{txt_files_path}/page-{str(i)}.txt\", \"a\") as f:\n",
        "            f.write(pdf_text)\n",
        "        txt_file_list.append(f\"{txt_files_path}/page-{str(i)}.txt\")\n",
        "        i += 1\n",
        "    return txt_file_list\n",
        "\n",
        "\n",
        "def get_opt_pattern(icd_10_code):\n",
        "  # create alternate pattern\n",
        "  code_arr = icd_10_code.split(\".\")\n",
        "  if len(code_arr) > 1:\n",
        "    code1 = f\"{code_arr[0]}. {code_arr[1]}\"\n",
        "    code2 = f\"{code_arr[0]} .{code_arr[1]}\"\n",
        "    code3 = f\"{code_arr[0]} . {code_arr[1]}\"\n",
        "    return [code1, code2, code3]\n",
        "  else:\n",
        "    return icd_10_code\n",
        "\n",
        "\n",
        "def highlight_icd10_code(pdf_page_dict: dict, pdf_file_name: str):\n",
        "    pdf_file = fitz.open(pdf_file_name)\n",
        "\n",
        "    def highlight_pdf(highlight):\n",
        "        for inst in highlight:\n",
        "          highlight = page.add_highlight_annot(inst)\n",
        "          highlight.update()\n",
        "          highlight = page.search_for(text_to_be_highlighted)\n",
        "          print(f\"Page-{page_num}: \", code, highlight, end='\\n')\n",
        "\n",
        "    for page_num, page in enumerate(pdf_file):\n",
        "        if page_num in pdf_page_dict:\n",
        "          for code in pdf_page_dict[page_num]:\n",
        "            text_to_be_highlighted = code\n",
        "            highlight = page.search_for(text_to_be_highlighted)\n",
        "            print(f\"Page-{page_num}: \", code, highlight, end='\\n')\n",
        "            if len(highlight) == 0:\n",
        "                alternate_code_list = get_opt_pattern(code)\n",
        "                for alt_code in alternate_code_list:\n",
        "                  text_to_be_highlighted = alt_code\n",
        "                  highlight = page.search_for(text_to_be_highlighted)\n",
        "                  # highlight pdf for option pattern\n",
        "                  highlight_pdf(highlight)\n",
        "            # highlight pdf for main pattern\n",
        "            highlight_pdf(highlight)\n",
        "\n",
        "    output_pdf_file_name = f\"{pdf_file_name.split('.')[0]}_output.pdf\"\n",
        "    pdf_file.save(output_pdf_file_name, garbage=4, deflate=True, clean=True)\n",
        "    return output_pdf_file_name\n",
        "\n",
        "\n",
        "def search_icd_10_code(txt_list):\n",
        "  pdf_page_vocab = {}\n",
        "  for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "      page_txt = f.read()\n",
        "      # filter the page that have line number instead of code\n",
        "      if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "        doc = nlp(page_txt)\n",
        "        code_list = [ent.text for ent in doc.ents]\n",
        "        if len(code_list) != 0:\n",
        "          #print(txt_file)\n",
        "          page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "          pdf_page_vocab[page_number] = code_list\n",
        "          # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "  return pdf_page_vocab"
      ],
      "metadata": {
        "id": "snzCZDEN29Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-1: splitting pdf file\n",
        "pdf_file_name = \"28page.pdf\"\n",
        "pdf_list = split_pdf(pdf_file_name)\n",
        "\n",
        "# Step-2: Extracting text from pdf\n",
        "txt_list = extract_text_from_pdf(pdf_list)\n",
        "\n",
        "# Step-3: loading and updating patterns to Spacy\n",
        "nlp.add_pipe(\"entity_ruler\").from_disk(\"./icd10_code_patterns-v1.jsonl\")\n",
        "\n",
        "# Step-4: Searching ICD-10 code\n",
        "#print (txt_list)\n",
        "pdf_page_vocab = search_icd_10_code(txt_list)\n",
        "\n",
        "# Step-5: Highlighting ICD-10 code into pdf\n",
        "output_file_name = highlight_icd10_code(pdf_page_vocab, pdf_file_name)\n",
        "print(f\"File[{output_file_name}] is saved after highlighting ICD-10 code\")"
      ],
      "metadata": {
        "id": "JPlZD1rj3D0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Phrase matching"
      ],
      "metadata": {
        "id": "3T0ApWNLUV1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher"
      ],
      "metadata": {
        "id": "ftahgHKBUYC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synid_df = pd.read_csv(cfg.csv_files[\"IMP_CSV\"])\n",
        "synid_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-Wmq-wtxVDUU",
        "outputId": "8a90f772-8cfc-40cc-9ae9-40a06a6f54c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     SynId                          Short_Description\n",
              "0  KW20262                                 US thyroid\n",
              "1  KW20261          no graphic evidence of malignancy\n",
              "2  KW20260                  no evidence of malignancy\n",
              "3  KW20259  scattered areas of fibroglandular density\n",
              "4  KW20258  scattered areas of fibroglandular density"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-9fb8b197-cd4b-4965-8cbe-09f68f09f80a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SynId</th>\n",
              "      <th>Short_Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KW20262</td>\n",
              "      <td>US thyroid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KW20261</td>\n",
              "      <td>no graphic evidence of malignancy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KW20260</td>\n",
              "      <td>no evidence of malignancy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KW20259</td>\n",
              "      <td>scattered areas of fibroglandular density</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KW20258</td>\n",
              "      <td>scattered areas of fibroglandular density</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9fb8b197-cd4b-4965-8cbe-09f68f09f80a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-2a7ea9b6-8911-4b0e-b56b-5a6b6e654afc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a7ea9b6-8911-4b0e-b56b-5a6b6e654afc')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-2a7ea9b6-8911-4b0e-b56b-5a6b6e654afc button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9fb8b197-cd4b-4965-8cbe-09f68f09f80a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9fb8b197-cd4b-4965-8cbe-09f68f09f80a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_icd_10_keyword_pattern(synid_df):\n",
        "  patterns = []\n",
        "  for _, row in synid_df.iterrows():\n",
        "    patterns.append(row[\"Short_Description\"])\n",
        "  return patterns"
      ],
      "metadata": {
        "id": "S25-C5AZWucL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = make_icd_10_keyword_pattern(synid_df)\n",
        "keywords[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STbmzWgwW8hG",
        "outputId": "59d7699f-89a1-40dc-c345-1b90c12c5d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['US thyroid',\n",
              " 'no graphic evidence of malignancy',\n",
              " 'no evidence of malignancy',\n",
              " 'scattered areas of fibroglandular density',\n",
              " 'scattered areas of fibroglandular density',\n",
              " 'BI-RADS 6',\n",
              " 'BI-RADS 5',\n",
              " 'BI-RADS 4C',\n",
              " 'BI-RADS 4B',\n",
              " 'BI-RADS 4A']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
        "patterns = list(nlp.tokenizer.pipe(keywords))\n",
        "phrase_matcher.add('keywords', patterns)"
      ],
      "metadata": {
        "id": "W0a69f63Y-5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-2.txt\", \"r\") as f:\n",
        "  page_txt = f.read()\n",
        "  # filter the page that have line number instead of code\n",
        "  #if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "  doc = nlp(page_txt)\n",
        "  matches = phrase_matcher(doc)\n",
        "\n",
        "  keyword_list = []\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start: end]\n",
        "    keyword_list.append(f\"{span}\")"
      ],
      "metadata": {
        "id": "W21nm7_VXbg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-HCX2ysX3H8",
        "outputId": "0dcb864c-fa0f-49fc-86ad-84a2a89045a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T', 'HHS', 'US']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-25.txt\", \"r\") as f:\n",
        "  page_txt = f.read()\n",
        "  # filter the page that have line number instead of code\n",
        "  #if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "  doc = nlp(page_txt)\n",
        "  matches = phrase_matcher(doc)\n",
        "\n",
        "  keyword_list = []\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start: end]\n",
        "    keyword_list.append(f\"{span}\")"
      ],
      "metadata": {
        "id": "Ga99oXcsYeZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB-D0G4FZ6sS",
        "outputId": "2e42c010-58c1-4973-82e0-97b5d832ac5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hypertriglyceridemia',\n",
              " 'Hypertriglyceridemia',\n",
              " 'EKG',\n",
              " 'Hypertension',\n",
              " 'Atrial Fibrillation',\n",
              " 'Headache']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-36.txt\", \"r\") as f:\n",
        "  page_txt = f.read()\n",
        "  # filter the page that have line number instead of code\n",
        "  #if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "  doc = nlp(page_txt)\n",
        "  matches = phrase_matcher(doc)\n",
        "\n",
        "  keyword_list = []\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start: end]\n",
        "    keyword_list.append(f\"{span}\")"
      ],
      "metadata": {
        "id": "-W4RdswabV3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJgGec4AbYER",
        "outputId": "7e40ee89-461f-487c-fd2e-2efe2c9a78b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hypertriglyceridemia',\n",
              " 'Hypertriglyceridemia',\n",
              " 'EKG',\n",
              " 'Hypertension',\n",
              " 'Atrial Fibrillation',\n",
              " 'Diplopia',\n",
              " 'Headache',\n",
              " 'Visual Loss',\n",
              " 'Gynecomastia',\n",
              " 'Dysphagia',\n",
              " 'Hematuria']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synid_df = pd.read_csv(\"synid_and_keywords_impairment.csv\")\n",
        "keywords = make_icd_10_keyword_pattern(synid_df)"
      ],
      "metadata": {
        "id": "xEqoN5-yidiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
        "patterns = list(nlp.tokenizer.pipe(keywords))\n",
        "phrase_matcher.add('keywords', patterns)"
      ],
      "metadata": {
        "id": "-G151LL_ij0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-36.txt\", \"r\") as f:\n",
        "  page_txt = f.read()\n",
        "  # filter the page that have line number instead of code\n",
        "  #if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "  doc = nlp(page_txt)\n",
        "  matches = phrase_matcher(doc)\n",
        "\n",
        "  keyword_list = []\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start: end]\n",
        "    keyword_list.append(f\"{span}\")"
      ],
      "metadata": {
        "id": "xmNbJ187irjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4toMzolitCG",
        "outputId": "7cdc57c5-fb3e-439a-8fe1-23be3ac2a46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hypertriglyceridemia',\n",
              " 'Hypertriglyceridemia',\n",
              " 'EKG',\n",
              " 'Hypertension',\n",
              " 'Atrial Fibrillation',\n",
              " 'Diplopia',\n",
              " 'Headache',\n",
              " 'Visual Loss',\n",
              " 'Gynecomastia',\n",
              " 'Dysphagia',\n",
              " 'Hematuria',\n",
              " 'Headaches']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_keyword_dict1[30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3bdIYhJgbNz",
        "outputId": "a0c6ff4c-9446-4837-a735-1753ea56e093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ALP',\n",
              " 'ALT',\n",
              " 'AST',\n",
              " 'Albumin',\n",
              " 'Alkaline Phosphatase',\n",
              " 'Anion Gap',\n",
              " 'Aspartate',\n",
              " 'BUN',\n",
              " 'Bilirubin Total',\n",
              " 'Blood Urea Nitrogen',\n",
              " 'CBC',\n",
              " 'CMP',\n",
              " 'CO2',\n",
              " 'Calcium',\n",
              " 'Chloride',\n",
              " 'Complete Blood Count',\n",
              " 'Comprehensive Metabolic Panel',\n",
              " 'Creatinine',\n",
              " 'Encounter',\n",
              " 'GFR',\n",
              " 'Glucose',\n",
              " 'Hypertriglyceridemia',\n",
              " 'Lipid Panel',\n",
              " 'MI',\n",
              " 'Potassium',\n",
              " 'Sodium',\n",
              " 'Urea',\n",
              " 'Urea Nitrogen',\n",
              " 'ppointment'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = [keyword for keyword in wrong_keyword_dict1[30]]\n",
        "len(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jiz1Nlji4t6",
        "outputId": "54ef3417-8146-46fe-d8cd-679a8c13005c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(set(keywords)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACLV0LxtkWji",
        "outputId": "be133bd8-2894-441a-8c07-2ae2281bcca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "fisKIpBvjLdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
        "patterns = list(nlp.tokenizer.pipe(keywords))\n",
        "phrase_matcher.add('keywords', patterns)"
      ],
      "metadata": {
        "id": "LLFgaNg-jJF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-30.txt\", \"r\") as f:\n",
        "  page_txt = f.read()\n",
        "  # filter the page that have line number instead of code\n",
        "  #if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "  doc = nlp(page_txt)\n",
        "  matches = phrase_matcher(doc)\n",
        "\n",
        "  keyword_list = []\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start: end]\n",
        "    keyword_list.append(f\"{span}\")\n",
        "\n",
        "print(len(keyword_list))\n",
        "len(list(set(keyword_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJcvrEv1jVtm",
        "outputId": "99084aaa-5ce1-4fd3-9d4c-f30f916dadfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "keywords = [keyword for keyword in wrong_keyword_dict1[30]]\n",
        "pattern1 = [{'ORTH': keywords}]\n",
        "phrase_matcher.add('keywords', None, pattern1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "9fDJwzw3gg2W",
        "outputId": "80526fd2-bf26-4ca9-87cb-5933bf18ae87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-13a45877d65f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwrong_keyword_dict1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpattern1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mphrase_matcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keywords'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/matcher/phrasematcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.phrasematcher.PhraseMatcher.add\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
          ]
        }
      ]
    }
  ]
}