{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0hK4nU35JVaA",
        "N4dDTs-rJL_h",
        "TycHgKTZJ6a6"
      ],
      "authorship_tag": "ABX9TyMRHIK8YT/MBzsWPO+AJAQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/nlp-research-and-practice/blob/main/text-similarity-works/13_icd_10_code_highlight_with_keyword_match_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "u6vazPC0Ja4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "!pip install pillow\n",
        "\n",
        "!sudo apt install build-essential libpoppler-cpp-dev pkg-config python3-dev\n",
        "!pip install -U pdftotext\n",
        "!pip install PyPDF2\n",
        "!pip install fitz\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "gdwgVvXuJdz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "id": "4QPcnYT5AKpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import difflib\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "import fitz\n",
        "import pdftotext\n",
        "from PyPDF2 import PdfFileReader, PdfReader, PdfFileWriter, PdfWriter\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.lang.en import English\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from string import punctuation\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process"
      ],
      "metadata": {
        "id": "gKbd7WsfyYX3",
        "outputId": "b19bf69d-ed43-4982-889b-a95ebeeae49d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "/usr/local/lib/python3.8/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Imv6fB5gPBe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p input_files"
      ],
      "metadata": {
        "id": "2dfljcHuOVw3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Core Classes"
      ],
      "metadata": {
        "id": "9oNqjgIsRpNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Highlighter:\n",
        "  def __init__(self, code_df):\n",
        "      # loading and updating patterns for ICD-10 code\n",
        "      self.nlp_code10 = English()\n",
        "      self.nlp_code10.add_pipe(\"entity_ruler\").from_disk(\"icd10_code_patterns-v5.jsonl\")\n",
        "\n",
        "      # define icd-10 code dataset\n",
        "      self.code_df = code_df\n",
        "      self.text_list = None\n",
        "\n",
        "      # define required directory path\n",
        "      self.PDF_FILES_PATH = \"pdf-files\"\n",
        "      self.TXT_FILES_PATH = \"txt-files\"\n",
        "      self.OUTPUT_FILES_PATH = \"output\"\n",
        "      create_directory(self.PDF_FILES_PATH)\n",
        "      create_directory(self.TXT_FILES_PATH)\n",
        "      create_directory(self.OUTPUT_FILES_PATH)\n",
        "\n",
        "  def split_pdf(self, pdf_path):\n",
        "      pdf_in_file = open(pdf_path, \"rb\")\n",
        "      pdf = PdfReader(pdf_in_file)\n",
        "      pdf_list = []\n",
        "      for page in range(len(pdf.pages)):\n",
        "          input_pdf = PdfReader(pdf_in_file)\n",
        "          output = PdfWriter()\n",
        "          #output.addPage(input_pdf.getPage(page))\n",
        "          output.add_page(input_pdf.pages[page])\n",
        "          with open(f\"{self.PDF_FILES_PATH}/page-{page}.pdf\", \"wb\") as outputStream:\n",
        "              output.write(outputStream)\n",
        "              pdf_list.append(f\"page-{page}.pdf\")\n",
        "      return pdf_list\n",
        "\n",
        "  def extract_text_from_pdf(self, pdf_list):\n",
        "    txt_file_list = []\n",
        "    i = 0\n",
        "    for pdf_file in pdf_list:\n",
        "      with open(os.path.join(self.PDF_FILES_PATH, pdf_file), \"rb\") as f:\n",
        "        pdf = pdftotext.PDF(f)\n",
        "\n",
        "      # Read all the text into one string\n",
        "      pdf_text = \"\\n\\n\".join(pdf)\n",
        "\n",
        "      # write text into file\n",
        "      with open(f\"{self.TXT_FILES_PATH}/page-{str(i)}.txt\", \"a\") as f:\n",
        "        f.write(pdf_text)\n",
        "      txt_file_list.append(f\"{self.TXT_FILES_PATH}/page-{str(i)}.txt\")\n",
        "      i += 1\n",
        "    self.text_list = txt_file_list\n",
        "    return txt_file_list\n",
        "\n",
        "  def highlight_icd_code(self, icd10_code_dict, pdf_file_name=None, cords_file_name=None):\n",
        "      pdf_file = fitz.open(pdf_file_name)\n",
        "      # create file to write coordinate\n",
        "      txt_output_file_name = open(f\"{self.OUTPUT_FILES_PATH}/{cords_file_name}\", \"a\")\n",
        "\n",
        "      def highlight_pdf(highlight, icd10_code, num_page):\n",
        "        cords_list = []\n",
        "        match_list = []\n",
        "        sentence_list = []\n",
        "        keyword = \"\"\n",
        "\n",
        "        for inst in highlight:\n",
        "          # do the color coding\n",
        "          keyword = self.get_keyword(icd10_code)\n",
        "          if len(keyword) > 0:\n",
        "            match_list, sentence_list = self.get_best_match(icd10_code, num_page)\n",
        "            # highlight code if threshold is more than 40\n",
        "            if match_list[0][1] > 40:\n",
        "              highlight = page.add_highlight_annot(inst)\n",
        "              highlight.set_colors(stroke=[0.66, 1, 0.07])  # light green\n",
        "              highlight.update()\n",
        "              highlight = page.search_for(icd10_code)\n",
        "              cords_list.append(highlight)\n",
        "            \n",
        "        if cords_list:\n",
        "          num_page = page_num + 1\n",
        "          code_cors_output = f\"| Page # | Code | Code Line # | ICD 10 description | Matched Keyword | Matched Keyword Line # | confidence |\"\n",
        "          for idx, match_found in enumerate(match_list):\n",
        "            if match_found[1] > 40:\n",
        "              # | Page # | Code | Code Line # | ICD 10 description | Matched Keyword | Matched Keyword Line # | confidence |\n",
        "              code_cors_output = f\"|Page-{num_page} | {icd10_code}~{reverse_code_pattern(icd10_code)} | {keyword if keyword else 'Not available'} | {match_found[0]}  | {sentence_list.index(match_found[0])} | {match_found[1]}\"\n",
        "              txt_output_file_name.write(\"%s\\n\" % code_cors_output)\n",
        "\n",
        "      for page_num, page in enumerate(pdf_file):\n",
        "        # highlight ICD-10 code\n",
        "        if page_num in icd10_code_dict:\n",
        "          for code in icd10_code_dict[page_num]:\n",
        "            highlight = page.search_for(code)\n",
        "            if len(highlight) == 0:\n",
        "              alternate_code_list = self.get_opt_pattern(code)\n",
        "              \n",
        "              for alt_code in alternate_code_list:\n",
        "                highlight = page.search_for(alt_code)\n",
        "                # highlight pdf for option pattern\n",
        "                highlight_pdf(highlight, alt_code, page_num)\n",
        "            # highlight pdf for main pattern\n",
        "            highlight_pdf(highlight, code, page_num)\n",
        "\n",
        "      txt_output_file_name.close()\n",
        "\n",
        "      pdf_output_file_name = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}_output.pdf\"\n",
        "      pdf_file.save(pdf_output_file_name, garbage=4, deflate=True, clean=True)\n",
        "\n",
        "      return pdf_output_file_name, cords_file_name\n",
        "\n",
        "  def get_opt_pattern(self, icd_10_code):\n",
        "    # create alternate pattern\n",
        "    code_arr = icd_10_code.split(\".\")\n",
        "    if len(code_arr) > 1:\n",
        "      code1 = f\"{code_arr[0]}. {code_arr[1]}\"\n",
        "      code2 = f\"{code_arr[0]} .{code_arr[1]}\"\n",
        "      code3 = f\"{code_arr[0]} . {code_arr[1]}\"\n",
        "      return [code1, code2, code3]\n",
        "    else:\n",
        "      return icd_10_code\n",
        "\n",
        "  def search_icd_code(self, txt_list):\n",
        "    pdf_page_vocab = {}\n",
        "    for txt_file in txt_list:\n",
        "      with open(txt_file, \"r\") as f:\n",
        "        page_txt = f.read()\n",
        "\n",
        "        # check the page that have line number instead of code\n",
        "        index_page = False\n",
        "        if re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "          index_page = True\n",
        "\n",
        "        doc = self.nlp_code10(page_txt)\n",
        "        code_list = []\n",
        "        for ent in doc.ents:\n",
        "          if index_page:\n",
        "            # check the code contain letter \"L\"\n",
        "            if re.search(\"(L[0-9]+)\", ent.text):\n",
        "              continue\n",
        "            else:\n",
        "              code_list.append(ent.text)\n",
        "          else:\n",
        "            code_list.append(ent.text)\n",
        "\n",
        "        #code_list = [ent.text for ent in doc.ents if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", ent.text)]\n",
        "        if len(code_list) != 0:\n",
        "          page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "          pdf_page_vocab[page_number] = list(set(code_list)) \n",
        "          # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "    return pdf_page_vocab\n",
        "\n",
        "  def get_keyword(self, p_code):\n",
        "    keyword = \"\"\n",
        "    # reverse code if required\n",
        "    code = reverse_code_pattern(p_code)\n",
        "    # get keyword from dataset\n",
        "    keyword_list = list(self.code_df.loc[self.code_df[\"Code\"] == code][\"Keyword\"])\n",
        "    if len(keyword_list) > 0:\n",
        "      keyword = keyword_list[0]\n",
        "    return keyword\n",
        "\n",
        "  def get_best_match(self, p_code, num_page):\n",
        "    # Step 1: reverse code pattern\n",
        "    reversed_icd_code = reverse_code_pattern(p_code)\n",
        "    # Step 2: fetch keyword based on code \n",
        "    keyword = self.get_keyword(reversed_icd_code)\n",
        "    # Step 3: prepare sentence list \n",
        "    with open(f\"{self.text_list[num_page]}\", \"r\") as f:\n",
        "      lines = [line.rstrip('\\n') for line in f]\n",
        "      sentence_list = [\" \".join(line.split()) for line in lines] # Remove extra spaces, tabs, and line breaks\n",
        "    # Step 4: get best 3 match ratio \n",
        "    match_list = process.extract(keyword, sentence_list, scorer = fuzz.ratio, limit = 3)\n",
        "    return match_list, sentence_list\n",
        "\n",
        "def reverse_code_pattern(p_code):\n",
        "  orig_code = p_code\n",
        "\n",
        "  # check for code contains space(\" \")\n",
        "  tmp_code = orig_code.split(\" \")\n",
        "  if len(tmp_code) > 1:\n",
        "    orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "\n",
        "  # check for code contains dot(\".\")\n",
        "  tmp_code = p_code.split(\".\")\n",
        "  if len(tmp_code) > 1:\n",
        "    orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "  \n",
        "  # check for code contains comma(\",\")\n",
        "  tmp_code = p_code.split(\",\")\n",
        "  if len(tmp_code) == 2:\n",
        "    orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "  elif len(tmp_code) == 2:\n",
        "    orig_code = f\"{tmp_code[0].strip()}.{tmp_code[2].strip()}\"\n",
        "\n",
        "  # handle if the first char of code is missing\n",
        "  alphabats = {\"Z\": \"2\", \"B\": \"8\", \"O\": \"0\", \"S\": \"5\", \"l\": \"1\", \"G\": \"6\", \"o\": \"9\", \"i\": \"1\"}\n",
        "  for key, val in alphabats.items():\n",
        "    # replcae char on 0 index\n",
        "    if orig_code.find(val) == 0:\n",
        "      #orig_code = orig_code.replace(val, key)\n",
        "      orig_code = replacer(orig_code, key, 0)\n",
        "    # replcae char on 1 index\n",
        "    if orig_code.find(key) == 1:\n",
        "      orig_code = replacer(orig_code, val, 1)\n",
        "      # replcae char on 2 index\n",
        "      if orig_code.find(key) == 2:\n",
        "        orig_code = replacer(orig_code, val, 2)\n",
        "      break\n",
        "\n",
        "  return orig_code\n",
        "\n",
        "def replacer(s, newstring, index, nofail=False):\n",
        "  # raise an error if index is outside of the string\n",
        "  if not nofail and index not in range(len(s)):\n",
        "      raise ValueError(\"index outside given string\")\n",
        "\n",
        "  # if not erroring, but the index is still not in the correct range..\n",
        "  if index < 0:  # add it to the beginning\n",
        "      return newstring + s\n",
        "  if index > len(s):  # add it to the end\n",
        "      return s + newstring\n",
        "\n",
        "  # insert the new string between \"slices\" of the original\n",
        "  return s[:index] + newstring + s[index + 1:]\n",
        "\n",
        "def create_directory(dir_name):\n",
        "  if not os.path.exists(dir_name):\n",
        "    os.makedirs(dir_name)"
      ],
      "metadata": {
        "id": "jRYlndgDUtds"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Highlighter2:\n",
        "  def __init__(self, code_df):\n",
        "      # loading and updating patterns for ICD-10 code\n",
        "      self.nlp_code10 = English()\n",
        "      self.nlp_code10.add_pipe(\"entity_ruler\").from_disk(\"icd10_code_patterns-v5.jsonl\")\n",
        "\n",
        "      # define icd-10 code dataset\n",
        "      self.code_df = code_df\n",
        "      self.text_list = None\n",
        "\n",
        "      # define required directory path\n",
        "      self.PDF_FILES_PATH = \"pdf-files\"\n",
        "      self.TXT_FILES_PATH = \"txt-files\"\n",
        "      self.OUTPUT_FILES_PATH = \"output\"\n",
        "      create_directory(self.PDF_FILES_PATH)\n",
        "      create_directory(self.TXT_FILES_PATH)\n",
        "      create_directory(self.OUTPUT_FILES_PATH)\n",
        "\n",
        "\n",
        "  def split_pdf(self, pdf_path):\n",
        "      pdf_in_file = open(pdf_path, \"rb\")\n",
        "      pdf = PdfReader(pdf_in_file)\n",
        "      pdf_list = []\n",
        "      for page in range(len(pdf.pages)):\n",
        "          input_pdf = PdfReader(pdf_in_file)\n",
        "          output = PdfWriter()\n",
        "          #output.addPage(input_pdf.getPage(page))\n",
        "          output.add_page(input_pdf.pages[page])\n",
        "          with open(f\"{self.PDF_FILES_PATH}/page-{page}.pdf\", \"wb\") as outputStream:\n",
        "              output.write(outputStream)\n",
        "              pdf_list.append(f\"page-{page}.pdf\")\n",
        "      return pdf_list\n",
        "\n",
        "  def extract_text_from_pdf(self, pdf_list):\n",
        "    txt_file_list = []\n",
        "    i = 0\n",
        "    for pdf_file in pdf_list:\n",
        "      with open(os.path.join(self.PDF_FILES_PATH, pdf_file), \"rb\") as f:\n",
        "        pdf = pdftotext.PDF(f)\n",
        "\n",
        "      # Read all the text into one string\n",
        "      pdf_text = \"\\n\\n\".join(pdf)\n",
        "\n",
        "      # write text into file\n",
        "      with open(f\"{self.TXT_FILES_PATH}/page-{str(i)}.txt\", \"a\") as f:\n",
        "        f.write(pdf_text)\n",
        "      txt_file_list.append(f\"{self.TXT_FILES_PATH}/page-{str(i)}.txt\")\n",
        "      i += 1\n",
        "    self.text_list = txt_file_list\n",
        "    return txt_file_list\n",
        "\n",
        "  def highlight_icd_code(self, icd10_code_dict, pdf_file_name=None, cords_file_name=None):\n",
        "      pdf_file = fitz.open(pdf_file_name)\n",
        "      # create file to write coordinate\n",
        "      txt_output_file_name = open(f\"{self.OUTPUT_FILES_PATH}/{cords_file_name}\", \"a\")\n",
        "\n",
        "      def highlight_pdf(highlight, icd10_code, num_page):\n",
        "          cords_list = []\n",
        "          keyword = \"\"\n",
        "          score = 0.0\n",
        "          for inst in highlight:\n",
        "            highlight = page.add_highlight_annot(inst)\n",
        "\n",
        "            # do the color coding\n",
        "            keyword = self.get_keyword(icd10_code)\n",
        "            if len(keyword) > 0:\n",
        "              score = get_similarity_score(keyword, self.text_list[num_page])\n",
        "              if score > 0.10:\n",
        "                highlight.set_colors(stroke=[0.66, 1, 0.07])  # light green\n",
        "              else:\n",
        "                highlight.set_colors(stroke=[1, 0.8, 0.8])  # light red\n",
        "              \"\"\"\n",
        "              if score > 0.50:\n",
        "                highlight.set_colors(stroke=[0.66, 1, 0.07])  # light green\n",
        "              else:\n",
        "                highlight.set_colors(stroke=[0.5, 1, 1])  # light aqua\n",
        "              \"\"\"\n",
        "            \n",
        "            highlight.update()\n",
        "            highlight = page.search_for(icd10_code)\n",
        "            cords_list.append(highlight)\n",
        "\n",
        "          if cords_list:\n",
        "            num_page = page_num + 1\n",
        "            # code_cors_output = f\"Page-{num_page} | {icd10_code} | {reverse_code_pattern(icd10_code)} | {keyword} | {score}  | {cords_list} \\n\"\n",
        "            code_cors_output = f\"Page-{num_page} | {icd10_code} | {reverse_code_pattern(icd10_code)} | {keyword if keyword else 'Not available'} | {round(score, 3)}  | \\n\"\n",
        "            txt_output_file_name.write(\"%s\\n\" % code_cors_output)\n",
        "\n",
        "      for page_num, page in enumerate(pdf_file):\n",
        "        # highlight ICD-10 code\n",
        "        if page_num in icd10_code_dict:\n",
        "          for code in icd10_code_dict[page_num]:\n",
        "            highlight = page.search_for(code)\n",
        "            if len(highlight) == 0:\n",
        "              alternate_code_list = self.get_opt_pattern(code)\n",
        "              \n",
        "              for alt_code in alternate_code_list:\n",
        "                highlight = page.search_for(alt_code)\n",
        "                # highlight pdf for option pattern\n",
        "                highlight_pdf(highlight, alt_code, page_num)\n",
        "            # highlight pdf for main pattern\n",
        "            highlight_pdf(highlight, code, page_num)\n",
        "\n",
        "      txt_output_file_name.close()\n",
        "\n",
        "      pdf_output_file_name = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}_output.pdf\"\n",
        "      pdf_file.save(pdf_output_file_name, garbage=4, deflate=True, clean=True)\n",
        "\n",
        "      return pdf_output_file_name, cords_file_name\n",
        "\n",
        "  def get_opt_pattern(self, icd_10_code):\n",
        "    # create alternate pattern\n",
        "    code_arr = icd_10_code.split(\".\")\n",
        "    if len(code_arr) > 1:\n",
        "      code1 = f\"{code_arr[0]}. {code_arr[1]}\"\n",
        "      code2 = f\"{code_arr[0]} .{code_arr[1]}\"\n",
        "      code3 = f\"{code_arr[0]} . {code_arr[1]}\"\n",
        "      return [code1, code2, code3]\n",
        "    else:\n",
        "      return icd_10_code\n",
        "\n",
        "  def search_icd_code(self, txt_list):\n",
        "    pdf_page_vocab = {}\n",
        "    for txt_file in txt_list:\n",
        "      with open(txt_file, \"r\") as f:\n",
        "        page_txt = f.read()\n",
        "\n",
        "        # check the page that have line number instead of code\n",
        "        index_page = False\n",
        "        if re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "          index_page = True\n",
        "\n",
        "        doc = self.nlp_code10(page_txt)\n",
        "        code_list = []\n",
        "        for ent in doc.ents:\n",
        "          if index_page:\n",
        "            # check the code contain letter \"L\"\n",
        "            if re.search(\"(L[0-9]+)\", ent.text):\n",
        "              continue\n",
        "            else:\n",
        "              code_list.append(ent.text)\n",
        "          else:\n",
        "            code_list.append(ent.text)\n",
        "\n",
        "        #code_list = [ent.text for ent in doc.ents if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", ent.text)]\n",
        "        if len(code_list) != 0:\n",
        "            page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "            pdf_page_vocab[page_number] = list(set(code_list)) \n",
        "            # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "    return pdf_page_vocab\n",
        "\n",
        "  def get_keyword(self, p_code):\n",
        "    keyword = \"\"\n",
        "    # reverse code if required\n",
        "    code = reverse_code_pattern(p_code)\n",
        "    # get keyword from dataset\n",
        "    keyword_list = list(self.code_df.loc[self.code_df[\"Code\"] == code][\"Keyword\"])\n",
        "    if len(keyword_list) > 0:\n",
        "      keyword = keyword_list[0]\n",
        "    return keyword\n",
        "\n",
        "def get_similarity_score(keyword, text_file):\n",
        "  # load text file\n",
        "  with open(text_file, \"r\") as f:\n",
        "    my_text = f.read()\n",
        "\n",
        "  # prepare key phrase\n",
        "  key_phrase_list = []\n",
        "  for textlist in my_text.split(\"\\n\"):\n",
        "    for key_phrase in textlist.split(\",\"):\n",
        "      if len(key_phrase) > 0:\n",
        "        key_phrase_list.append(key_phrase)\n",
        "        \n",
        "  # return max similarity score\n",
        "  return max([SequenceMatcher(None, k_phrase, keyword).ratio() for k_phrase in key_phrase_list])\n",
        "\n",
        "def reverse_code_pattern(p_code):\n",
        "  orig_code = p_code\n",
        "\n",
        "  # check for code contains space(\" \")\n",
        "  tmp_code = orig_code.split(\" \")\n",
        "  if len(tmp_code) > 1:\n",
        "    orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "\n",
        "  # check for code contains dot(\".\")\n",
        "  tmp_code = p_code.split(\".\")\n",
        "  if len(tmp_code) > 1:\n",
        "    orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "  \n",
        "  # check for code contains comma(\",\")\n",
        "  tmp_code = p_code.split(\",\")\n",
        "  if len(tmp_code) == 2:\n",
        "    orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "  elif len(tmp_code) == 2:\n",
        "    orig_code = f\"{tmp_code[0].strip()}.{tmp_code[2].strip()}\"\n",
        "\n",
        "  # handle if the first char of code is missing\n",
        "  alphabats = {\"Z\": \"2\", \"B\": \"8\", \"O\": \"0\", \"S\": \"5\", \"l\": \"1\", \"G\": \"6\", \"o\": \"9\", \"i\": \"1\"}\n",
        "  for key, val in alphabats.items():\n",
        "    # replcae char on 0 index\n",
        "    if orig_code.find(val) == 0:\n",
        "      #orig_code = orig_code.replace(val, key)\n",
        "      orig_code = replacer(orig_code, key, 0)\n",
        "    # replcae char on 1 index\n",
        "    if orig_code.find(key) == 1:\n",
        "      orig_code = replacer(orig_code, val, 1)\n",
        "      # replcae char on 2 index\n",
        "      if orig_code.find(key) == 2:\n",
        "        orig_code = replacer(orig_code, val, 2)\n",
        "      break\n",
        "\n",
        "  return orig_code\n",
        "\n",
        "def replacer(s, newstring, index, nofail=False):\n",
        "  # raise an error if index is outside of the string\n",
        "  if not nofail and index not in range(len(s)):\n",
        "      raise ValueError(\"index outside given string\")\n",
        "\n",
        "  # if not erroring, but the index is still not in the correct range..\n",
        "  if index < 0:  # add it to the beginning\n",
        "      return newstring + s\n",
        "  if index > len(s):  # add it to the end\n",
        "      return s + newstring\n",
        "\n",
        "  # insert the new string between \"slices\" of the original\n",
        "  return s[:index] + newstring + s[index + 1:]\n",
        "\n",
        "def create_directory(dir_name):\n",
        "  if not os.path.exists(dir_name):\n",
        "    os.makedirs(dir_name)"
      ],
      "metadata": {
        "id": "W0oEZzP_RrlP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Class-based Searching & Highlighting"
      ],
      "metadata": {
        "id": "k14TfaJgX-y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def purge(file_path):\n",
        "  for f in glob.glob(file_path):\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "-gz3UJ9AJcaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-0: Define prerequisite instance\n",
        "INPUT_PDF_FILES_PATH = \"input_pdf_files_path\"\n",
        "\n",
        "highlighter = Highlighter()"
      ],
      "metadata": {
        "id": "y20Jew29JiqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for pdf_file in os.listdir(INPUT_PDF_FILES_PATH):\n",
        "  pdf_file_name = f\"{INPUT_PDF_FILES_PATH}/{pdf_file}\"\n",
        "  cords_file_name = f\"{pdf_file_name.split('/')[1].split('.')[0]}_cords.txt\"\n",
        "\n",
        "  # Step-1: splitting pdf file\n",
        "  pdf_list = highlighter.split_pdf(pdf_file_name)\n",
        "\n",
        "  # Step-2: Extracting text from pdf\n",
        "  txt_list = highlighter.extract_text_from_pdf(pdf_list)\n",
        "\n",
        "  # Step-3: Searching ICD-10 cod\n",
        "  icd10_code_dict = highlighter.search_icd_code(txt_list)\n",
        "\n",
        "  # Step-4: Highlighting ICD-10 code into pdf\n",
        "  pdf_output_file, txt_output_file = highlighter.highlight_icd_code(icd10_code_dict,\n",
        "                                                                    pdf_file_name=pdf_file_name,\n",
        "                                                                    cords_file_name=cords_file_name)\n",
        "  print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "  print(f\"Highlighted coordinates are saved into [{txt_output_file}] file.\")\n",
        "\n",
        "  # remove all pdf and text files\n",
        "  purge(\"pdf-files/*.pdf\")\n",
        "  purge(\"txt-files/*.txt\")\n",
        "  pdf_list = []\n",
        "  txt_list = []"
      ],
      "metadata": {
        "id": "8Jebcbz4YChR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf input_pdf_files_path\n",
        "!mkdir -p input_pdf_files_path\n",
        "!rm -rf output"
      ],
      "metadata": {
        "id": "gsxg17J0kLdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf output"
      ],
      "metadata": {
        "id": "7biDWeNPmHXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip output.zip output/*.*"
      ],
      "metadata": {
        "id": "OF-Bc6axsumx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keyword Matching & Highlighting "
      ],
      "metadata": {
        "id": "7s7ZxYMUBAol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Step 1 - Z87.5\n",
        "- Step 2 - Personal history of complications of pregnancy, childbirth and the puerperium\n",
        "- Step 3 - Page keyword\n",
        "- Step 4 - calculate cosine similirity\n",
        "- Step 5 - \"Green\" > 60% otherwise \"Yellow\""
      ],
      "metadata": {
        "id": "9KDyJlx9bIH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def purge(file_path):\n",
        "  for f in glob.glob(file_path):\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "32V_ohLAgvLg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-0: create highlighter instance\n",
        "INPUT_PDF_FILES_PATH = \"input_files\"\n",
        "code_df = pd.read_csv(\"icd_10_code_and_keywords_v2.csv\")\n",
        "\n",
        "highlighter = Highlighter(code_df)"
      ],
      "metadata": {
        "id": "leBmw505CQUl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for pdf_file in os.listdir(INPUT_PDF_FILES_PATH):\n",
        "  pdf_file_name = f\"{INPUT_PDF_FILES_PATH}/{pdf_file}\"\n",
        "  cords_file_name = f\"{pdf_file_name.split('/')[1].split('.')[0]}_cords.txt\"\n",
        "\n",
        "  # Step-1: splitting pdf file\n",
        "  pdf_list = highlighter.split_pdf(pdf_file_name)\n",
        "\n",
        "  # Step-2: Extracting text from pdf\n",
        "  txt_list = highlighter.extract_text_from_pdf(pdf_list)\n",
        "\n",
        "  # Step-3: Searching ICD-10 cod\n",
        "  icd10_code_dict = highlighter.search_icd_code(txt_list)\n",
        "\n",
        "  # Step-4: Highlighting ICD-10 code into pdf\n",
        "  pdf_output_file, txt_output_file = highlighter.highlight_icd_code(icd10_code_dict,\n",
        "                                                                    pdf_file_name=pdf_file_name,\n",
        "                                                                    cords_file_name=cords_file_name)\n",
        "  print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "  print(f\"Highlighted coordinates are saved into [{txt_output_file}] file.\")\n",
        "\n",
        "  # remove all pdf and text files\n",
        "  purge(\"pdf-files/*.pdf\")\n",
        "  purge(\"txt-files/*.txt\")\n",
        "  pdf_list = []\n",
        "  txt_list = []"
      ],
      "metadata": {
        "id": "hiOT-aTfedMT",
        "outputId": "a04831bd-c35b-42c0-b015-4f243295863c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File[output/01_final_output.pdf] is saved after highlighting ICD-10 code\n",
            "Highlighted coordinates are saved into [01_final_cords.txt] file.\n",
            "CPU times: user 1.53 s, sys: 23.4 ms, total: 1.55 s\n",
            "Wall time: 1.57 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf input_files\n",
        "!mkdir -p input_files\n",
        "!rm -rf output\n",
        "!mkdir -p output"
      ],
      "metadata": {
        "id": "p-HzgMk3g4cg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip output.zip output/*.*"
      ],
      "metadata": {
        "id": "MeS8LHhjjfzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-1: spliting pdf file\n",
        "pdf_file_name = \"01_final.pdf\"\n",
        "pdf_list = highlighter.split_pdf(pdf_file_name)\n",
        "\n",
        "# Step-2: Extracting text from pdf\n",
        "txt_list = highlighter.extract_text_from_pdf(pdf_list)"
      ],
      "metadata": {
        "id": "GJrJ862VBKhE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-3: Searching ICD-10 code\n",
        "page_code10_dict = highlighter.search_icd_code(txt_list)"
      ],
      "metadata": {
        "id": "wTFQkZtHBiLO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Clean Code"
      ],
      "metadata": {
        "id": "1NI81pVD-jhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_code10_dict[7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Stm7poqQBx-e",
        "outputId": "8ff7789e-9d76-43e2-922a-47bd18103797"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['G47.33', 'G47.34', 'G47.31', 'G47.10', 'G47,39', 'G47.8', 'RO6.3', 'R06.83']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_icd10_code = [reverse_code_pattern(code) for code in page_code10_dict[7]]\n",
        "clean_icd10_code"
      ],
      "metadata": {
        "id": "iYaQaJA8-JM4",
        "outputId": "da5e3437-ffb0-4f5c-dfa0-46ec28d23215",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['G47.33', 'G47.34', 'G47.31', 'G47.10', 'G47.39', 'G47.8', 'R06.3', 'R06.83']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fetch Keyword"
      ],
      "metadata": {
        "id": "laWIku_V_Ln2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_df = pd.read_csv(\"icd_10_code_and_keywords_v2.csv\")"
      ],
      "metadata": {
        "id": "1cqncdV0Lpj7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_df = code_df.loc[code_df[\"Code\"] == \"R06.3\"]\n",
        "keyword_df"
      ],
      "metadata": {
        "id": "2cGddOlEUwyC",
        "outputId": "69c67d01-67b3-436c-9db1-5513b0a5d6d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Code             Keyword\n",
              "29835  R06.3  Periodic breathing"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7bc03f94-7d56-4aec-8daa-98083a6c3e1b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Code</th>\n",
              "      <th>Keyword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>29835</th>\n",
              "      <td>R06.3</td>\n",
              "      <td>Periodic breathing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bc03f94-7d56-4aec-8daa-98083a6c3e1b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7bc03f94-7d56-4aec-8daa-98083a6c3e1b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7bc03f94-7d56-4aec-8daa-98083a6c3e1b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(code_df.loc[code_df[\"Code\"] == \"R06.83\"][\"Keyword\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3bJOVpdVGxT",
        "outputId": "d50b9b7a-3bcc-4ecd-e016-3164d427b09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Snoring']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_keyword(p_code):\n",
        "  keyword = \"\"\n",
        "   # reverse code if required\n",
        "  code = reverse_code_pattern(p_code)\n",
        "  # get keyword from dataset\n",
        "  keyword_list = list(code_df.loc[code_df[\"Code\"] == code][\"Keyword\"])\n",
        "  if len(keyword_list) > 0:\n",
        "    keyword = keyword_list[0]\n",
        "  return keyword"
      ],
      "metadata": {
        "id": "BEI2miiQX0BD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_keyword(\"R06.83\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RrWQ0QD6Ny6o",
        "outputId": "4995d7a1-9f92-45e6-ccea-75c558ab2d35"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Snoring'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Find keyword match"
      ],
      "metadata": {
        "id": "UK0Bb4L__7rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_match(text_file, keyword):\n",
        "  with open(text_file) as f:\n",
        "    lines = [line.rstrip('\\n') for line in f]\n",
        "\n",
        "    ratios = [fuzz.ratio(keyword, line) for line in lines]\n",
        "    best_match = lines[ratios.index(max(ratios))]\n",
        "    print(f\"{lines.index(best_match) + 1}:{keyword}, {fuzz.partial_ratio(keyword, best_match)}: {best_match}\")"
      ],
      "metadata": {
        "id": "5LaNVFvDAjUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{txt_list[7]}\", \"r\") as f:\n",
        "  lines = [line.rstrip('\\n') for line in f]\n",
        "  lines = [\" \".join(line.split()) for line in lines] # Remove extra spaces, tabs, and line breaks\n",
        "lines"
      ],
      "metadata": {
        "id": "Ywx50dACOT5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similarity_score(keyword, text_file):\n",
        "  # load text file\n",
        "  with open(text_file, \"r\") as f:\n",
        "    my_text = f.read()\n",
        "\n",
        "  # prepare key phrase\n",
        "  key_phrase_list = []\n",
        "  for textlist in my_text.split(\"\\n\"):\n",
        "    for key_phrase in textlist.split(\",\"):\n",
        "      if len(key_phrase) > 0:\n",
        "        key_phrase_list.append(key_phrase)\n",
        "  # get max similarity score\n",
        "  score_list = [SequenceMatcher(None, k_phrase, keyword).ratio() for k_phrase in key_phrase_list]\n",
        "  max_score = max(score_list)\n",
        "  # get index position\n",
        "  index_pos = score_list.index(max_score)\n",
        "  # get most similar phrase\n",
        "  most_similar_phrase = key_phrase_list[index_pos]\n",
        "  return max_score, most_similar_phrase"
      ],
      "metadata": {
        "id": "w6oOPyyNcd4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_similarity_score(\"Snoring\", txt_list[7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wikn8KpBd2VT",
        "outputId": "4efa55d1-5739-49a6-e23f-a1c5f30ff572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3333333333333333, ' octigraphy')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{txt_list[7]}\", \"r\") as f:\n",
        "  my_text = f.read()\n",
        "for keyword in my_text.split(\",\"):\n",
        "  seq = SequenceMatcher(None, keyword, \"Snoring\")\n",
        "  print(f\"{round(seq.ratio(), 3)} : {keyword}\")\n",
        "  if round(seq.ratio(), 3) > .70:\n",
        "    print(f\"Max ratio found: {round(seq.ratio(), 3)} : {keyword}\")"
      ],
      "metadata": {
        "id": "vBZpE5-zZYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text clean up"
      ],
      "metadata": {
        "id": "tCMut1zqMm76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{txt_list[7]}\", \"r\") as f:\n",
        "  my_text = f.read()"
      ],
      "metadata": {
        "id": "qaQ1wb0PMyMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(my_text)\n",
        "sentences = [sentence.text for sentence in doc.sents]"
      ],
      "metadata": {
        "id": "nwcAI5BnMuzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "id": "E5SUHuvrPRn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocess(text_file):\n",
        "  sentence_list = []\n",
        "  stopwords = [\"is\", \"a\"]\n",
        "  doc = nlp(my_text)\n",
        "  sentences = [sentence.text for sentence in doc.sents]\n",
        "  for sent in sentences:\n",
        "    clean_text = \" \".join(sent.split())  # Remove extra spaces, tabs, and line breaks\n",
        "    clean_text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", clean_text) # Remove punctuation\n",
        "    clean_text = re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", clean_text)     # Remove numbers\n",
        "    clean_text = \" \".join([w for w in clean_text.split() if not w.isdigit()]) # Remove digits= Side effect: removes extra spaces\n",
        "    clean_text = \" \".join([w for w in clean_text.split() if w.isalpha()]) # Remove non-alphabetic characters= Side effect: removes extra spaces\n",
        "    clean_text = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", clean_text) # Remove all special characters and punctuation\n",
        "    # Remove stopwords from a list\n",
        "    tokens = clean_text.split()\n",
        "    clean_tokens = [t for t in tokens if not t in stopwords]\n",
        "    clean_text = \" \".join(clean_tokens)\n",
        "    # Remove short tokens\n",
        "    tokens = clean_text.split()\n",
        "    clean_tokens = [t for t in tokens if len(t) > 3]\n",
        "    clean_text = \" \".join(clean_tokens)\n",
        "    # Remove repeated characters\n",
        "    clean_text = re.sub(r'(.)\\1{3,}',r'\\1', clean_text)\n",
        "    if len(clean_text) > 0:\n",
        "      sentence_list.append(clean_text)\n",
        "  return sentence_list"
      ],
      "metadata": {
        "id": "QnlOii79PXSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = text_preprocess(my_text)\n",
        "sentence_list"
      ],
      "metadata": {
        "id": "anIOSkxxQzwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{txt_list[7]}\", \"r\") as f:\n",
        "  lines = [line.rstrip('\\n') for line in f]\n",
        "  sentence_list = [\" \".join(line.split()) for line in lines] # Remove extra spaces, tabs, and line breaks\n",
        "sentence_list"
      ],
      "metadata": {
        "id": "7jG4M79MLz5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_match(sentence_list, keyword):\n",
        "  print(\"#\"*10)\n",
        "  print(f\"Matching for : {keyword}\")\n",
        "  print()\n",
        "  ratios = [fuzz.ratio(keyword, sentence) for sentence in sentence_list]\n",
        "  best_match = sentence_list[ratios.index(max(ratios))]\n",
        "  print(f\"Best match: {fuzz.ratio(keyword, best_match)} | {best_match}\")\n",
        "  print(f\"Before Match: {fuzz.ratio(keyword, sentence_list[sentence_list.index(best_match) - 1])} | {sentence_list[sentence_list.index(best_match) - 1]}\")\n",
        "  print(f\"After Match: {fuzz.ratio(keyword, sentence_list[sentence_list.index(best_match) + 1])} | {sentence_list[sentence_list.index(best_match) + 1]}\")\n",
        "  print()\n",
        "\n",
        "  p_ratios = [fuzz.partial_ratio(keyword, sentence) for sentence in sentence_list]\n",
        "  p_best_match = sentence_list[p_ratios.index(max(p_ratios))]\n",
        "  print(f\"Partial Best match: {fuzz.partial_ratio(keyword, p_best_match)} | {p_best_match}\")\n",
        "  print(f\"Partial Before Match: {fuzz.partial_ratio(keyword, sentence_list[sentence_list.index(p_best_match) - 1])} | {sentence_list[sentence_list.index(p_best_match) - 1]}\")\n",
        "  print(f\"Partial After Match: {fuzz.partial_ratio(keyword, sentence_list[sentence_list.index(p_best_match) + 1])} | {sentence_list[sentence_list.index(p_best_match) + 1]}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "yRK1ILJFSZYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_list = [get_keyword(code) for code in clean_icd10_code]\n",
        "keyword_list"
      ],
      "metadata": {
        "id": "lpyTbAmGEkg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[get_best_match(sentence_list, keyword) for keyword in keyword_list]"
      ],
      "metadata": {
        "id": "zpQ1gCbVXNFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_match(sentence_list, keyword_list):\n",
        "  for keyword in keyword_list:\n",
        "    #for sentence in sentence_list:\n",
        "    print(\"#\"*10)\n",
        "    print(f\"Matching for : {keyword}\")\n",
        "    match_list = process.extract(keyword, sentence_list, scorer = fuzz.ratio, limit = 3)\n",
        "    for match_found in match_list:\n",
        "      print(f\"{match_found[0]} | {match_found[1]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "Tj28h6fOPXSd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_best_match(sentence_list, keyword_list)\n",
        "#process.extract(query, choices, scorer = fuzz.partial_ratio, limit = 2)"
      ],
      "metadata": {
        "id": "Lpb7D5IOOk5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list.index(\"Moderate Obstructive Sleep Apnea .\")"
      ],
      "metadata": {
        "id": "7PF541tZWM00",
        "outputId": "5961f3d1-1f3b-4b1b-e55e-05c4f65d34c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, sent in enumerate(sentence_list):\n",
        "  print(f\"{idx}>{sent}\")"
      ],
      "metadata": {
        "id": "_EsIKRI9WXpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##All Together"
      ],
      "metadata": {
        "id": "xnOyuVdwJf7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_keyword(p_code):\n",
        "  keyword = \"\"\n",
        "   # reverse code if required\n",
        "  code = reverse_code_pattern(p_code)\n",
        "  # get keyword from dataset\n",
        "  keyword_list = list(code_df.loc[code_df[\"Code\"] == code][\"Keyword\"])\n",
        "  if len(keyword_list) > 0:\n",
        "    keyword = keyword_list[0]\n",
        "  return keyword\n",
        "\n",
        "def get_best_match(code10_dict, txt_list, num_page):\n",
        "  # Step 1: reverse code pattern\n",
        "  clean_icd10_code = [reverse_code_pattern(code) for code in page_code10_dict[num_page]]\n",
        "  # Step 2: fetch keyword based on code \n",
        "  keyword_list = [get_keyword(code) for code in clean_icd10_code]\n",
        "  # Step 3: prepare sentence list \n",
        "  with open(f\"{txt_list[num_page]}\", \"r\") as f:\n",
        "    lines = [line.rstrip('\\n') for line in f]\n",
        "    sentence_list = [\" \".join(line.split()) for line in lines] # Remove extra spaces, tabs, and line breaks\n",
        "\n",
        "  # Step 4: get best match \n",
        "  for keyword in keyword_list:\n",
        "    #for sentence in sentence_list:\n",
        "    print(\"#\"*10)\n",
        "    print(f\"Matching for : {keyword}\")\n",
        "    match_list = process.extract(keyword, sentence_list, scorer = fuzz.ratio, limit = 3)\n",
        "    print(match_list[0][1])\n",
        "    for idx, match_found in enumerate(match_list):\n",
        "      if match_found[1] > 40:\n",
        "        print(f\"{idx}> {match_found[0]} | {match_found[1]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "DmuQsrthJh24"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_match(p_code, num_page):\n",
        "    # Step 1: reverse code pattern\n",
        "    reversed_icd_code = reverse_code_pattern(p_code)\n",
        "    # Step 2: fetch keyword based on code \n",
        "    keyword = get_keyword(reversed_icd_code)\n",
        "    # Step 3: prepare sentence list \n",
        "    with open(f\"{txt_list[num_page]}\", \"r\") as f:\n",
        "      lines = [line.rstrip('\\n') for line in f]\n",
        "      sentence_list = [\" \".join(line.split()) for line in lines] # Remove extra spaces, tabs, and line breaks\n",
        "    # Step 4: get best 3 match ratio \n",
        "    match_list = process.extract(keyword, sentence_list, scorer = fuzz.ratio, limit = 3)\n",
        "    return match_list, sentence_list"
      ],
      "metadata": {
        "id": "enKktNs2TkPd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_icd10_code = [reverse_code_pattern(code) for code in page_code10_dict[8]]\n",
        "clean_icd10_code"
      ],
      "metadata": {
        "id": "XK9QcFYaUJvi",
        "outputId": "1027e1a4-ae73-487a-a9c0-40736fd73c46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O68', 'O00', 'O80', 'O41']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_list, sentence_list = get_best_match(\"O68\", 8)\n",
        "match_list"
      ],
      "metadata": {
        "id": "OKm64hdlLVow",
        "outputId": "916ddaf1-6d54-4772-f668-2edf8c88c80d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('} End Study Time: 7:23:14 AM { | Maan of Desaturations. Nadirs (%): 91',\n",
              "  36),\n",
              " ('S (R| S] slslels(elalele/s|slslel™i[~lrrrl\"tel\"l>', 30),\n",
              " ('whatsoever. Copyright (c) 2022. All rights reserved. Innodata Synodex, LLC',\n",
              "  29)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fuzzy"
      ],
      "metadata": {
        "id": "lUbkfe9gA8uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "id": "uK6DLGv7bSar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz"
      ],
      "metadata": {
        "id": "XcZjog3PbKAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str1 = 'This document printed distributed unauthorized purpose whatsoever'\n",
        "str2 = 'Other disorders of amniotic fluid and membranes'\n",
        "\n",
        "ratio = fuzz.ratio(str1, str2)\n",
        "partial_ratio = fuzz.partial_ratio(str1, str2)\n",
        "\n",
        "print(ratio)\n",
        "print(partial_ratio)"
      ],
      "metadata": {
        "id": "m03_HEsxbXNg",
        "outputId": "a424e11b-6de0-41fb-c278-3284fb87c28f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for keyword in sentence_list:\n",
        "  ratio = fuzz.partial_ratio(keyword, \"Snoring\")\n",
        "  print(f\"{round(ratio, 3)} : {keyword}\")"
      ],
      "metadata": {
        "id": "ZFTz7A9hbrjX",
        "outputId": "6ee3ad60-20ac-42a3-a7c4-50bbfe6c287d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43 : Redacted person nine Male North Texas Neuroscience Cente\n",
            "43 : Sunil Mathews\n",
            "43 : NeTessc NerthyTaxas Neurmacienee sinap Gantmr Diplomate American Board Psychiatry Neurolagy ABPN Diplomate ABPN Clinical Neurophysiology EMGEEG\n",
            "29 : Diplomate ABPN Sleep Medicine INTERPRETATION SLEEP STUDY\n",
            "14 : Chart Number\n",
            "29 : Height Male Weight Referring Physiclan Sunil Mathews Epworth Score Neck Size Clinical Reason Study Suspected Sleep Apnea Study Type WatchPAT Study Date MethodsTechnique standards American Acadenty Sleep\n",
            "71 : medicine were followd monitoring patient\n",
            "86 : Home Sleep Test includedrerordingof peripheral arterial tone oxygen saturation heart rata octigraphy body position snoring\n",
            "29 : RBland were caleuleted from this data\n",
            "14 : Tatal slecp awake time NREM sleap were assassad\n",
            "29 : Minimal oxygen saturation Moderate Obstructive Sleep Apnea\n",
            "100 : Severe Related Respiratory Events Hypoxemia Loud Disruptive Snoring glalsslelelselelslelelssleles Obesity Reduced Sleep Ffficiancy minutes hurts\n",
            "43 : Excessive Daytime Sleepiness SISRBlS RECOMMENDATIONS Cifoltowup with physician discuss treatment options which Inciude CPAP oral appliance surgical management Upper airway\n",
            "29 : RICPAP Titration with CFlewof followup with physician further assessment\n",
            "43 : Clin sleep diagnostic racammended insufficient data from home sleep test\n",
            "43 : Cidefer further testing\n",
            "57 : Refer Slage Specialist managament stean disorder Treatment based clintcal palysomnographic findings\n",
            "29 : clinically indicated patient should pursue weight control limit\n",
            "57 : alcohol befare sleep avald drinking\n",
            "57 : operating heavy machinery when sinepy\n",
            "86 : DIAGNOSIS obstructive sleep apnea Steeprelated hypoventilation ClPrimary snoring Sieap eclated breathing disorder Upper airway resistance fPerladic Limb Movement Sleep central sleep apnea\n",
            "43 : CoHypersomnia unspecified\n",
            "29 : CheyneStokes respiration\n",
            "57 : JOther Click hare enter text Interpreting physician Sunil Mathews Date elelalas Colinas Ridge Suite leving Tels wiww\n",
            "29 : ptnsecom Page Rights Reserved\n",
            "43 : This document contains proprietary confidential personal data intended viewing only authorized users\n",
            "43 : This document printed distributed unauthorized purpose whatsoever\n",
            "57 : Copyright\n",
            "43 : rights reserved\n",
            "43 : Innodata Synodex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_match(text_file, keyword):\n",
        "  with open(text_file) as f:\n",
        "    names = [line.rstrip('\\n') for line in f]\n",
        "    ratios = [fuzz.ratio(keyword, name) for name in names]\n",
        "    best_match = names[ratios.index(max(ratios))]\n",
        "    print(fuzz.partial_ratio(name_to_match,best_match))\n",
        "    print(best_match)\n",
        "  return best_match"
      ],
      "metadata": {
        "id": "uXQSMdjknoux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_best_match(txt_list[7], \"Primary central sleep apnea\")"
      ],
      "metadata": {
        "id": "6ccqOnHwo76x",
        "outputId": "8334fb8a-afa6-464e-a7be-264f5459f98c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n",
            "               APS.Extract® All Rights Reserved\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'               APS.Extract® All Rights Reserved'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(txt_list[7]) as f:\n",
        "    names = [line.rstrip('\\n') for line in f]\n",
        "#for names in lines:\n",
        "#    print (names[0])\n",
        "    icdkeywords = ['Obstructive sleep apnea','Primary central sleep apnea']\n",
        "    #name_to_match = 'Sleep related hypoventilation'\n",
        "    #name_to_match = 'ZZZZZ'\n",
        "    #print (\"Names:\", names)\n",
        "    #print (\"name to match:\", name_to_match)\n",
        "    #ratios = [fuzz.ratio(name_to_match, names)]\n",
        "    for name_to_match in icdkeywords:\n",
        "        ratios = [fuzz.ratio(name_to_match, name) for name in names]\n",
        "        print(ratios)\n",
        "        print(max(ratios))\n",
        "        best_match = names[ratios.index(max(ratios))]\n",
        "        print(fuzz.partial_ratio(name_to_match, best_match))\n",
        "        print(fuzz.token_set_ratio(name_to_match, best_match))\n",
        "        print(best_match)"
      ],
      "metadata": {
        "id": "Qj42ezHoo1Ca",
        "outputId": "ac659327-cf3a-414a-cb48-15842028a102",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 0, 19, 1, 9, 0, 0, 3, 3, 4, 4, 13, 6, 11, 3, 3, 1, 2, 7, 15, 10, 1, 15, 1, 9, 23, 1, 3, 6, 5, 3, 12, 1, 7, 13, 4, 15, 12, 2, 0, 8, 11, 19, 13, 8, 19, 5, 12, 7, 17, 2, 22, 10, 13, 15, 10, 5, 8, 4, 5, 5, 4, 2, 3, 20, 1, 20]\n",
            "23\n",
            "91\n",
            "100\n",
            "                                                                                                    Moderate Obstructive Sleep Apnea                       .\n",
            "[3, 0, 21, 1, 7, 0, 0, 5, 4, 4, 3, 9, 10, 13, 5, 6, 2, 6, 11, 14, 15, 2, 19, 2, 14, 16, 2, 5, 10, 6, 3, 4, 1, 4, 13, 6, 11, 11, 6, 1, 7, 7, 13, 14, 7, 17, 13, 15, 9, 17, 6, 15, 15, 14, 20, 11, 7, 9, 3, 7, 6, 4, 2, 5, 27, 4, 22]\n",
            "27\n",
            "34\n",
            "34\n",
            "               APS.Extract® All Rights Reserved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove extra spaces, tabs, and line breaks\n",
        "clean_text = \" \".join(my_text.split())\n",
        "clean_text"
      ],
      "metadata": {
        "id": "OXZdxZAPNF1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation\n",
        "clean_text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", clean_text)\n",
        "clean_text"
      ],
      "metadata": {
        "id": "lPBuuR5BNygV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove numbers\n",
        "clean_text = re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", clean_text)\n",
        "clean_text"
      ],
      "metadata": {
        "id": "zu_gyFOpOFgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove digits\n",
        "clean_text = \" \".join([w for w in clean_text.split() if not w.isdigit()]) # Side effect: removes extra spaces\n",
        "clean_text"
      ],
      "metadata": {
        "id": "BjvumO5mOOk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove non-alphabetic characters\n",
        "clean_text = \" \".join([w for w in clean_text.split() if w.isalpha()]) # Side effect: removes extra spaces\n",
        "clean_text"
      ],
      "metadata": {
        "id": "0Dmye1AqOWLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all special characters and punctuation\n",
        "clean_text = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", clean_text)\n",
        "clean_text"
      ],
      "metadata": {
        "id": "M9fL4BflOfFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords from a list\n",
        "stopwords = [\"is\", \"a\"]\n",
        "tokens = clean_text.split()\n",
        "clean_tokens = [t for t in tokens if not t in stopwords]\n",
        "clean_text = \" \".join(clean_tokens)"
      ],
      "metadata": {
        "id": "K_6C3V9_Omrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text"
      ],
      "metadata": {
        "id": "HkYix4fOOxMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove short tokens\n",
        "tokens = clean_text.split()\n",
        "clean_tokens = [t for t in tokens if len(t) > 1]\n",
        "clean_text = \" \".join(clean_tokens)\n",
        "clean_text"
      ],
      "metadata": {
        "id": "-RfKuKojO12j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove repeated characters\n",
        "clean_text = re.sub(r'(.)\\1{3,}',r'\\1', clean_text)\n",
        "clean_text"
      ],
      "metadata": {
        "id": "daIJ_SCuO_R0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}