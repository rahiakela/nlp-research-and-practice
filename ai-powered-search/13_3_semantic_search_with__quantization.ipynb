{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/nlp-research-and-practice/blob/main/ai-powered-search/13_3_semantic_search_with__quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51uM8gPIW0mO"
      },
      "source": [
        "## Setup\n",
        "\n",
        "In this notebook, we\"re going to install a transformer model, analyze the embedding output, and compare some vectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#outdoors\n",
        "![ ! -d 'outdoors' ] && git clone --depth=1 https://github.com/ai-powered-search/outdoors.git\n",
        "! cd outdoors && git pull\n",
        "! cd outdoors && cat outdoors.tgz.part* > outdoors.tgz\n",
        "! cd outdoors && mkdir -p '../data/outdoors/' && tar -xvf outdoors.tgz -C '../data/outdoors/'"
      ],
      "metadata": {
        "id": "5xShS9pNXKtl",
        "outputId": "ac8f758a-94d0-469e-d8f7-97afd3127aa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'outdoors'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 25 (delta 0), reused 22 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (25/25), 491.39 MiB | 23.32 MiB/s, done.\n",
            "Updating files: 100% (23/23), done.\n",
            "Already up to date.\n",
            "README.md\n",
            "concepts.pickle\n",
            "._guesses.csv\n",
            "guesses.csv\n",
            "._guesses_all.json\n",
            "guesses_all.json\n",
            "outdoors_concepts.pickle\n",
            "outdoors_embeddings.pickle\n",
            "._outdoors_golden_answers.csv\n",
            "outdoors_golden_answers.csv\n",
            "._outdoors_golden_answers.xlsx\n",
            "outdoors_golden_answers.xlsx\n",
            "._outdoors_golden_answers_20210130.csv\n",
            "outdoors_golden_answers_20210130.csv\n",
            "outdoors_labels.pickle\n",
            "outdoors_question_answering_contexts.json\n",
            "outdoors_questionanswering_test_set.json\n",
            "outdoors_questionanswering_train_set.json\n",
            "._posts.csv\n",
            "posts.csv\n",
            "predicates.pickle\n",
            "pull_aips_dependency.py\n",
            "._question-answer-seed-contexts.csv\n",
            "question-answer-seed-contexts.csv\n",
            "question-answer-squad2-guesses.csv\n",
            "._roberta-base-squad2-outdoors\n",
            "roberta-base-squad2-outdoors/\n",
            "roberta-base-squad2-outdoors/._tokenizer_config.json\n",
            "roberta-base-squad2-outdoors/tokenizer_config.json\n",
            "roberta-base-squad2-outdoors/._special_tokens_map.json\n",
            "roberta-base-squad2-outdoors/special_tokens_map.json\n",
            "roberta-base-squad2-outdoors/._config.json\n",
            "roberta-base-squad2-outdoors/config.json\n",
            "roberta-base-squad2-outdoors/._merges.txt\n",
            "roberta-base-squad2-outdoors/merges.txt\n",
            "roberta-base-squad2-outdoors/._training_args.bin\n",
            "roberta-base-squad2-outdoors/training_args.bin\n",
            "roberta-base-squad2-outdoors/._pytorch_model.bin\n",
            "roberta-base-squad2-outdoors/pytorch_model.bin\n",
            "roberta-base-squad2-outdoors/._vocab.json\n",
            "roberta-base-squad2-outdoors/vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install faiss-cpu --no-cache\n",
        "!pip install nmslib\n",
        "# !pip install faiss-gpu"
      ],
      "metadata": {
        "id": "HQRWkAsbmB_i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA3xmRRHW0mQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "sys.path.append(\"../..\")\n",
        "# from aips import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import tqdm\n",
        "\n",
        "import faiss\n",
        "import sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
        "from sentence_transformers.quantization import quantize_embeddings\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "import nmslib\n",
        "\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ruXhtGPW0mR"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\n",
        "    \"mixedbread-ai/mxbai-embed-large-v1\",\n",
        "    similarity_fn_name=SimilarityFunction.DOT_PRODUCT,\n",
        "    truncate_dim=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get embeddings"
      ],
      "metadata": {
        "id": "KzFEJ0VxFGTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(texts, model, cache_name, ignore_cache=False):\n",
        "  cache_file_name = f\"data/outdoors/{cache_name}.pickle\"\n",
        "  if ignore_cache or not os.path.isfile(cache_file_name):\n",
        "    embeddings = model.encode(texts, normalize_embeddings=True)\n",
        "    os.makedirs(os.path.dirname(cache_file_name), exist_ok=True)\n",
        "    with open(cache_file_name, \"wb\") as cache_file:\n",
        "      pickle.dump(embeddings, cache_file)\n",
        "  else:\n",
        "    with open(cache_file_name, \"rb\") as cache_file:\n",
        "      embeddings = pickle.load(cache_file)\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "WipDDFrAFdsl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(scores, ids, data):\n",
        "    results = generate_search_results(scores, ids, data)\n",
        "    display(results)\n",
        "    return results\n",
        "\n",
        "def get_outdoors_data():\n",
        "    outdoors_dataframe = pd.read_csv(\"data/outdoors/posts.csv\")\n",
        "    # outdoors_data = list(outdoors_dataframe.to_dict())\n",
        "    outdoors_data = list(outdoors_dataframe.to_dict(orient='records'))\n",
        "    return outdoors_data\n",
        "\n",
        "def display_statistics(search_results, baseline_search_results=None, start_message=\"Recall\"):\n",
        "    index_name = search_results[\"index_name\"]\n",
        "    time_taken = search_results[\"time_taken\"]\n",
        "    index_size = search_results[\"size\"]\n",
        "    improvement_ms = \"\"\n",
        "    improvement_size = \"\"\n",
        "    recall = 1.0\n",
        "    if baseline_search_results:\n",
        "        full_search_time = baseline_search_results[\"time_taken\"]\n",
        "        time_imp = round((full_search_time - time_taken) * 100 / full_search_time, 2)\n",
        "        improvement_ms = f\" ({time_imp}% improvement)\"\n",
        "        improvement_size = f\" ({round((baseline_search_results['size'] - index_size) * 100 / baseline_search_results['size'], 2)}% improvement)\"\n",
        "        recall = calculate_recall(baseline_search_results[\"results\"], search_results[\"results\"])\n",
        "\n",
        "    print(f\"{index_name} search took: {time_taken:.3f} ms{improvement_ms}\")\n",
        "    print(f\"{index_name} index size: {round(index_size / 1000000, 2)} MB{improvement_size}\")\n",
        "    print(f\"{start_message}: {round(recall, 4)}\")\n",
        "\n",
        "def calculate_recall(scored_full_results, scored_quantized_results):\n",
        "    recalls = []\n",
        "    for i in range(len(scored_full_results)):\n",
        "        full_ids = [r[\"id\"] for r in scored_full_results[i]]\n",
        "        quantized_ids = [r[\"id\"] for r in scored_quantized_results[i]]\n",
        "        recalls.append((len(set(full_ids).intersection(set(quantized_ids))) /\n",
        "                       len(set(quantized_ids))))\n",
        "    return sum(recalls) / len(recalls)\n",
        "\n",
        "def generate_search_results(faiss_scores, faiss_ids):\n",
        "    outdoors_data = get_outdoors_data()\n",
        "    faiss_results = []\n",
        "    for i in range(len(faiss_scores)):\n",
        "        results = []\n",
        "        for j, id in enumerate(faiss_ids[i]):\n",
        "            id = int(id)\n",
        "            result = {\"score\": faiss_scores[i][j],\n",
        "                      \"title\": outdoors_data[id][\"title\"],\n",
        "                      \"body\": outdoors_data[id][\"body\"],\n",
        "                      \"id\": id}\n",
        "            results.append(result)\n",
        "        faiss_results.append(results)\n",
        "    return faiss_results\n",
        "\n",
        "def time_and_execute_search(index, index_name, query_embeddings, k=25, num_runs=100):\n",
        "    search_times = []\n",
        "    faiss_scores = None\n",
        "    faiss_ids = None\n",
        "\n",
        "    for i in range(num_runs):\n",
        "        start_time = time.time()\n",
        "        faiss_scores, faiss_ids = index.search(query_embeddings, k=k)\n",
        "        time_taken = ((time.time() - start_time) * 1000)\n",
        "        search_times.append(time_taken)\n",
        "\n",
        "    results = {\"results\": generate_search_results(faiss_scores, faiss_ids),\n",
        "               \"time_taken\": np.average(search_times),\n",
        "               \"faiss_scores\": faiss_scores, \"faiss_ids\": faiss_ids}\n",
        "    index_stats = {}\n",
        "    if index_name:\n",
        "        index_stats ={\n",
        "            \"index_name\": index_name,\n",
        "            \"size\": os.path.getsize(index_name)\n",
        "        }\n",
        "    return results | index_stats"
      ],
      "metadata": {
        "id": "Z05FCJIODT-9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scalar quantization"
      ],
      "metadata": {
        "id": "Zbjb6yEHPscY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outdoors_dataframe = pd.read_csv(\"data/outdoors/posts.csv\")\n",
        "post_texts = [f\"{post['title']} {post['body']}\" for _, post in outdoors_dataframe.iterrows()]\n",
        "# post_texts"
      ],
      "metadata": {
        "id": "sx8hTmXfrDhS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's index full-precision embeddings using FAISS\n",
        "def index_full_precision_embeddings(doc_embeddings, name):\n",
        "  # IndexFlatIP is a simple, unoptimized index supporting different embedding formats\n",
        "  index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
        "  index.add(doc_embeddings)      # Adds documents to the index\n",
        "  faiss.write_index(index, name) # Writes the index to disk\n",
        "  return index\n",
        "\n",
        "def get_outdoors_embeddings(model):\n",
        "  outdoors_dataframe = pd.read_csv(\"data/outdoors/posts.csv\")\n",
        "  post_texts = [\n",
        "      f\"{post['title']} {post['body']}\"\n",
        "      for _, post in outdoors_dataframe.iterrows()\n",
        "  ]\n",
        "  return np.array(get_embeddings(post_texts, model, \"outdoors_mrl_normed\"))\n",
        "\n",
        "# Generates embeddings for the outdoors dataset\n",
        "outdoors_embeddings = get_outdoors_embeddings(model)\n",
        "# Creates a full-precision(Float32) FAISS index\n",
        "full_index = index_full_precision_embeddings(outdoors_embeddings, \"full_embeddings\")"
      ],
      "metadata": {
        "id": "sNWZ9u50OifI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's generate full-precision query embeddings\n",
        "def get_test_queries():\n",
        "  return[\n",
        "      \"tent poles\", \"hiking trails\", \"mountain forests\",\n",
        "      \"white water\", \"best waterfalls\", \"mountain biking\",\n",
        "      \"snowboarding slopes\", \"bungee jumping\", \"public parks\"\n",
        "  ]\n",
        "\n",
        "# Gets test queries for benchmarking\n",
        "queries = get_test_queries()\n",
        "# Generates embeddings for each query\n",
        "query_embeddings = model.encode(queries, normalize_embeddings=True, convert_to_numpy=True)\n",
        "\n",
        "# Generates search time, index size, and recall statistics for the full-precision (Float32) index\n",
        "full_results = time_and_execute_search(full_index, \"full_embeddings\", query_embeddings, k=25)\n",
        "# Displays the benchmarking stats for the full-precision index\n",
        "display_statistics(full_results)"
      ],
      "metadata": {
        "id": "GDKLy9PWoxyW",
        "outputId": "ec462cb7-02e2-4068-f8d1-0c6ee33df851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full_embeddings search took: 41.694 ms\n",
            "full_embeddings index size: 80.22 MB\n",
            "Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define functions for benchmark quantized search approaches\n",
        "def evaluate_search(full_index, optimized_index, optimized_index_name,\n",
        "                    query_embeddings, optimized_query_embeddings,\n",
        "                    k=25, display=True, log=False):\n",
        "    full_results = time_and_execute_search(full_index, \"full_embeddings\", query_embeddings, k=k)\n",
        "    optimized_results = time_and_execute_search(optimized_index, optimized_index_name, optimized_query_embeddings, k=k)\n",
        "    if display:\n",
        "        display_statistics(optimized_results, full_results)\n",
        "    return optimized_results, full_results\n",
        "\n",
        "def evaluate_rerank_search(full_index, optimized_index,\n",
        "                           query_embeddings,\n",
        "                           optimized_embeddings,\n",
        "                           k=50, limit=25):\n",
        "    results, full_results = evaluate_search(full_index, optimized_index, None, query_embeddings,\n",
        "                                            optimized_embeddings, display=False, k=k)\n",
        "\n",
        "    doc_embeddings = get_outdoors_embeddings(model) #This can point to a cheap on-disk data source containing the original full-precision embeddings\n",
        "    rescore_scores, rescore_ids = [], []\n",
        "    for i in range(len(results[\"results\"])):\n",
        "        embedding_ids = results[\"faiss_ids\"][i]\n",
        "        top_k_embeddings = [doc_embeddings[id] for id in embedding_ids]\n",
        "        query_embedding = query_embeddings[i]\n",
        "        scores = query_embedding @ np.array(top_k_embeddings).T\n",
        "        indices = scores.argsort()[::-1][:limit]\n",
        "        top_k_indices = embedding_ids[indices]\n",
        "        top_k_scores = scores[indices]\n",
        "        rescore_scores.append(top_k_scores)\n",
        "        rescore_ids.append(top_k_indices)\n",
        "\n",
        "    results = generate_search_results(rescore_scores, rescore_ids)\n",
        "    recall = calculate_recall(full_results[\"results\"], results)\n",
        "    print(f\"Reranked recall: {round(recall, 4)}\")"
      ],
      "metadata": {
        "id": "9MzyzbqTrjl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s implement Float16 scalar quantization\n",
        "def index_float16_precision_embeddings(doc_embeddings, name):\n",
        "  float16_embeddings = quantize_embeddings(doc_embeddings, precision=\"float16\")\n",
        "  print(f\"Float16 embeddings shape: {float16_embeddings.shape}\")\n",
        "  # IndexFlatIP is a simple, unoptimized index supporting different embedding formats\n",
        "  index = faiss.IndexFlatIP(float16_embeddings.shape[1])\n",
        "  index.add(float16_embeddings)      # Adds documents to the index\n",
        "  faiss.write_index(index, name)  # Writes the index to disk\n",
        "  return index\n",
        "\n",
        "float16_index_name = \"int16_embeddings\"\n",
        "float16_index = index_float16_precision_embeddings(outdoors_embeddings, int16_index_name)\n",
        "# Quantizes the query embeddings to Int16 precision\n",
        "quantized_queries = quantize_embeddings(query_embeddings, calibration_embeddings=outdoors_embeddings, precision=\"float16\")\n",
        "# Performs benchmarks for search time, index size, and recall\n",
        "evaluate_search(full_index, float16_index, float16_index_name, query_embeddings, quantized_queries)\n",
        "# Performs benchmarks again allowing reranking of top results with full-precision embeddings\n",
        "evaluate_rerank_search(full_index, float16_index, query_embeddings, quantized_queries)"
      ],
      "metadata": {
        "id": "Z0GdYZ3mTGUG",
        "outputId": "6cbd17e2-6dc2-4efd-bdab-1826d3431596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'int16_index_name' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-6ae4802d9735>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfloat16_index_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"int16_embeddings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mfloat16_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_float16_precision_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdoors_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint16_index_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# Quantizes the query embeddings to Int16 precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mquantized_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantize_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalibration_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutdoors_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'int16_index_name' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s implement Int8 scalar quantization\n",
        "def index_int8_precision_embeddings(doc_embeddings, name):\n",
        "  int8_embeddings = quantize_embeddings(doc_embeddings, precision=\"int8\")\n",
        "  print(f\"Int8 embeddings shape: {int8_embeddings.shape}\")\n",
        "  # IndexFlatIP is a simple, unoptimized index supporting different embedding formats\n",
        "  index = faiss.IndexFlatIP(int8_embeddings.shape[1])\n",
        "  index.add(int8_embeddings)      # Adds documents to the index\n",
        "  faiss.write_index(index, name)  # Writes the index to disk\n",
        "  return index\n",
        "\n",
        "int8_index_name = \"int8_embeddings\"\n",
        "int8_index = index_int8_precision_embeddings(outdoors_embeddings, int8_index_name)\n",
        "# Quantizes the query embeddings to Int8 precision\n",
        "quantized_queries = quantize_embeddings(query_embeddings, calibration_embeddings=outdoors_embeddings, precision=\"int8\")\n",
        "# Performs benchmarks for search time, index size, and recall\n",
        "evaluate_search(full_index, int8_index, int8_index_name, query_embeddings, quantized_queries)\n",
        "# Performs benchmarks again allowing reranking of top results with full-precision embeddings\n",
        "evaluate_rerank_search(full_index, int8_index, query_embeddings, quantized_queries)"
      ],
      "metadata": {
        "id": "GOqBwlcwrnwc",
        "outputId": "b88b9a23-c667-49f3-d988-6d35c9c24079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Results for: <em>dehyd</em></h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.633 | The re-hydration time for deydrated foods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Binary quantization"
      ],
      "metadata": {
        "id": "8uvVrI9LS7Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def index_binary_embeddings(doc_embeddings, binary_index_name):\n",
        "  # Quantizes the doc embeddings to binary (1 bit per dimension)\n",
        "  binary_embeddings = quantize_embeddings(doc_embeddings, precision=\"binary\").astype(np.uint8)\n",
        "  print(f\"Binary embeddings shape: {binary_embeddings.shape}\")\n",
        "  # Creates the binary embeddings index\n",
        "  index = faiss.IndexBinaryFlat(binary_embeddings.shape[1] * 8)\n",
        "  index.add(binary_embeddings)      # Adds documents to the index\n",
        "  faiss.write_index(index, binary_index_name)  # Writes the index to disk\n",
        "  return index"
      ],
      "metadata": {
        "id": "mpwq95wBr_YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_index_name = \"binary_embeddings\"\n",
        "binary_index = index_binary_embeddings(outdoors_embeddings, binary_index_name)\n",
        "# Quantizes the query embeddings to binary\n",
        "quantized_queries = quantize_embeddings(\n",
        "    query_embeddings,\n",
        "    calibration_embeddings=outdoors_embeddings,\n",
        "    precision=\"binary\").astype(np.uint8) # Saves every 8 dimensions as 1 byte, encoded as unsigned Int8\n",
        "# Performs benchmarks for search time, index size, and recall\n",
        "evaluate_search(full_index, binary_index, binary_index_name, query_embeddings, quantized_queries)\n",
        "# Performs benchmarks again allowing reranking of top results with full-precision embeddings\n",
        "evaluate_rerank_search(full_index, binary_index, query_embeddings, quantized_queries)"
      ],
      "metadata": {
        "id": "2t1bk4BpgWEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Product quantization"
      ],
      "metadata": {
        "id": "wPVPVTqJiUbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def index_pq_embeddings(doc_embeddings, index_name, num_subvectors=16):\n",
        "  dimensions = doc_embeddings.shape[1]\n",
        "  # Divides the embedding into M=16 subvectors (of 64 dimensions each)\n",
        "  M = num_subvectors\n",
        "  # 8 bits = 256 maximum cluster centroids per subvector\n",
        "  num_bits = 8\n",
        "  # Creates the PQ embeddings index\n",
        "  pq_index = faiss.IndexPQ(dimensions, M, num_bits)\n",
        "  # Generates the cluster centroids using k-means clustering\n",
        "  pq_index.train(doc_embeddings)\n",
        "  pq_index.add(doc_embeddings)\n",
        "  faiss.write_index(pq_index, index_name)  # Writes the index to disk\n",
        "  return pq_index"
      ],
      "metadata": {
        "id": "Ri48ytjriVWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pq_index_name = \"pq_embeddings\"\n",
        "pq_index = index_pq_embeddings(outdoors_embeddings, pq_index_name)\n",
        "\n",
        "# Performs benchmarks for search time, index size, and recall\n",
        "evaluate_search(full_index, pq_index, pq_index_name, query_embeddings, query_embeddings)\n",
        "# Performs benchmarks again allowing reranking of top results with full-precision embeddings\n",
        "evaluate_rerank_search(full_index, pq_index, query_embeddings, query_embeddings)"
      ],
      "metadata": {
        "id": "xXHyEGiFNTzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pq_index_name = \"pq_embeddings\"\n",
        "pq_index = index_pq_embeddings(outdoors_embeddings, pq_index_name, num_subvectors=32)\n",
        "\n",
        "# Performs benchmarks for search time, index size, and recall\n",
        "evaluate_search(full_index, pq_index, pq_index_name, query_embeddings, query_embeddings)\n",
        "# Performs benchmarks again allowing reranking of top results with full-precision embeddings\n",
        "evaluate_rerank_search(full_index, pq_index, query_embeddings, query_embeddings)"
      ],
      "metadata": {
        "id": "jT4fDaJvP-jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pq_index_name = \"pq_embeddings\"\n",
        "pq_index = index_pq_embeddings(outdoors_embeddings, pq_index_name, num_subvectors=64)\n",
        "\n",
        "# Performs benchmarks for search time, index size, and recall\n",
        "evaluate_search(full_index, pq_index, pq_index_name, query_embeddings, query_embeddings)\n",
        "# Performs benchmarks again allowing reranking of top results with full-precision embeddings\n",
        "evaluate_rerank_search(full_index, pq_index, query_embeddings, query_embeddings)"
      ],
      "metadata": {
        "id": "MefQoZPkQCxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Matryoshka Representation Learning"
      ],
      "metadata": {
        "id": "KEgI4N6GQRZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mrl_embeddings(embeddings, num_dimensions):\n",
        "  mrl_embeddings = np.array(list(map(lambda e: e[num_dimensions], embeddings)))\n",
        "  return mrl_embeddings\n",
        "\n",
        "def index_mrl_embeddings(doc_embeddings, num_dimensions, mrl_index_name):\n",
        "  mrl_doc_embeddings = get_mrl_embeddings(doc_embeddings, num_dimensions)\n",
        "  print(f\"{mrl_index_name} embeddings shape: {mrl_doc_embeddings.shape}\")\n",
        "  # An MRL index is a standard index, just with a reduced number of dimensions\n",
        "  mrl_index = index_full_precision_embeddings(mrl_doc_embeddings, mrl_index_name)\n",
        "  return mrl_index"
      ],
      "metadata": {
        "id": "DWhx8kVUQSVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original embeddings shape: {outdoors_embeddings.shape}\")\n",
        "# 1024 dimensions\n",
        "original_dimensions = outdoors_embeddings.shape[1]\n",
        "\n",
        "for num_dimensions in [\n",
        "    original_dimensions // 2,   # 512 dimensions\n",
        "    original_dimensions // 4,   # 256 dimensions\n",
        "    original_dimensions // 8]:  # 128 dimensions\n",
        "  mrl_index_name = f\"mrl_embeddings_{num_dimensions}\"\n",
        "  mrl_index = index_mrl_embeddings(outdoors_embeddings, num_dimensions, mrl_index_name)\n",
        "  mrl_queries = get_mrl_embeddings(query_embeddings, num_dimensions)\n",
        "  # Benchmark MRL search\n",
        "  evaluate_search(full_index, mrl_index, mrl_index_name, query_embeddings, mrl_queries)\n",
        "  # Benchmark MRL search + Reranking\n",
        "  evaluate_rerank_search(full_index, mrl_index, query_embeddings, mrl_queries)"
      ],
      "metadata": {
        "id": "saR9vYEMqh62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Combining techniques"
      ],
      "metadata": {
        "id": "BHet9CqTsLtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def index_binary_ivf_mrl_embeddings(reduced_mrl_doc_embeddings, binary_index_name):\n",
        "  # Binary quantization\n",
        "  binary_embeddings = quantize_embeddings(\n",
        "      reduced_mrl_doc_embeddings,\n",
        "      calibration_embeddings=reduced_mrl_doc_embeddings,\n",
        "      precision=\"binary\"\n",
        "  ).astype(np.unit8)\n",
        "\n",
        "  # Configuration so the index knows how the doc embeddings have been quantized\n",
        "  dimensions = reduced_mrl_doc_embeddings.shape[1]\n",
        "  quantizer = faiss.IndexBinaryFlat(dimensions)\n",
        "\n",
        "  # ANN-IVF Flat Algorithm: uses a binary-quantized IVF index for ANN search\n",
        "  num_clusters = 256\n",
        "  index = faiss.IndexBinaryIVF(quantizer, dimensions, num_clusters)\n",
        "  index.nprobe = 4\n",
        "\n",
        "  # Trains, adds documents, and saves the combined index to disk\n",
        "  index.train(binary_embeddings)\n",
        "  index.add(binary_embeddings)\n",
        "  faiss.write_index_binary(index, binary_index_name)\n",
        "  return index"
      ],
      "metadata": {
        "id": "i_K4-7CqsMah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MRL: gets reduced-dimension doc embeddings\n",
        "mrl_dimensions = outdoors_embeddings.shape[1] // 2\n",
        "reduced_mrl_doc_embeddings = get_mrl_embeddings(outdoors_embeddings, mrl_dimensions)"
      ],
      "metadata": {
        "id": "Ko3UTrb1UKbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_ivf_mrl_index_name = \"binary_ivf_mrl_embeddings\"\n",
        "binary_ivf_mrl_index = index_binary_ivf_mrl_embeddings(\n",
        "    reduced_mrl_doc_embeddings,\n",
        "    mrl_dimensions,\n",
        "    binary_ivf_mrl_index_name\n",
        ")"
      ],
      "metadata": {
        "id": "sLImJx4JUs4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MRL: gets reduced-dimension query embeddings\n",
        "mrl_queries = get_mrl_embeddings(query_embeddings, mrl_dimensions)\n",
        "# Binary quantization: applies quantization to the query embeddings\n",
        "quantized_queries = quantize_embeddings(\n",
        "      mrl_queries,\n",
        "      calibration_embeddings=reduced_mrl_doc_embeddings,\n",
        "      precision=\"binary\"\n",
        "  ).astype(np.unit8)"
      ],
      "metadata": {
        "id": "6b23CD-8VBut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmarks the binary ANN, binary quantization, and MRL embeddings\n",
        "evaluate_search(full_index, binary_ivf_mrl_index, binary_ivf_mrl_index_name, query_embeddings, quantized_queries)\n",
        "# Benchmarks again with reranking using full-precision embeddings\n",
        "evaluate_rerank_search(full_index, binary_ivf_mrl_index, query_embeddings, quantized_queries)"
      ],
      "metadata": {
        "id": "t_sHcqcYVsWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-encoders v/s bi-encoders"
      ],
      "metadata": {
        "id": "ieClag6vYkrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache_name = \"outdoors_semantic_search_embeddings\"\n",
        "\n",
        "def get_embeddings(texts, model, cache_name, ignore_cache=False):\n",
        "  cache_file_name = f\"data/outdoors/{cache_name}.pickle\"\n",
        "  if ignore_cache or not os.path.isfile(cache_file_name):\n",
        "    embeddings = model.encode(texts)\n",
        "    os.makedirs(os.path.dirname(cache_file_name), exist_ok=True)\n",
        "    with open(cache_file_name, \"wb\") as cache_file:\n",
        "      pickle.dump(embeddings, cache_file)\n",
        "  else:\n",
        "    with open(cache_file_name, \"rb\") as cache_file:\n",
        "      embeddings = pickle.load(cache_file)\n",
        "  return embeddings\n",
        "\n",
        "def normalize_embedding(embedding):\n",
        "  normalized = np.divide(embedding, np.linalg.norm(embedding))\n",
        "  return list(map(float, normalized))\n",
        "\n",
        "def print_labels(query, matches):\n",
        "  display(HTML(f\"<h4>Results for: <em>{query}</em></h4>\"))\n",
        "  for (l, d) in matches:\n",
        "    print(str(int(d * 1000) / 1000), \"|\", l)\n",
        "\n",
        "def display_results(query, search_results):\n",
        "    print_labels(query, [(d[\"title\"], d[\"score\"]) for d in search_results])\n",
        "\n",
        "def get_outdoor_titles():\n",
        "    outdoors_dataframe = pd.read_csv(\"data/outdoors/posts.csv\")\n",
        "    outdoors_dataframe = outdoors_dataframe[[\"id\", \"title\"]]\n",
        "    outdoors_dataframe = outdoors_dataframe.dropna()\n",
        "    # print(f\"Calculating embeddings for {outdoors_dataframe.count()} docs.\")\n",
        "    return outdoors_dataframe\n",
        "\n",
        "def bi_encoder_embedding_search(model, index, query, phrases, k=20, min_similarity=0.75):\n",
        "  matches = []\n",
        "  # Gets the embeddings for query\n",
        "  query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "  query_embedding = normalize_embedding(query_embedding)\n",
        "  ids, distances = index.knnQuery(query_embedding, k=k)\n",
        "  for i in range(len(ids)):\n",
        "    # Converts negative dot product distance into a positive dot product\n",
        "    distance = distances[i] * -1\n",
        "    if distance > min_similarity:\n",
        "      matches.append((phrases[ids[i]], distance))\n",
        "  if not len(matches):\n",
        "    # No neighbors found! Returns just the original term\n",
        "    matches.append((phrases[ids[1]], distances[1] * -1))\n",
        "  return matches\n",
        "\n",
        "def cross_encoder_semantic_search(model, titles, query, limit=10):\n",
        "    ranks = model.rank(query, titles)\n",
        "    # Print the scores\n",
        "    print(\"Results for:\", query)\n",
        "    for rank in ranks:\n",
        "      if rank['score'] > .25:\n",
        "        print(f\"{rank['score']:.2f}\\t{titles[rank['corpus_id']]}\")"
      ],
      "metadata": {
        "id": "9KtYBMrvsdzz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bi_encoder_model = SentenceTransformer(\"roberta-base-nli-stsb-mean-tokens\")\n",
        "cross_encoder_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
      ],
      "metadata": {
        "id": "FW6Gijsi4aO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache_name = \"outdoors_semantic_search_embeddings\"\n",
        "outdoor_titles = list(get_outdoor_titles()[\"title\"])\n",
        "embeddings = get_embeddings(outdoor_titles, bi_encoder_model, cache_name, ignore_cache=True)"
      ],
      "metadata": {
        "id": "nD3rQlgX51k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bi-Encoder"
      ],
      "metadata": {
        "id": "ERqy4quK5LTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do bi-encoder\n",
        "# initialize a new index, using a HNSW index on Dot Product\n",
        "titles_index = nmslib.init(method='hnsw', space='negdotprod')\n",
        "normalized_embeddings = list(map(normalize_embedding, embeddings))\n",
        "\n",
        "# All the embeddings can be added in a single batch\n",
        "titles_index.addDataPointBatch(normalized_embeddings)\n",
        "# Commits the index to memory. This must be done before you can query for nearest neighbors\n",
        "titles_index.createIndex(print_progress=True)"
      ],
      "metadata": {
        "id": "TV_HUQSA4IGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(query, phrases, log=False):\n",
        "  matches = bi_encoder_embedding_search(titles_index, query, phrases, k=5, min_similarity=0.6)\n",
        "  if log:\n",
        "    print_labels(query, matches)"
      ],
      "metadata": {
        "id": "aQtqrerI6qsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_search(\"mountain hike\", outdoor_titles, log=True)"
      ],
      "metadata": {
        "id": "-Vlh-TWn6rLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Encoder"
      ],
      "metadata": {
        "id": "Ypy4SUne6h3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"mountain hike\"\n",
        "\n",
        "# Generates a pair of query + document title to score for each document\n",
        "search_results = cross_encoder_semantic_search(cross_encoder_model, outdoor_titles, query)"
      ],
      "metadata": {
        "id": "gRgMi24rp0nH",
        "outputId": "5299c5ea-1674-4214-bc69-727a78ac30ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for: mountain hike\n",
            "1.53\tAn x-hour hike. Is that to the peak/summit or back and forth?\n",
            "1.32\tTimberland classic 6 inch boots for mountain hiking/trekking\n",
            "0.43\tWhat are the simplest 5000 m mountains to hike?\n",
            "0.32\tWhat constitutes mountain exposure when hiking or scrambling?\n",
            "0.31\tRifugio (Mountain Hut) trek in Austria\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}