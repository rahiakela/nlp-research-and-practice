{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPltLEUmsjZv3pdyers9YXb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/nlp-research-and-practice/blob/main/nlp-in-real-world/02-advanced-nlp-applications/02_ner_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "LCbxNJZpcfKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install transformers\n",
        "!pip -q install spacy\n",
        "!pip install spacy-transformers==1.1.5 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "3WhrYAcJcgVG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForTokenClassification\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "mXUnM_u0c059"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://noisy-text.github.io/2017/files/wnut17train.conll"
      ],
      "metadata": {
        "id": "k-sYdNzug3LE",
        "outputId": "ab98d660-ec1f-4482-e197-de94606d2de1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-22 09:53:49--  http://noisy-text.github.io/2017/files/wnut17train.conll\n",
            "Resolving noisy-text.github.io (noisy-text.github.io)... 185.199.109.153, 185.199.111.153, 185.199.108.153, ...\n",
            "Connecting to noisy-text.github.io (noisy-text.github.io)|185.199.109.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 493781 (482K) [application/octet-stream]\n",
            "Saving to: ‘wnut17train.conll’\n",
            "\n",
            "wnut17train.conll   100%[===================>] 482.21K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-11-22 09:53:49 (10.0 MB/s) - ‘wnut17train.conll’ saved [493781/493781]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##spaCy fine-tuning"
      ],
      "metadata": {
        "id": "JpidC2lIcm26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [\n",
        "    (\n",
        "        \"Chef added some salt and pepper to the rice.\",\n",
        "        {\n",
        "            \"entities\": [\n",
        "            (16, 20, 'SPICE'),\n",
        "            (25, 31, 'SPICE'),\n",
        "            (39, 43, 'INGREDIENT')\n",
        "          ]\n",
        "        }\n",
        "    ),\n",
        "    (\n",
        "        \"The pasta was set to boil with some salt.\",\n",
        "        {\n",
        "            \"entities\": [\n",
        "            (4, 9, 'INGREDIENT'),\n",
        "            (36, 40, 'SPICE')\n",
        "          ]\n",
        "        }\n",
        "    ),\n",
        "    (\n",
        "        \"Adding egg to the rice dish with some pepper.\",\n",
        "        {\n",
        "            \"entities\": [\n",
        "            (7, 10, 'INGREDIENT'),\n",
        "            (18, 22, 'INGREDIENT'),\n",
        "            (38, 44, 'SPICE')\n",
        "          ]\n",
        "        }\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "H4qMPoWamTde"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "print(\"Created a blank en model\")\n",
        "\n",
        "nlp.add_pipe(\"ner\", last=True)\n",
        "ner = nlp.get_pipe(\"ner\")\n",
        "print(\"pipe_names\", nlp.pipe_names)\n",
        "\n",
        "for _, annotations in train_data:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])\n",
        "\n",
        "# begin training\n",
        "optimizer = nlp.begin_training()"
      ],
      "metadata": {
        "id": "sOha62hycnfc",
        "outputId": "91a086fd-2a55-42d7-96c1-6da94bd7f58b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a blank en model\n",
            "pipe_names ['ner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_iter = 100\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiece\", \"trf_tok2vec\"]\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "  for _ in range(n_iter):\n",
        "    random.shuffle(train_data)\n",
        "    losses = {}\n",
        "    for batch in spacy.util.minibatch(train_data, size=2):\n",
        "      for text, annots in batch:\n",
        "        doc = nlp.make_doc(text)\n",
        "        nlp.update([Example.from_dict(doc, annots)], drop=0.5, sgd=optimizer, losses=losses)\n",
        "    print(f\"losses: {losses}\")"
      ],
      "metadata": {
        "id": "P02brnERdE57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entities(raw_text):\n",
        "  doc = nlp(raw_text)\n",
        "  result = []\n",
        "  for word in doc.ents:\n",
        "    result.append((word.text, word.label_))\n",
        "  return result"
      ],
      "metadata": {
        "id": "ArxlMZBEh1IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_entities(\"Add water to the spaghetti\"))"
      ],
      "metadata": {
        "id": "EFfn3jFaX3EV",
        "outputId": "e0359dc3-ccd7-47d3-e323-0efbd1f75561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('water', 'SPICE'), ('spaghetti', 'SPICE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_entities(\"Add some paprika on top to your pasta.\"))"
      ],
      "metadata": {
        "id": "faN039OjX7xQ",
        "outputId": "4f2b7fd7-a3f5-497e-9308-da93a52f4130",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('paprika', 'SPICE'), ('pasta', 'INGREDIENT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformers fine-tuning"
      ],
      "metadata": {
        "id": "SocN2Ugna8Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_tokens(raw_text):\n",
        "  raw_docs = re.split(r\"\\n\\t?\\n\", raw_text)\n",
        "  token_docs = []\n",
        "  tag_docs = []\n",
        "\n",
        "  for doc in raw_docs:\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    for line in doc.split(\"\\n\"):\n",
        "      row = line.split(\"\\t\")\n",
        "      if len(row) == 1:\n",
        "        token = row[0]\n",
        "        tag = None\n",
        "      else:\n",
        "        token, tag = line.split(\"\\t\")\n",
        "      tokens.append(token)\n",
        "      tags.append(tag)\n",
        "    token_docs.append(tokens)\n",
        "    tag_docs.append(tags)\n",
        "  return token_docs, tag_docs"
      ],
      "metadata": {
        "id": "ho85PyOlbGJr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_wnut(file_path):\n",
        "  file_path = Path(file_path)\n",
        "\n",
        "  raw_text = file_path.read_text().strip()\n",
        "  token_docs, tag_docs = split_into_tokens(raw_text)\n",
        "  return token_docs, tag_docs"
      ],
      "metadata": {
        "id": "PwD8lv7OstD8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts, tags = read_wnut(\"wnut17train.conll\")"
      ],
      "metadata": {
        "id": "z895Dn8av6gK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0][10:17], tags[0][10:17], sep=\"\\n\")"
      ],
      "metadata": {
        "id": "kJJQs4VkwB6d",
        "outputId": "3bc52535-cf41-43c4-c5f7-cfdfc214f81a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['for', 'two', 'weeks', '.', 'Empire', 'State', 'Building']\n",
            "['O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting our data into training and validation set\n",
        "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=0.2)"
      ],
      "metadata": {
        "id": "VSXC1EVywLzt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's encode the tokens, using pre-trained DistilBert tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-cased\")"
      ],
      "metadata": {
        "id": "QnKACCLzxCnX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(\n",
        "    train_texts,\n",
        "    is_split_into_words=True,     # we have ready-split tokens\n",
        "    return_offsets_mapping=True,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "val_encodings = tokenizer(\n",
        "    val_texts,\n",
        "    is_split_into_words=True,     # we have ready-split tokens\n",
        "    return_offsets_mapping=True,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")"
      ],
      "metadata": {
        "id": "x1ufmX2_eTyL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "metadata": {
        "id": "mnIH1CH0e_Tu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_tags(tags, encodings):\n",
        "  labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
        "  encoded_labels = []\n",
        "  for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
        "    # create an empty array of -100\n",
        "    doc_enc_labels = np.ones(len(doc_offset), dtype=int) * -100\n",
        "    arr_offset = np.array(doc_offset)\n",
        "    # set labels whose first offset position is 0 and the second is not 0\n",
        "    doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)] = doc_labels\n",
        "    encoded_labels.append(doc_enc_labels.tolist())\n",
        "  return encoded_labels"
      ],
      "metadata": {
        "id": "QCWesOpff-X_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)\n",
        "print(f\"There are total {len(tag2id.keys())} entity tags in the data: {tag2id.keys()}\")"
      ],
      "metadata": {
        "id": "jDrQrKbOhJ5t",
        "outputId": "658d0063-5836-4e95-a851-e9e9b4430710",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are total 13 entity tags in the data: dict_keys(['I-group', 'O', 'I-product', 'B-product', 'B-group', 'B-creative-work', 'I-corporation', 'B-location', 'I-person', 'B-person', 'I-creative-work', 'I-location', 'B-corporation'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings.pop(\"offset_mapping\")  # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (dict(train_encodings), train_labels)\n",
        ")\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (dict(val_encodings), val_labels)\n",
        ")"
      ],
      "metadata": {
        "id": "3-xGRzvXiJK4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can load in a token classification model and specify the number of labels.\n",
        "\n",
        "Then, our model is ready for fine-tuning."
      ],
      "metadata": {
        "id": "lz3edLHBkaxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFDistilBertForTokenClassification.from_pretrained(\"distilbert-base-cased\", num_labels=len(unique_tags))"
      ],
      "metadata": {
        "id": "BMsc8EL1kUtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do fine-tuning\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=model.compute_loss)\n",
        "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)"
      ],
      "metadata": {
        "id": "SMtQrDYekqZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aIza72EulrQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}