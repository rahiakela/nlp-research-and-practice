{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPeEiL6zTg3vpdodLcVfq37",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/applied-text-analysis-with-python/4_text_vectorization_and_transformation_pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Vectorization and Transformation Pipeline"
      ],
      "metadata": {
        "id": "WivVR4njQ_RN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will demonstrate how to use the vectorization process to combine\n",
        "linguistic techniques from NLTK with machine learning techniques in Scikit-Learn\n",
        "and Gensim, creating custom transformers that can be used inside repeatable and\n",
        "reusable pipelines.\n",
        "\n",
        "In order\n",
        "to perform machine learning on text, we need to transform our documents into vector\n",
        "representations such that we can apply numeric machine learning. This process is\n",
        "called feature extraction or more simply, vectorization, and is an essential first step\n",
        "toward language-aware analysis.\n",
        "\n",
        "Representing documents numerically gives us the ability to perform meaningful ana\n",
        "lytics\n",
        "and also creates the instances on which machine learning algorithms operate\n",
        "\n",
        "For this reason, we must now make a critical shift in how we think about language—\n",
        "from a sequence of words to points that occupy a high-dimensional semantic space.\n",
        "Points in space can be close together or far apart, tightly clustered or evenly distributed.\n",
        "\n",
        "By\n",
        "encoding similarity as distance, we can begin to derive the primary components of\n",
        "documents and draw decision boundaries in our semantic space.\n",
        "\n",
        "The simplest encoding of semantic space is the bag-of-words model, whose primary\n",
        "insight is that meaning and similarity are encoded in vocabulary."
      ],
      "metadata": {
        "id": "4Agr70PyRK5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "sy8djaWNR-63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string"
      ],
      "metadata": {
        "id": "OeCMS-sVSAtv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "  \"The elephant sneezed at the sight of potatoes.\",\n",
        "  \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
        "  \"Wondering, she opened the door to the studio.\",\n",
        "]"
      ],
      "metadata": {
        "id": "fK6gRo94dcVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Words in Space"
      ],
      "metadata": {
        "id": "NjcyUc6LSA4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will look at four types of vector encoding—frequency,\n",
        "one-hot, TF–IDF, and distributed representations—and discuss their implementations\n",
        "in Scikit-Learn, Gensim, and NLTK.\n",
        "\n",
        "To set this up, let’s create a list of our documents and tokenize them for the proceeding\n",
        "vectorization examples."
      ],
      "metadata": {
        "id": "0rEPgZmISQbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  stem = nltk.stem.SnowballStemmer(\"english\")\n",
        "  text = text.lower()\n",
        "\n",
        "  for token in nltk.word_tokenize(text):\n",
        "    if token in string.punctuation:\n",
        "      continue\n",
        "    yield stem.stem(token)"
      ],
      "metadata": {
        "id": "rYKuC_oASBsb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Frequency Vectors"
      ],
      "metadata": {
        "id": "wU7iCK11dY0a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DS8kDzI3daC6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}