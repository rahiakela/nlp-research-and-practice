{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/nlp-research-and-practice/blob/main/ai-powered-search/13_3_semantic_search_with__quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51uM8gPIW0mO"
      },
      "source": [
        "## Setup\n",
        "\n",
        "In this notebook, we\"re going to install a transformer model, analyze the embedding output, and compare some vectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#outdoors\n",
        "![ ! -d 'outdoors' ] && git clone --depth=1 https://github.com/ai-powered-search/outdoors.git\n",
        "! cd outdoors && git pull\n",
        "! cd outdoors && cat outdoors.tgz.part* > outdoors.tgz\n",
        "! cd outdoors && mkdir -p '../data/outdoors/' && tar -xvf outdoors.tgz -C '../data/outdoors/'"
      ],
      "metadata": {
        "id": "5xShS9pNXKtl",
        "outputId": "9eeb5c66-70cb-4b4b-c0d0-7cbf955ce6c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'outdoors'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 25 (delta 0), reused 22 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (25/25), 491.39 MiB | 25.47 MiB/s, done.\n",
            "Updating files: 100% (23/23), done.\n",
            "Already up to date.\n",
            "README.md\n",
            "concepts.pickle\n",
            "._guesses.csv\n",
            "guesses.csv\n",
            "._guesses_all.json\n",
            "guesses_all.json\n",
            "outdoors_concepts.pickle\n",
            "outdoors_embeddings.pickle\n",
            "._outdoors_golden_answers.csv\n",
            "outdoors_golden_answers.csv\n",
            "._outdoors_golden_answers.xlsx\n",
            "outdoors_golden_answers.xlsx\n",
            "._outdoors_golden_answers_20210130.csv\n",
            "outdoors_golden_answers_20210130.csv\n",
            "outdoors_labels.pickle\n",
            "outdoors_question_answering_contexts.json\n",
            "outdoors_questionanswering_test_set.json\n",
            "outdoors_questionanswering_train_set.json\n",
            "._posts.csv\n",
            "posts.csv\n",
            "predicates.pickle\n",
            "pull_aips_dependency.py\n",
            "._question-answer-seed-contexts.csv\n",
            "question-answer-seed-contexts.csv\n",
            "question-answer-squad2-guesses.csv\n",
            "._roberta-base-squad2-outdoors\n",
            "roberta-base-squad2-outdoors/\n",
            "roberta-base-squad2-outdoors/._tokenizer_config.json\n",
            "roberta-base-squad2-outdoors/tokenizer_config.json\n",
            "roberta-base-squad2-outdoors/._special_tokens_map.json\n",
            "roberta-base-squad2-outdoors/special_tokens_map.json\n",
            "roberta-base-squad2-outdoors/._config.json\n",
            "roberta-base-squad2-outdoors/config.json\n",
            "roberta-base-squad2-outdoors/._merges.txt\n",
            "roberta-base-squad2-outdoors/merges.txt\n",
            "roberta-base-squad2-outdoors/._training_args.bin\n",
            "roberta-base-squad2-outdoors/training_args.bin\n",
            "roberta-base-squad2-outdoors/._pytorch_model.bin\n",
            "roberta-base-squad2-outdoors/pytorch_model.bin\n",
            "roberta-base-squad2-outdoors/._vocab.json\n",
            "roberta-base-squad2-outdoors/vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install faiss-cpu --no-cache\n",
        "# !pip install faiss-gpu"
      ],
      "metadata": {
        "id": "HQRWkAsbmB_i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xA3xmRRHW0mQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "sys.path.append(\"../..\")\n",
        "# from aips import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import tqdm\n",
        "\n",
        "import faiss\n",
        "import sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
        "from sentence_transformers.quantization import quantize_embeddings\n",
        "\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ruXhtGPW0mR"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\n",
        "    \"mixedbread-ai/mxbai-embed-large-v1\",\n",
        "    similarity_fn_name=SimilarityFunction.DOT_PRODUCT,\n",
        "    truncate_dim=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get embeddings"
      ],
      "metadata": {
        "id": "KzFEJ0VxFGTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(texts, model, cache_name, ignore_cache=False):\n",
        "  cache_file_name = f\"data/outdoors/{cache_name}.pickle\"\n",
        "  if ignore_cache or not os.path.isfile(cache_file_name):\n",
        "    embeddings = model.encode(texts, normalize_embeddings=True)\n",
        "    os.makedirs(os.path.dirname(cache_file_name), exist_ok=True)\n",
        "    with open(cache_file_name, \"wb\") as cache_file:\n",
        "      pickle.dump(embeddings, cache_file)\n",
        "  else:\n",
        "    with open(cache_file_name, \"rb\") as cache_file:\n",
        "      embeddings = pickle.load(cache_file)\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "WipDDFrAFdsl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(scores, ids, data):\n",
        "    results = generate_search_results(scores, ids, data)\n",
        "    display(results)\n",
        "    return results\n",
        "\n",
        "def get_outdoors_data():\n",
        "    outdoors_dataframe = pd.read_csv(\"data/outdoors/posts.csv\")\n",
        "    outdoors_data = list(outdoors_dataframe.to_dict())\n",
        "    return outdoors_data\n",
        "\n",
        "def display_statistics(search_results, baseline_search_results=None, start_message=\"Recall\"):\n",
        "    index_name = search_results[\"index_name\"]\n",
        "    time_taken = search_results[\"time_taken\"]\n",
        "    index_size = search_results[\"size\"]\n",
        "    improvement_ms = \"\"\n",
        "    improvement_size = \"\"\n",
        "    recall = 1.0\n",
        "    if baseline_search_results:\n",
        "        full_search_time = baseline_search_results[\"time_taken\"]\n",
        "        time_imp = round((full_search_time - time_taken) * 100 / full_search_time, 2)\n",
        "        improvement_ms = f\" ({time_imp}% improvement)\"\n",
        "        improvement_size = f\" ({round((baseline_search_results['size'] - index_size) * 100 / baseline_search_results['size'], 2)}% improvement)\"\n",
        "        recall = calculate_recall(baseline_search_results[\"results\"], search_results[\"results\"])\n",
        "\n",
        "    print(f\"{index_name} search took: {time_taken:.3f} ms{improvement_ms}\")\n",
        "    print(f\"{index_name} index size: {round(index_size / 1000000, 2)} MB{improvement_size}\")\n",
        "    print(f\"{start_message}: {round(recall, 4)}\")\n",
        "\n",
        "def calculate_recall(scored_full_results, scored_quantized_results):\n",
        "    recalls = []\n",
        "    for i in range(len(scored_full_results)):\n",
        "        full_ids = [r[\"id\"] for r in scored_full_results[i]]\n",
        "        quantized_ids = [r[\"id\"] for r in scored_quantized_results[i]]\n",
        "        recalls.append((len(set(full_ids).intersection(set(quantized_ids))) /\n",
        "                       len(set(quantized_ids))))\n",
        "    return sum(recalls) / len(recalls)\n",
        "\n",
        "def generate_search_results(faiss_scores, faiss_ids):\n",
        "    outdoors_data = get_outdoors_data()\n",
        "    faiss_results = []\n",
        "    for i in range(len(faiss_scores)):\n",
        "        results = []\n",
        "        for j, id in enumerate(faiss_ids[i]):\n",
        "            id = int(id)\n",
        "            result = {\"score\": faiss_scores[i][j],\n",
        "                      \"title\": outdoors_data[id][\"title\"],\n",
        "                      \"body\": outdoors_data[id][\"body\"],\n",
        "                      \"id\": id}\n",
        "            results.append(result)\n",
        "        faiss_results.append(results)\n",
        "    return faiss_results\n",
        "\n",
        "def time_and_execute_search(index, index_name, query_embeddings, k=25, num_runs=100):\n",
        "    search_times = []\n",
        "    faiss_scores = None\n",
        "    faiss_ids = None\n",
        "\n",
        "    for i in range(num_runs):\n",
        "        start_time = time.time()\n",
        "        faiss_scores, faiss_ids = index.search(query_embeddings, k=k)\n",
        "        time_taken = ((time.time() - start_time) * 1000)\n",
        "        search_times.append(time_taken)\n",
        "\n",
        "    results = {\"results\": generate_search_results(faiss_scores, faiss_ids),\n",
        "               \"time_taken\": np.average(search_times),\n",
        "               \"faiss_scores\": faiss_scores, \"faiss_ids\": faiss_ids}\n",
        "    index_stats = {}\n",
        "    if index_name:\n",
        "        index_stats ={\n",
        "            \"index_name\": index_name,\n",
        "            \"size\": os.path.getsize(index_name)\n",
        "        }\n",
        "    return results | index_stats"
      ],
      "metadata": {
        "id": "Z05FCJIODT-9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scalar quantization"
      ],
      "metadata": {
        "id": "Zbjb6yEHPscY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outdoors_dataframe = pd.read_csv(\"data/outdoors/posts.csv\")\n",
        "post_texts = [f\"{post['title']} {post['body']}\" for _, post in outdoors_dataframe.iterrows()]\n",
        "# post_texts"
      ],
      "metadata": {
        "id": "sx8hTmXfrDhS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's index full-precision embeddings using FAISS\n",
        "def index_full_precision_embeddings(doc_embeddings, name):\n",
        "  # IndexFlatIP is a simple, unoptimized index supporting different embedding formats\n",
        "  index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
        "  index.add(doc_embeddings)      # Adds documents to the index\n",
        "  faiss.write_index(index, name) # Writes the index to disk\n",
        "  return index\n",
        "\n",
        "def get_outdoors_embeddings(model):\n",
        "  outdoors_dataframe = pd.read_csv(\"data/outdoors/posts.csv\")\n",
        "  post_texts = [\n",
        "      f\"{post['title']} {post['body']}\"\n",
        "      for _, post in outdoors_dataframe.iterrows()\n",
        "  ]\n",
        "  return np.array(get_embeddings(post_texts, model, \"outdoors_mrl_normed\"))\n",
        "\n",
        "# Generates embeddings for the outdoors dataset\n",
        "outdoors_embeddings = get_outdoors_embeddings(model)\n",
        "# Creates a full-precision(Float32) FAISS index\n",
        "full_index = index_full_precision_embeddings(outdoors_embeddings, \"full_embeddings\")"
      ],
      "metadata": {
        "id": "sNWZ9u50OifI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's generate full-precision query embeddings\n",
        "def get_test_queries():\n",
        "  return[\n",
        "      \"tent poles\", \"hiking trails\", \"mountain forests\",\n",
        "      \"white water\", \"best waterfalls\", \"mountain biking\",\n",
        "      \"snowboarding slopes\", \"bungee jumping\", \"public parks\"\n",
        "  ]\n",
        "\n",
        "# Gets test queries for benchmarking\n",
        "queries = get_test_queries()\n",
        "# Generates embeddings for each query\n",
        "query_embeddings = model.encode(queries, normalize_embeddings=True, convert_to_numpy=True)\n",
        "\n",
        "# Generates search time, index size, and recall statistics for the full-precision (Float32) index\n",
        "full_results = time_and_execute_search(full_index, \"full_embeddings\", query_embeddings, k=25)\n",
        "# Displays the benchmarking stats for the full-precision index\n",
        "display_statistics(full_results)"
      ],
      "metadata": {
        "id": "GDKLy9PWoxyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define functions for benchmark quantized search approaches\n",
        "def evaluate_search(full_index, optimized_index, optimized_index_name,\n",
        "                    query_embeddings, optimized_query_embeddings,\n",
        "                    k=25, display=True, log=False):\n",
        "    full_results = time_and_execute_search(full_index, \"full_embeddings\", query_embeddings, k=k)\n",
        "    optimized_results = time_and_execute_search(optimized_index, optimized_index_name, optimized_query_embeddings, k=k)\n",
        "    if display:\n",
        "        display_statistics(optimized_results, full_results)\n",
        "    return optimized_results, full_results\n",
        "\n",
        "def evaluate_rerank_search(full_index, optimized_index,\n",
        "                           query_embeddings,\n",
        "                           optimized_embeddings,\n",
        "                           k=50, limit=25):\n",
        "    results, full_results = evaluate_search(full_index, optimized_index, None, query_embeddings,\n",
        "                                            optimized_embeddings, display=False, k=k)\n",
        "\n",
        "    doc_embeddings = get_outdoors_embeddings(model) #This can point to a cheap on-disk data source containing the original full-precision embeddings\n",
        "    rescore_scores, rescore_ids = [], []\n",
        "    for i in range(len(results[\"results\"])):\n",
        "        embedding_ids = results[\"faiss_ids\"][i]\n",
        "        top_k_embeddings = [doc_embeddings[id] for id in embedding_ids]\n",
        "        query_embedding = query_embeddings[i]\n",
        "        scores = query_embedding @ numpy.array(top_k_embeddings).T\n",
        "        indices = scores.argsort()[::-1][:limit]\n",
        "        top_k_indices = embedding_ids[indices]\n",
        "        top_k_scores = scores[indices]\n",
        "        rescore_scores.append(top_k_scores)\n",
        "        rescore_ids.append(top_k_indices)\n",
        "\n",
        "    results = generate_search_results(rescore_scores, rescore_ids)\n",
        "    recall = calculate_recall(full_results[\"results\"], results)\n",
        "    print(f\"Reranked recall: {round(recall, 4)}\")"
      ],
      "metadata": {
        "id": "9MzyzbqTrjl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s implement Int8 scalar quantization"
      ],
      "metadata": {
        "id": "GOqBwlcwrnwc",
        "outputId": "b88b9a23-c667-49f3-d988-6d35c9c24079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Results for: <em>dehyd</em></h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.633 | The re-hydration time for deydrated foods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_search(\"polar bear\", titles, log=True)"
      ],
      "metadata": {
        "id": "-Tiwj_NIr3wi",
        "outputId": "9f4997c3-58b6-4ad2-da46-e49471dff8db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Results for: <em>polar bear</em></h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.611 | Bear spray vs. rifles against polar bears?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_search(\"bear\", titles, log=True)"
      ],
      "metadata": {
        "id": "mpwq95wBr_YE",
        "outputId": "f9acfd8f-d31d-47cd-b437-76af4321846b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Results for: <em>bear</em></h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.63 | Running in bear country\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}