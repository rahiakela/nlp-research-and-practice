{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0hK4nU35JVaA",
        "N4dDTs-rJL_h",
        "TycHgKTZJ6a6"
      ],
      "authorship_tag": "ABX9TyMlvgqx8Oc2Su+RRxtJuL+G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/nlp-research-and-practice/blob/main/text-similarity-works/19_icd_10_code_common_keyword_and_keyword_impairment_highlighting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "u6vazPC0Ja4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "!pip install pillow\n",
        "\n",
        "!sudo apt install build-essential libpoppler-cpp-dev pkg-config python3-dev\n",
        "!pip install -U pdftotext\n",
        "!pip install PyPDF2\n",
        "!pip install fitz\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "gdwgVvXuJdz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "id": "4QPcnYT5AKpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import fitz\n",
        "import pdftotext\n",
        "from PyPDF2 import PdfFileReader, PdfReader, PdfFileWriter, PdfWriter\n",
        "\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "from concurrent import futures\n",
        "from keyword_extraction import call"
      ],
      "metadata": {
        "id": "gKbd7WsfyYX3",
        "outputId": "02795618-951c-463a-cfec-fc49a3c6ef31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "/usr/local/lib/python3.8/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Imv6fB5gPBe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee370edc-99d9-4499-bc59-080d2a50bfad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p input_files"
      ],
      "metadata": {
        "id": "2dfljcHuOVw3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Core Classes"
      ],
      "metadata": {
        "id": "9oNqjgIsRpNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Highlighter:\n",
        "    def __init__(self):\n",
        "      # loading and updating patterns for ICD-10 code\n",
        "      self.nlp_code10 = English()\n",
        "      self.nlp_code10.add_pipe(\"entity_ruler\").from_disk(\"icd10_code_patterns-v5.jsonl\")\n",
        "\n",
        "      # loading stop words\n",
        "      self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "      # define icd-10 code dataset\n",
        "      self.code_df = pd.read_csv(\"icd_10_code_and_keywords_v2.csv\")\n",
        "\n",
        "      # define required directory path\n",
        "      self.PDF_FILES_PATH = \"pdf-files\"\n",
        "      self.TXT_FILES_PATH = \"txt-files\"\n",
        "      self.OUTPUT_FILES_PATH = \"output\"\n",
        "      create_directory(self.PDF_FILES_PATH)\n",
        "      create_directory(self.TXT_FILES_PATH)\n",
        "      create_directory(self.OUTPUT_FILES_PATH)\n",
        "\n",
        "      self.code_y2_coords_list = []\n",
        "\n",
        "    def set_impairment_keyword_dict(self, imp_keyword_dict):\n",
        "      self.impairment_keyword_dict = imp_keyword_dict\n",
        "\n",
        "    def split_pdf(self, pdf_path):\n",
        "        pdf_in_file = open(pdf_path, \"rb\")\n",
        "        pdf = PdfReader(pdf_in_file)\n",
        "        pdf_list = []\n",
        "        for page in range(len(pdf.pages)):\n",
        "            input_pdf = PdfReader(pdf_in_file)\n",
        "            output = PdfWriter()\n",
        "            output.add_page(input_pdf.pages[page])\n",
        "            with open(f\"{self.PDF_FILES_PATH}/page-{page}.pdf\", \"wb\") as outputStream:\n",
        "                output.write(outputStream)\n",
        "                pdf_list.append(f\"page-{page}.pdf\")\n",
        "        return pdf_list\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_list):\n",
        "        txt_file_list = []\n",
        "        i = 0\n",
        "        for pdf_file in pdf_list:\n",
        "            with open(os.path.join(self.PDF_FILES_PATH, pdf_file), \"rb\") as f:\n",
        "                pdf = pdftotext.PDF(f)\n",
        "\n",
        "            # Read all the text into one string\n",
        "            pdf_text = \"\\n\\n\".join(pdf)\n",
        "\n",
        "            # write text into file\n",
        "            with open(f\"{self.TXT_FILES_PATH}/page-{str(i)}.txt\", \"a\") as f:\n",
        "                f.write(pdf_text)\n",
        "            txt_file_list.append(f\"{self.TXT_FILES_PATH}/page-{str(i)}.txt\")\n",
        "            i += 1\n",
        "        self.text_list = txt_file_list\n",
        "        return txt_file_list\n",
        "\n",
        "    def highlight_icd_code(self, icd10_code_dict, match_threshold=30, coordinate=False, pdf_file_name=None, cords_file_name=None):\n",
        "        pdf_file = fitz.open(pdf_file_name)\n",
        "        # create file to write coordinate\n",
        "        txt_output_file_name = open(f\"{self.OUTPUT_FILES_PATH}/{cords_file_name}\", \"a\")\n",
        "        json_data = []\n",
        "        # add file header\n",
        "        txt_output_file_name.write(\n",
        "            \"| Page | Found Code | Actual ICD10-Code | ICD 10 description | Common Words | Matched paragraph | confidence |\\n \")\n",
        "\n",
        "        def highlight_code(page_obj, p_highlight, match_score=0):\n",
        "            # highlight code if threshold is more than match threshold and common words exists\n",
        "            if match_score >= match_threshold:\n",
        "                page_highlight = page_obj.add_highlight_annot(p_highlight)\n",
        "                page_highlight.set_colors(stroke=[0.66, 1, 0.07])  # light green\n",
        "                page_highlight.update()\n",
        "            else:\n",
        "                # highlight and set color coding dont have common words\n",
        "                page_highlight = page.add_highlight_annot(p_highlight)\n",
        "                page_highlight.set_colors(stroke=[0.92, 0.59, 0.48])  # dark salmon\n",
        "                page_highlight.update()\n",
        "\n",
        "        def highlight_alternate_code(page_obj, p_code):\n",
        "          alternate_code_list = self.get_alternate_code_pattern(p_code)\n",
        "          for alt_code in alternate_code_list:\n",
        "            highlight_list = page.search_for(alt_code)\n",
        "            for highlight in highlight_list:\n",
        "              if len(highlight) > 0:\n",
        "                print(f\"alternate-code:{code}, Coordinate: {highlight}\")\n",
        "                #self.code_y2_coords_list.extend(highlight[3])\n",
        "                # highlight pdf for option pattern\n",
        "                highlight_code(page_obj, highlight)\n",
        "\n",
        "        def highlight_common_words(page_obj, common_words_coord_dict):\n",
        "            highlight_coords_dict = {}\n",
        "            highlighted_word_list = []\n",
        "            for common_word, common_word_coord in common_words_coord_dict.items():\n",
        "                highlight = page_obj.add_highlight_annot(common_word_coord)\n",
        "                highlight.update()\n",
        "                highlight_coords_dict[common_word] = common_word_coord\n",
        "                highlighted_word_list.append(common_word)\n",
        "            return highlight_coords_dict, highlighted_word_list\n",
        "\n",
        "        def highlight_keyword_impairment(page_obj, imp_keywords_coord_dict):\n",
        "            highlight_coords_dict = {}\n",
        "            for common_word, imp_keywords_coord in imp_keywords_coord_dict.items():\n",
        "              highlight = page_obj.add_highlight_annot(imp_keywords_coord)\n",
        "              highlight.set_colors(stroke=[0.98, 0.80, 0.69])  # apricot\n",
        "              highlight.update()\n",
        "              highlight_coords_dict[common_word] = imp_keywords_coord\n",
        "            return highlight_coords_dict\n",
        "\n",
        "        for page_num, page in enumerate(pdf_file):\n",
        "          already_done_page_list = []\n",
        "          self.already_found_word_list = []\n",
        "          # highlight ICD-10 code\n",
        "          if page_num in icd10_code_dict:\n",
        "            # let's store code y2 coords \n",
        "            for code in icd10_code_dict[page_num]:\n",
        "              highlight_list = page.search_for(code)\n",
        "              self.code_y2_coords_list.extend([highlight[3] for highlight in highlight_list])\n",
        "            # now, let highlight every code and its common words\n",
        "            for code in icd10_code_dict[page_num]:\n",
        "                num_page = page_num + 1\n",
        "                highlight_list = page.search_for(code)\n",
        "                print(f\"Page-{num_page}-{code}, Coordinate: {highlight_list}\")\n",
        "                # highlight_list = fn(highlight_list)\n",
        "                # highlight code that dont have coordinate using alternate pattern\n",
        "                if len(highlight_list) == 0:\n",
        "                  highlight_alternate_code(page, code)\n",
        "                \n",
        "                for highlight in highlight_list:\n",
        "                    json_data_object = {}\n",
        "                    keyword = self.get_keyword(code)\n",
        "                    # get match score and common words coordinate\n",
        "                    all_match_keyword_list, all_match_imp_keyword_list = self.get_best_token_match(page_num, code, page, highlight[3], match_threshold)\n",
        "\n",
        "                    # Step-1: highlight and set color coding dont have common words\n",
        "                    if not all_match_keyword_list:\n",
        "                        page_highlight = page.add_highlight_annot(highlight)\n",
        "                        page_highlight.set_colors(stroke=[0.92, 0.59, 0.48])  # dark salmon\n",
        "                        page_highlight.update()\n",
        "                        # prepare json object\n",
        "                        json_data_object[f\"page-{num_page}\"] = {\n",
        "                            \"icd10_code\": reverse_code_pattern(code),\n",
        "                            \"page_icd10_code\": code,\n",
        "                            \"page_icd10_code_coord\": f\"{highlight}\",\n",
        "                            \"icd10_code_desc\": keyword if keyword else \"UNVALIDATED\",\n",
        "                            \"page_icd_common_words\": \"UNVALIDATED\",\n",
        "                            \"page_icd_keywords_coord\": {},\n",
        "                            \"page_icd_keywords_found\": False,\n",
        "                            \"match_confidence\": 0\n",
        "                        }\n",
        "                        json_data.append(json_data_object)\n",
        "                        # log data to file\n",
        "                        code_cors_output = (f\"|Page-{num_page} | {code} | {reverse_code_pattern(code)}\"\n",
        "                                            f\"| {keyword if keyword else 'No keyword found'} | 'UNVALIDATED' | 0 |\")\n",
        "                        txt_output_file_name.write(\"%s\\n\" % code_cors_output)\n",
        "                        if coordinate:\n",
        "                            txt_output_file_name.write(\"%s\\n\" % f\"|{reverse_code_pattern(code)}:{highlight}|\")\n",
        "\n",
        "                    # Step-2: highlight and set color coding that have common words\n",
        "                    for match_keyword_dict in all_match_keyword_list:\n",
        "                      # highlight ICD-10 code\n",
        "                      highlight_code(page, highlight, match_keyword_dict[\"score\"])\n",
        "                      # highlight common words\n",
        "                      highlight_coords_dict = {}\n",
        "                      highlight_coords_dict, highlighted_word_list = highlight_common_words(page, match_keyword_dict[\"common_words_coords\"])\n",
        "                      # write all info into text file\n",
        "                      if match_keyword_dict[\"score\"] >= match_threshold:\n",
        "                          # | Page | Found Code | Actual ICD10-Code | ICD 10 description | Common Words | confidence |\n",
        "                          code_cors_output = (f\"|Page-{num_page} | {code} | {reverse_code_pattern(code)}\"\n",
        "                                              f\" | {keyword if keyword else 'No keyword found'}\"\n",
        "                                              f\" | {match_keyword_dict['common_words'] if len(match_keyword_dict['common_words']) > 0 else 'UNVALIDATED'}\"\n",
        "                                              f\" | {match_keyword_dict['paragraph']} | {match_keyword_dict['score']} |\")\n",
        "                          txt_output_file_name.write(\"%s\\n\" % code_cors_output)\n",
        "                      else:\n",
        "                          code_cors_output = (\n",
        "                              f\"|Page-{num_page} | {code} | {reverse_code_pattern(code)} | {keyword if keyword else 'No keyword found'}\"\n",
        "                              f\" | {match_keyword_dict['common_words'] if len(match_keyword_dict['common_words']) > 0 else 'UNVALIDATED'}\"\n",
        "                              f\" | {match_keyword_dict['paragraph']} | {match_keyword_dict['score']} |\")\n",
        "                          txt_output_file_name.write(\"%s\\n\" % code_cors_output)\n",
        "                      if coordinate:\n",
        "                          txt_output_file_name.write(\"%s\\n\" % f\"|{reverse_code_pattern(code)}:{highlight}|\")\n",
        "                          txt_output_file_name.write(\"%s\\n\" % f\"|{highlight_coords_dict}|\")\n",
        "\n",
        "                      # prepare json object\n",
        "                      json_data_object[f\"page-{num_page}\"] = {\n",
        "                          \"icd10_code\": reverse_code_pattern(code),\n",
        "                          \"page_icd10_code\": code,\n",
        "                          \"page_icd10_code_coord\": f\"{highlight}\",\n",
        "                          \"icd10_code_desc\": keyword if keyword else \"UNVALIDATED\",\n",
        "                          \"page_icd_common_words\": match_keyword_dict[\"common_words\"] if len(\n",
        "                              match_keyword_dict[\"common_words\"]) > 0 else 'UNVALIDATED',\n",
        "                          \"page_icd_keywords_coord\": f\"{highlight_coords_dict}\",\n",
        "                          \"page_icd_keywords_found\": \"True\" if len(match_keyword_dict[\"common_words\"]) > 0 else \"False\",\n",
        "                          \"match_confidence\": match_keyword_dict['score']\n",
        "                      }\n",
        "                      json_data.append(json_data_object)\n",
        "                    \n",
        "                    # Step-3: highlight and set color coding for impairment keyword common words\n",
        "                    for match_imp_keyword_dict in all_match_imp_keyword_list:\n",
        "                      # highlight common words\n",
        "                      highlight_coords_dict = {}\n",
        "                      if \"imp_common_keyword_coords\" in match_imp_keyword_dict:\n",
        "                        print(f\"match_imp_keyword_dict: \\n{match_imp_keyword_dict['imp_common_keyword_coords']}\")\n",
        "                        highlight_coords_dict = highlight_keyword_impairment(page, match_imp_keyword_dict[\"imp_common_keyword_coords\"])\n",
        "                        already_done_page_list.append(page_num)\n",
        "                        if coordinate:\n",
        "                          txt_output_file_name.write(\"%s\\n\" % f\"|{highlight_coords_dict}|\")\n",
        "                        # prepare json object\n",
        "                        #json_data_object[f\"page-{num_page}\"] = {\"impairment_keyword_coords\": f\"{highlight_coords_dict}\"}\n",
        "                        #json_data.append(json_data_object)\n",
        "\n",
        "                    txt_output_file_name.write(\"\\n\")  # add extra line on every match code\n",
        "          \n",
        "          # highlight ICD key phrase\n",
        "          if page_num in self.impairment_keyword_dict and page_num not in already_done_page_list:\n",
        "            exact_match_list = []\n",
        "            key_phrase_sents = self.impairment_keyword_dict[page_num]\n",
        "            for key_phrase_sent in key_phrase_sents:\n",
        "              json_data_object = {}\n",
        "              coordinates = page.search_for(key_phrase_sent)\n",
        "              cords_list = []\n",
        "              keyword_cors_output = \"\"\n",
        "              for inst in coordinates:\n",
        "                if self.is_exact_match(page, key_phrase_sent, inst):\n",
        "                  highlight = page.add_highlight_annot(inst)\n",
        "                  highlight.set_colors(stroke=[0.98, 0.80, 0.69])  # apricot\n",
        "                  highlight.update()\n",
        "                  cords_list.append(highlight)\n",
        "                  num_page = page_num + 1\n",
        "                  keyword_cors_output = f\"Page-{num_page} | {key_phrase_sent}\"\n",
        "\n",
        "              if cords_list:\n",
        "                txt_output_file_name.write(\"%s\\n\" % keyword_cors_output)\n",
        "\n",
        "              #json_data_object[f\"page-{num_page}\"] = {\"impairment_keyword_coords\": {key_phrase_sent: f\"{cords_list}\"}}\n",
        "              #json_data.append(json_data_object)\n",
        "\n",
        "        # build and write json file\n",
        "        json_data_dump_file = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}.json\"\n",
        "        with open(json_data_dump_file, \"w\") as json_out_file:\n",
        "          json.dump(json_data, json_out_file)\n",
        "\n",
        "        # close log file\n",
        "        txt_output_file_name.close()\n",
        "        # create highlighted pdf file\n",
        "        pdf_output_file_name = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}_output.pdf\"\n",
        "        pdf_file.save(pdf_output_file_name, garbage=4, deflate=True, clean=True)\n",
        "\n",
        "        return pdf_output_file_name, cords_file_name\n",
        "\n",
        "    def get_best_token_match(self, page_num, p_code, page_obj, code_y2_coord, match_threshold):\n",
        "      all_match_keyword_list = []\n",
        "      all_match_imp_keyword_list = []\n",
        "\n",
        "      # Step 1: reverse code pattern\n",
        "      reversed_icd_code = reverse_code_pattern(p_code)\n",
        "      # Step 2: fetch keyword based on code\n",
        "      keyword = self.get_keyword(reversed_icd_code)\n",
        "      # Step 3: get code paragraph\n",
        "      code_paragraph_list, non_code_paragraph_list = self.get_paragraph(page_obj, code_y2_coord)\n",
        "      #print(f\"code_paragraph_list: {non_code_paragraph_list}\")\n",
        "      \n",
        "      # Step 4: prepare code_paragraph for common words\n",
        "      for code_paragraph in code_paragraph_list:\n",
        "        match_keyword_dict = {}\n",
        "        common_words = self.get_common_words(keyword, code_paragraph)\n",
        "        if len(common_words) > 0:\n",
        "          match_keyword_dict[\"common_words\"] = common_words\n",
        "          # Step 4: get best token match ratio\n",
        "          clean_paragraph = \" \".join(self.clean_text(code_paragraph))\n",
        "          match_keyword_dict[\"paragraph\"] = clean_paragraph\n",
        "          # match_score = fuzz.token_set_ratio(keyword, clean_paragraph) \n",
        "          # if fuzz.token_set_ratio(keyword, clean_paragraph) >= match_threshold else 0\n",
        "          match_keyword_dict[\"score\"] = fuzz.token_set_ratio(keyword, clean_paragraph) \n",
        "          # Step 5: build common words coordinate dict\n",
        "          common_words_coord_dict = {}\n",
        "          for common_word in common_words:\n",
        "            highlight_list = page_obj.search_for(common_word)\n",
        "            for highlight in highlight_list:\n",
        "              # get common word y2 coord value\n",
        "              common_word_y2_coords = highlight[3]\n",
        "              if (code_y2_coord - 20) <= common_word_y2_coords <= (code_y2_coord + 20):\n",
        "                  common_words_coord_dict[common_word] = highlight\n",
        "                  #self.already_found_word_list.append(common_word)\n",
        "          match_keyword_dict[\"common_words_coords\"] = common_words_coord_dict\n",
        "          all_match_keyword_list.append(match_keyword_dict)\n",
        "\n",
        "      def get_common_word_list(p_paragraph):\n",
        "        common_words = []\n",
        "        for keyword_impairment in self.impairment_keyword_dict[page_num]:\n",
        "          word_found = self.get_common_words(keyword_impairment, p_paragraph)\n",
        "          if len(word_found) > 0:\n",
        "            common_words.extend(word_found)\n",
        "        return common_words\n",
        "\n",
        "      # Step 5: prepare non code_paragraph for common words\n",
        "      for non_code_paragraph in non_code_paragraph_list:\n",
        "        #print(f\"non_code_paragraph: {non_code_paragraph}\")\n",
        "        match_imp_keyword_dict = {}\n",
        "        imp_common_words_coord_dict = {}\n",
        "        #print(f\"keyword_impairment: {keyword_impairment}\")\n",
        "        common_word_list = get_common_word_list(non_code_paragraph)\n",
        "        for common_word in common_word_list:\n",
        "          if len(common_word) > 1:\n",
        "            print(f\"common_word_1: {common_word}\")\n",
        "            highlight_list = page_obj.search_for(common_word)\n",
        "            #print(f\"highlight_list: {highlight_list}\")\n",
        "            for highlight in highlight_list:\n",
        "              # get common word y2 coord value\n",
        "              common_word_y2_coords = highlight[3]\n",
        "              if common_word_y2_coords not in self.code_y2_coords_list and common_word.lower() not in self.already_found_word_list:\n",
        "                #if self.is_exact_match(page_obj, keyword_impairment, highlight):\n",
        "                self.already_found_word_list.append(common_word.lower())\n",
        "                imp_common_words_coord_dict[common_word] = highlight\n",
        "                print(f\"common_word: {common_word}, coords: {highlight}\")\n",
        "        match_imp_keyword_dict[\"imp_common_keyword_coords\"] = imp_common_words_coord_dict\n",
        "        print(f\"match_imp_keyword_dict: {match_imp_keyword_dict['imp_common_keyword_coords']}\")\n",
        "        all_match_imp_keyword_list.append(match_imp_keyword_dict)\n",
        "\n",
        "      return all_match_keyword_list, all_match_imp_keyword_list\n",
        "\n",
        "    def search_icd_code(self, txt_list):\n",
        "      pdf_page_vocab = {}\n",
        "      for txt_file in txt_list:\n",
        "          with open(txt_file, \"r\") as f:\n",
        "              page_txt = f.read()\n",
        "\n",
        "              # check the page that have line number instead of code\n",
        "              index_page = False\n",
        "              if re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "                  index_page = True\n",
        "\n",
        "              doc = self.nlp_code10(page_txt)\n",
        "              code_list = []\n",
        "              for ent in doc.ents:\n",
        "                  if index_page:\n",
        "                      # check the code contain letter \"L\"\n",
        "                      if re.search(\"(L[0-9]+)\", ent.text):\n",
        "                          continue\n",
        "                      else:\n",
        "                          code_list.append(ent.text)\n",
        "                  else:\n",
        "                      code_list.append(ent.text)\n",
        "\n",
        "              # code_list = [ent.text for ent in doc.ents if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", ent.text)]\n",
        "              if len(code_list) != 0:\n",
        "                  page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "                  pdf_page_vocab[page_number] = list(set(code_list))\n",
        "                  # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "      return pdf_page_vocab\n",
        "\n",
        "    def get_keyword(self, p_code):\n",
        "        keyword = \"\"\n",
        "        # reverse code if required\n",
        "        code = reverse_code_pattern(p_code)\n",
        "        # get keyword from dataset\n",
        "        keyword_list = list(self.code_df.loc[self.code_df[\"Code\"] == code][\"Keyword\"])\n",
        "        if len(keyword_list) > 0:\n",
        "            keyword = keyword_list[0]\n",
        "        return keyword\n",
        "\n",
        "    def get_paragraph(self, page_obj, code_y2_coord):\n",
        "        code_paragraph_list = []\n",
        "        non_code_paragraph_list = []\n",
        "        page_content = page_obj.get_text(\"blocks\", sort=False)\n",
        "        for content in page_content:\n",
        "          #print(f\"content found: {content[4]}\")\n",
        "          if content[1] <= code_y2_coord <= content[3]:\n",
        "              if len(self.clean_text(content[4])) > 0:\n",
        "                  code_paragraph = content[4]\n",
        "                  code_paragraph_list.append(code_paragraph)\n",
        "          else:\n",
        "            if len(self.clean_text(content[4])) > 0:\n",
        "              non_code_paragraph_list.append(content[4])\n",
        "        return code_paragraph_list, non_code_paragraph_list\n",
        "\n",
        "    def get_common_words(self, sent1, sent2):\n",
        "        clean_token1 = self.clean_text(sent1)\n",
        "        clean_token2 = self.clean_text(sent2)\n",
        "        token_set1 = set(clean_token1)\n",
        "        token_set2 = set(clean_token2)\n",
        "\n",
        "        common_word_set = set()\n",
        "\n",
        "        def get_common(token_set1, token_set2):\n",
        "            for w1 in token_set1:\n",
        "                for w2 in token_set2:\n",
        "                    if w1.lower() == w2.lower():\n",
        "                        common_word_set.add(w1)\n",
        "\n",
        "        get_common(token_set1, token_set2)\n",
        "        get_common(token_set2, token_set1)\n",
        "        return list(common_word_set)\n",
        "\n",
        "    def clean_text(self, sent):\n",
        "        # tokenize sentence\n",
        "        sent1 = word_tokenize(sent)\n",
        "        # filter stop words\n",
        "        filtered_sent = [w for w in sent1 if not w.lower() in self.stop_words]\n",
        "        filtered_sent = [w for w in filtered_sent if re.sub(re.compile('\\W'), '', w)]\n",
        "        clean_tokens = []\n",
        "        for token in filtered_sent:\n",
        "            if token.find(\"-\"):\n",
        "                tokens = token.split(\"-\")\n",
        "                clean_tokens.extend(tokens)\n",
        "            else:\n",
        "                clean_tokens.append(token)\n",
        "        return clean_tokens\n",
        "\n",
        "    def is_exact_match(self, page, term, clip):\n",
        "      # clip is an item from page.search_for(term, quads=True)\n",
        "      termLen = len(term)\n",
        "      termBboxLen = max(clip.height, clip.width)\n",
        "      termfontSize = termBboxLen/termLen\n",
        "      f = termfontSize*2\n",
        "      #clip = clip.rect\n",
        "      text_block = page.get_text(\"blocks\", clip = clip + (-f, -f, f, f), flags=0)[0][4]\n",
        "      #re.sub(r\"[^a-zA-Z\\d\\s:]\", \"\", \"(HIV]\")\n",
        "      #if re.sub(r\"[^a-zA-Z\\d\\s:]\", \"\", text_block.lower().strip()) in [t.lower() for t in term.split()]:\n",
        "      if re.sub(r\"[^a-zA-Z\\d\\s:]\", \"\", text_block.strip()) in [t for t in term.split()]:\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "    def get_alternate_code_pattern(self, p_code):\n",
        "      # create alternate pattern\n",
        "      code_patterns = []\n",
        "      code_arr = p_code.split(\".\")\n",
        "      if len(code_arr) > 1:\n",
        "        code1 = f\"{code_arr[0]}. {code_arr[1]}\"\n",
        "        code2 = f\"{code_arr[0]} .{code_arr[1]}\"\n",
        "        code3 = f\"{code_arr[0]} . {code_arr[1]}\"\n",
        "        code4 = f\"{code_arr[0]} {code_arr[1]}\"\n",
        "        code44 = f\"{code_arr[0]},{code_arr[1]}\"\n",
        "        code45 = f\"{code_arr[0]}, {code_arr[1]}\"\n",
        "        code46 = f\"{code_arr[0]} ,{code_arr[1]}\"\n",
        "        code47 = f\"{code_arr[0]} , {code_arr[1]}\"\n",
        "        code_patterns.extend([code1, code2, code3, code4, code44, code45, code46, code47])\n",
        "        # handle if the first char of code is missing\n",
        "        alphabats = {\"Z\": \"2\", \"B\": \"8\", \"O\": \"0\", \"S\": \"5\", \"l\": \"1\", \"G\": \"6\", \"o\": \"9\", \"i\": \"1\"}\n",
        "        for key, val in alphabats.items():\n",
        "          if p_code.startswith(key):\n",
        "            code5 = p_code.replace(key, val)\n",
        "            code_patterns.extend([code5])\n",
        "          # replcae char on 1 index if it is not present in icd9 code dataset\n",
        "          if p_code.find(val) == 1:\n",
        "            code6 = replacer(p_code, key, 1)\n",
        "            code_patterns.extend([code6])\n",
        "            # replcae char on 2 index\n",
        "            if p_code.find(val) == 2:\n",
        "              code7 = replacer(code6, key, 2)\n",
        "              code_patterns.extend([code7])\n",
        "      return code_patterns\n",
        "\n",
        "def find_nearest(array, value):\n",
        "    array = np.asarray(array)\n",
        "    idx = (np.abs(array - value)).argmin()\n",
        "    return array[idx]\n",
        "\n",
        "\n",
        "def reverse_code_pattern(p_code):\n",
        "    orig_code = p_code\n",
        "\n",
        "    # check for code contains space(\" \")\n",
        "    tmp_code = orig_code.split(\" \")\n",
        "    if len(tmp_code) > 1:\n",
        "        orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "\n",
        "    # check for code contains dot(\".\")\n",
        "    tmp_code = p_code.split(\".\")\n",
        "    if len(tmp_code) > 1:\n",
        "        orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "\n",
        "    # check for code contains comma(\",\")\n",
        "    tmp_code = p_code.split(\",\")\n",
        "    if len(tmp_code) == 2:\n",
        "        orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "    elif len(tmp_code) == 2:\n",
        "        orig_code = f\"{tmp_code[0].strip()}.{tmp_code[2].strip()}\"\n",
        "\n",
        "    # handle if the first char of code is missing\n",
        "    alphabats = {\"Z\": \"2\", \"B\": \"8\", \"O\": \"0\", \"S\": \"5\", \"l\": \"1\", \"G\": \"6\", \"o\": \"9\", \"i\": \"1\"}\n",
        "    for key, val in alphabats.items():\n",
        "        # replcae char on 0 index\n",
        "        if orig_code.find(val) == 0:\n",
        "            # orig_code = orig_code.replace(val, key)\n",
        "            orig_code = replacer(orig_code, key, 0)\n",
        "        # replcae char on 1 index\n",
        "        if orig_code.find(key) == 1:\n",
        "            orig_code = replacer(orig_code, val, 1)\n",
        "            # replcae char on 2 index\n",
        "            if orig_code.find(key) == 2:\n",
        "                orig_code = replacer(orig_code, val, 2)\n",
        "            break\n",
        "\n",
        "    return orig_code\n",
        "\n",
        "\n",
        "def replacer(s, newstring, index, nofail=False):\n",
        "    # raise an error if index is outside of the string\n",
        "    if not nofail and index not in range(len(s)):\n",
        "        raise ValueError(\"index outside given string\")\n",
        "\n",
        "    # if not erroring, but the index is still not in the correct range..\n",
        "    if index < 0:  # add it to the beginning\n",
        "        return newstring + s\n",
        "    if index > len(s):  # add it to the end\n",
        "        return s + newstring\n",
        "\n",
        "    # insert the new string between \"slices\" of the original\n",
        "    return s[:index] + newstring + s[index + 1:]\n",
        "\n",
        "\n",
        "def create_directory(dir_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)"
      ],
      "metadata": {
        "id": "jRYlndgDUtds"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentence Extractor"
      ],
      "metadata": {
        "id": "W5REPlzYjQ96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceExtractor:\n",
        "  def __init__(self):\n",
        "      self.MAX_WORKERS = 20\n",
        "\n",
        "  def get_json_array_list(self, text_path):\n",
        "      json_arr = None\n",
        "      try:\n",
        "          # print(f\"Running '{text_path}'\")\n",
        "          json_arr = call(text_path)\n",
        "          # print(f\"Got json for '{json_arr}'\")\n",
        "      except Exception as err:\n",
        "          print(f\"Error for file[{text_path}] is:\\n{err}\")\n",
        "      return json_arr\n",
        "\n",
        "  def get_wrong_keyword_dict(self, text_files_list, with_thread=False, with_process=False):\n",
        "      def get_sorted_dict(p_json_arr_list):\n",
        "          wrong_keyword_dict = {\n",
        "              idx: set([list(element.values())[0] for element in json_arr if json_arr])\n",
        "              for idx, json_arr in enumerate(p_json_arr_list)\n",
        "          }\n",
        "          return dict(sorted(wrong_keyword_dict.items(), key=lambda item: item[0]))\n",
        "\n",
        "      if with_thread:\n",
        "          # take care so that unnecessary thread should not be created\n",
        "          workers = min(self.MAX_WORKERS, len(text_files_list))\n",
        "          with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
        "              json_arr_list = executor.map(self.get_json_array_list, text_files_list)\n",
        "          return get_sorted_dict(json_arr_list)\n",
        "      if with_process:\n",
        "          with futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
        "              json_arr_list = executor.map(self.get_json_array_list, text_files_list)\n",
        "          return get_sorted_dict(json_arr_list)\n",
        "      else:\n",
        "          json_arr_list = list(map(self.get_json_array_list, text_files_list))\n",
        "          tmp_wrong_keyword_dict = {\n",
        "              idx: set([list(element.values())[0] for element in json_arr if json_arr is not None])\n",
        "              for idx, json_arr in enumerate(json_arr_list)\n",
        "          }\n",
        "          return tmp_wrong_keyword_dict\n",
        "\n",
        "  def extract_sentence(self, wrong_keyword_list, sample_text_list):\n",
        "      match_keyword_dict = {}\n",
        "      for key, keyword_set in wrong_keyword_list.items():\n",
        "          match_dicts = {}\n",
        "          for key_phrase in keyword_set:\n",
        "              # print(key, key_phrase)\n",
        "              with open(sample_text_list[key], \"r\") as f:\n",
        "                  file_txt = f.read()\n",
        "              # match_list = re.findall(f\"([^\\n]*?(?i){key_phrase}[^.]*\\.)\", file_txt)\n",
        "              match_list = re.findall(f\"([^\\n]*{key_phrase}[^\\n]*\\n)\", file_txt)\n",
        "              if match_list:\n",
        "                  match_dicts[key_phrase] = [match.replace(\"\\n\", \"\") for match in match_list]\n",
        "          match_keyword_dict[key] = match_dicts\n",
        "      return match_keyword_dict"
      ],
      "metadata": {
        "id": "7idpKdOBUR5j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keyword Matching & Highlighting "
      ],
      "metadata": {
        "id": "7s7ZxYMUBAol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf input_files\n",
        "!mkdir -p input_files\n",
        "!rm -rf output\n",
        "!mkdir -p output"
      ],
      "metadata": {
        "id": "wfJwndx9vcTP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def purge(file_path):\n",
        "  for f in glob.glob(file_path):\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "32V_ohLAgvLg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-0: create highlighter instance\n",
        "INPUT_PDF_FILES_PATH = \"input_files\"\n",
        "\n",
        "highlighter = Highlighter()\n",
        "sent_extractor = SentenceExtractor()"
      ],
      "metadata": {
        "id": "leBmw505CQUl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_extractor = SentenceExtractor()"
      ],
      "metadata": {
        "id": "dzrXJcM2zwWP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "wrong_keyword_dict1 = None\n",
        "icd10_code_dict1 = None\n",
        "for pdf_file in os.listdir(INPUT_PDF_FILES_PATH):\n",
        "  pdf_file_name = f\"{INPUT_PDF_FILES_PATH}/{pdf_file}\"\n",
        "  cords_file_name = f\"{pdf_file_name.split('/')[1].split('.')[0]}_cords.txt\"\n",
        "\n",
        "  # Step-1: splitting pdf file\n",
        "  pdf_list = highlighter.split_pdf(pdf_file_name)\n",
        "\n",
        "  # Step-2: Extracting text from pdf\n",
        "  txt_list = highlighter.extract_text_from_pdf(pdf_list)\n",
        "\n",
        "  # Step-3: Searching ICD-10 code\n",
        "  icd10_code_dict = highlighter.search_icd_code(txt_list)\n",
        "  icd10_code_dict1 = icd10_code_dict\n",
        "\n",
        "  # Step-4: Get closet match of ICD-10 keyword\n",
        "  wrong_keyword_dict = sent_extractor.get_wrong_keyword_dict(txt_list)\n",
        "  wrong_keyword_dict1 = wrong_keyword_dict\n",
        "  highlighter.set_impairment_keyword_dict(wrong_keyword_dict)\n",
        "\n",
        "  # Step-4: Highlighting ICD-10 code into pdf\n",
        "  pdf_output_file, txt_output_file = highlighter.highlight_icd_code(icd10_code_dict,\n",
        "                                                                    match_threshold=35,\n",
        "                                                                    coordinate=True,\n",
        "                                                                    pdf_file_name=pdf_file_name,\n",
        "                                                                    cords_file_name=cords_file_name)\n",
        "  print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "  print(f\"Highlighted coordinates are saved into [{txt_output_file}] file.\")\n",
        "\n",
        "  # remove all pdf and text files\n",
        "  purge(\"pdf-files/*.pdf\")\n",
        "  purge(\"txt-files/*.txt\")\n",
        "  pdf_list = []\n",
        "  txt_list = []"
      ],
      "metadata": {
        "id": "hiOT-aTfedMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "purge(\"pdf-files/*.pdf\")\n",
        "purge(\"txt-files/*.txt\")"
      ],
      "metadata": {
        "id": "JlOdZaO9IL5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip output.zip output/*.*"
      ],
      "metadata": {
        "id": "MeS8LHhjjfzU",
        "outputId": "98d7ddaf-85f0-420e-8fa4-257dd42c1c88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: output/APS_38600000R_final_cords.txt (deflated 80%)\n",
            "  adding: output/APS_38600000R_final.json (deflated 85%)\n",
            "  adding: output/APS_38600000R_final_output.pdf (deflated 2%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "icd10_code_dict1[28]"
      ],
      "metadata": {
        "id": "g6DPMRvWKz0A",
        "outputId": "55fe0fa5-502d-4637-a873-e2fd85368297",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['R03.0', '278.9', 'E78.3', '268.25', 'R53.83']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_keyword_dict1[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVboLP-X4REu",
        "outputId": "bc843660-bd8b-4d7e-e1d0-f0fb421bd0d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AH',\n",
              " 'AIDS',\n",
              " 'ALL',\n",
              " 'HHS',\n",
              " 'HIV',\n",
              " 'Human Immunodeficiency Virus',\n",
              " 'Infectious Disease',\n",
              " 'TEN'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_keyword_dict1[0]"
      ],
      "metadata": {
        "id": "oy3VrsX1vpuQ",
        "outputId": "f49d8fd6-29d3-4591-8314-349fa86a2626",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Atrial Fibrillation',\n",
              " 'Headache',\n",
              " 'Hypertension',\n",
              " 'Hypertriglyceridemia',\n",
              " 'MD',\n",
              " 'MI',\n",
              " 'WAS'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_code_paragraph = \"\"\"\n",
        "Family History (Ranya Aziz; 12/27/2021               11:04 AM)\n",
        "Father      Instable health, Hypertension, Atrial Fibrillation.\n",
        "Brother 1 |n good health.\n",
        "Sister 1 In good health.\n",
        "Mother       Instable health.\n",
        "\"\"\"\n",
        "common_words = []\n",
        "for keyword_impairment in wrong_keyword_dict1[0]:\n",
        "  word_found = highlighter.get_common_words(keyword_impairment, non_code_paragraph)\n",
        "  if len(word_found) > 0:\n",
        "    common_words.extend(word_found)\n",
        "common_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt5S5dtjnFkh",
        "outputId": "a2135324-ba2f-4f0c-cdc6-ca905098ed57"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hypertension', 'Atrial', 'Fibrillation']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ICD 10 Code"
      ],
      "metadata": {
        "id": "RIRH21XK258d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p pdf-files\n",
        "!mkdir -p txt-files"
      ],
      "metadata": {
        "id": "Ebs7E8X23lri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define directory path after creating it\n",
        "pdf_files_path = \"pdf-files\"\n",
        "txt_files_path = \"txt-files\"\n",
        "\n",
        "# create nlp instance\n",
        "nlp = English()\n",
        "\n",
        "\n",
        "def split_pdf(pdf_path):\n",
        "  pdf_in_file = open(pdf_path, \"rb\")\n",
        "  pdf = PdfReader(pdf_in_file)\n",
        "  pdf_list = []\n",
        "  for page in range(len(pdf.pages)):\n",
        "      inputpdf = PdfReader(pdf_in_file)\n",
        "      output = PdfWriter()\n",
        "      output.add_page(inputpdf.pages[page])\n",
        "      with open(f\"{pdf_files_path}/page-{page}.pdf\", \"wb\") as outputStream:\n",
        "          output.write(outputStream)\n",
        "          pdf_list.append(f\"page-{page}.pdf\")\n",
        "  return pdf_list\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_list):\n",
        "    txt_file_list = []\n",
        "    i = 0\n",
        "    for pdf_file in pdf_list:\n",
        "        with open(os.path.join(pdf_files_path, pdf_file), \"rb\") as f:\n",
        "            pdf = pdftotext.PDF(f)\n",
        "\n",
        "        # Read all the text into one string\n",
        "        pdf_text = \"\\n\\n\".join(pdf)\n",
        "\n",
        "        # write text into file\n",
        "        with open(f\"{txt_files_path}/page-{str(i)}.txt\", \"a\") as f:\n",
        "            f.write(pdf_text)\n",
        "        txt_file_list.append(f\"{txt_files_path}/page-{str(i)}.txt\")\n",
        "        i += 1\n",
        "    return txt_file_list\n",
        "\n",
        "\n",
        "def get_opt_pattern(icd_10_code):\n",
        "  # create alternate pattern\n",
        "  code_arr = icd_10_code.split(\".\")\n",
        "  if len(code_arr) > 1:\n",
        "    code1 = f\"{code_arr[0]}. {code_arr[1]}\"\n",
        "    code2 = f\"{code_arr[0]} .{code_arr[1]}\"\n",
        "    code3 = f\"{code_arr[0]} . {code_arr[1]}\"\n",
        "    return [code1, code2, code3]\n",
        "  else:\n",
        "    return icd_10_code\n",
        "\n",
        "\n",
        "def highlight_icd10_code(pdf_page_dict: dict, pdf_file_name: str):\n",
        "    pdf_file = fitz.open(pdf_file_name)\n",
        "\n",
        "    def highlight_pdf(highlight):\n",
        "        for inst in highlight:\n",
        "          highlight = page.add_highlight_annot(inst)\n",
        "          highlight.update()\n",
        "          highlight = page.search_for(text_to_be_highlighted)\n",
        "          print(f\"Page-{page_num}: \", code, highlight, end='\\n')\n",
        "\n",
        "    for page_num, page in enumerate(pdf_file):\n",
        "        if page_num in pdf_page_dict:\n",
        "          for code in pdf_page_dict[page_num]:\n",
        "            text_to_be_highlighted = code\n",
        "            highlight = page.search_for(text_to_be_highlighted)\n",
        "            print(f\"Page-{page_num}: \", code, highlight, end='\\n')\n",
        "            if len(highlight) == 0:\n",
        "                alternate_code_list = get_opt_pattern(code)\n",
        "                for alt_code in alternate_code_list:\n",
        "                  text_to_be_highlighted = alt_code\n",
        "                  highlight = page.search_for(text_to_be_highlighted)\n",
        "                  # highlight pdf for option pattern\n",
        "                  highlight_pdf(highlight)\n",
        "            # highlight pdf for main pattern\n",
        "            highlight_pdf(highlight)\n",
        "\n",
        "    output_pdf_file_name = f\"{pdf_file_name.split('.')[0]}_output.pdf\"\n",
        "    pdf_file.save(output_pdf_file_name, garbage=4, deflate=True, clean=True)\n",
        "    return output_pdf_file_name\n",
        "\n",
        "\n",
        "def search_icd_10_code(txt_list):\n",
        "  pdf_page_vocab = {}\n",
        "  for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "      page_txt = f.read()\n",
        "      # filter the page that have line number instead of code\n",
        "      if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "        doc = nlp(page_txt)\n",
        "        code_list = [ent.text for ent in doc.ents]\n",
        "        if len(code_list) != 0:\n",
        "          #print(txt_file)\n",
        "          page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "          pdf_page_vocab[page_number] = code_list\n",
        "          # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "  return pdf_page_vocab"
      ],
      "metadata": {
        "id": "snzCZDEN29Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-1: splitting pdf file\n",
        "pdf_file_name = \"28page.pdf\"\n",
        "pdf_list = split_pdf(pdf_file_name)\n",
        "\n",
        "# Step-2: Extracting text from pdf\n",
        "txt_list = extract_text_from_pdf(pdf_list)\n",
        "\n",
        "# Step-3: loading and updating patterns to Spacy\n",
        "nlp.add_pipe(\"entity_ruler\").from_disk(\"./icd10_code_patterns-v1.jsonl\")\n",
        "\n",
        "# Step-4: Searching ICD-10 code\n",
        "#print (txt_list)\n",
        "pdf_page_vocab = search_icd_10_code(txt_list)\n",
        "\n",
        "# Step-5: Highlighting ICD-10 code into pdf\n",
        "output_file_name = highlight_icd10_code(pdf_page_vocab, pdf_file_name)\n",
        "print(f\"File[{output_file_name}] is saved after highlighting ICD-10 code\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPlZD1rj3D0Z",
        "outputId": "e3130f4e-4908-40fa-c3df-f02f29d7cd92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "File[28page_output.pdf] is saved after highlighting ICD-10 code\n"
          ]
        }
      ]
    }
  ]
}