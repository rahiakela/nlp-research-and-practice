{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0hK4nU35JVaA",
        "N4dDTs-rJL_h",
        "TycHgKTZJ6a6"
      ],
      "authorship_tag": "ABX9TyM4nuJrnXhOC32Xysa+Rsxp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/nlp-research-and-practice/blob/main/text-similarity-works/19_icd_10_code_common_keyword_and_keyword_impairment_highlighting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "u6vazPC0Ja4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "!pip install pillow\n",
        "\n",
        "!sudo apt install build-essential libpoppler-cpp-dev pkg-config python3-dev\n",
        "!pip install -U pdftotext\n",
        "!pip install PyPDF2\n",
        "!pip install fitz\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "gdwgVvXuJdz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "id": "4QPcnYT5AKpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "YyREdYtyqNMK",
        "outputId": "fbd2d792-781d-49fc-a69e-e02ffc53d9e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/csv_files/\n",
            "  inflating: data/csv_files/icd_10_code_and_keywords_v2.csv  \n",
            "  inflating: data/csv_files/icd_10_code_and_keywords_v3.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment.csv  \n",
            "   creating: data/pattern_files/\n",
            "  inflating: data/pattern_files/icd10_code_patterns-v6.jsonl  \n",
            "   creating: data/pkl_files/\n",
            "  inflating: data/pkl_files/dict_words.pkl  \n",
            "  inflating: data/pkl_files/knocaps_dict_index.pickle  \n",
            "   creating: data/txt_files/\n",
            "  inflating: data/txt_files/kcaps.txt  \n",
            "  inflating: data/txt_files/knocaps.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "from functools import lru_cache\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import fitz\n",
        "import pdftotext\n",
        "from PyPDF2 import PdfFileReader, PdfReader, PdfFileWriter, PdfWriter\n",
        "\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from array import *\n",
        "\n",
        "from concurrent import futures\n",
        "from keyword_matcher import KeywordMatcher\n",
        "import config as cfg"
      ],
      "metadata": {
        "id": "gKbd7WsfyYX3",
        "outputId": "037d5df2-25e4-430a-be50-e2f1e25c0948",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Imv6fB5gPBe2",
        "outputId": "ffebd144-0142-464a-eabe-25b091b25b86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Core Classes"
      ],
      "metadata": {
        "id": "9oNqjgIsRpNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Highlighter:\n",
        "    def __init__(self, match_threshold=30):\n",
        "        # loading and updating patterns for ICD-10 code\n",
        "        self.nlp_code10 = English()\n",
        "        self.nlp_code10.add_pipe(\"entity_ruler\").from_disk(cfg.pattern_files[\"jsonl\"])\n",
        "\n",
        "        # loading stop words\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.match_threshold = match_threshold\n",
        "\n",
        "        # define icd-10 code dataset\n",
        "        self.code_df = pd.read_csv(cfg.csv_files[\"CODE_CSV\"])\n",
        "        self.synid_df = pd.read_csv(cfg.csv_files[\"IMP_CSV\"])\n",
        "\n",
        "        # define required directory path\n",
        "        self.PDF_FILES_PATH = \"pdf-files\"\n",
        "        self.TXT_FILES_PATH = \"txt-files\"\n",
        "        self.OUTPUT_FILES_PATH = cfg.output_path[\"OUT\"]\n",
        "        create_directory(self.PDF_FILES_PATH)\n",
        "        create_directory(self.TXT_FILES_PATH)\n",
        "        create_directory(self.OUTPUT_FILES_PATH)\n",
        "\n",
        "    def set_impairment_keyword_dict(self, imp_keyword_dict):\n",
        "        self.impairment_keyword_dict = imp_keyword_dict\n",
        "        \n",
        "    def set_match_threshold(self, match_val):\n",
        "        self.match_threshold = match_val\n",
        "\n",
        "    def split_pdf(self, pdf_path):\n",
        "        pdf_in_file = open(pdf_path, \"rb\")\n",
        "        pdf = PdfReader(pdf_in_file)\n",
        "        pdf_list = []\n",
        "        for page in range(len(pdf.pages)):\n",
        "            input_pdf = PdfReader(pdf_in_file)\n",
        "            output = PdfWriter()\n",
        "            output.add_page(input_pdf.pages[page])\n",
        "            with open(f\"{self.PDF_FILES_PATH}/page-{page}.pdf\", \"wb\") as outputStream:\n",
        "                output.write(outputStream)\n",
        "                pdf_list.append(f\"page-{page}.pdf\")\n",
        "        return pdf_list\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_list):\n",
        "        txt_file_list = []\n",
        "        i = 0\n",
        "        for pdf_file in pdf_list:\n",
        "            with open(os.path.join(self.PDF_FILES_PATH, pdf_file), \"rb\") as f:\n",
        "                pdf = pdftotext.PDF(f)\n",
        "\n",
        "            # Read all the text into one string\n",
        "            pdf_text = \"\\n\\n\".join(pdf)\n",
        "\n",
        "            # write text into file\n",
        "            with open(f\"{self.TXT_FILES_PATH}/page-{str(i)}.txt\", \"a\") as f:\n",
        "                f.write(pdf_text)\n",
        "            txt_file_list.append(f\"{self.TXT_FILES_PATH}/page-{str(i)}.txt\")\n",
        "            i += 1\n",
        "        self.text_list = txt_file_list\n",
        "        return txt_file_list\n",
        "\n",
        "    def highlight_all(self, icd10_code_dict, pdf_file_name=None, highlight_all=False):\n",
        "        pdf_file = fitz.open(pdf_file_name)\n",
        "        json_data = []\n",
        "        json_imp_data = []\n",
        "            \n",
        "        for page_num, page in enumerate(pdf_file):\n",
        "            already_done_page_list = []\n",
        "            self.code_y2_coords_list = []\n",
        "            self.already_found_word_list = []\n",
        "            self.highlighted_coord_list = []\n",
        "            code_highlight_dict = {}\n",
        "\n",
        "            # highlight ICD-10 code\n",
        "            if page_num in icd10_code_dict:\n",
        "                num_page = page_num + 1\n",
        "                \n",
        "                # let's store code y2 coords\n",
        "                for code in icd10_code_dict[page_num]:\n",
        "                    highlight_list = page.search_for(code)\n",
        "                    #print(f\"Page-{num_page}-{code}, Coordinate: {highlight_list}\")\n",
        "                    if len(highlight_list) > 0:\n",
        "                        code_highlight_dict[code] = highlight_list\n",
        "                        self.code_y2_coords_list.extend([highlight[3] for highlight in highlight_list])\n",
        "                    else:\n",
        "                        for alt_code in self.get_alternate_code_pattern(code):\n",
        "                            highlight_list = page.search_for(alt_code)\n",
        "                            for highlight in highlight_list:\n",
        "                                if len(highlight) > 0:\n",
        "                                    code_highlight_dict[code] = [highlight]\n",
        "                                    # print(f\"alternate-code:{code}, Coordinate: {highlight}\")\n",
        "                                    self.code_y2_coords_list.extend([highlight[3]])\n",
        "                                    # print(f\"[highlight[3]]: {[highlight[3]]}\")\n",
        "\n",
        "                filtered_code_highlight_dict = filter_duplicate_coordinate(code_highlight_dict)\n",
        "                # now, let highlight every code and its common words\n",
        "                for code, highlight_list in filtered_code_highlight_dict.items():\n",
        "                    prev_highlight = fitz.Rect(0, 0, 0, 0)\n",
        "                    for highlight in highlight_list:\n",
        "                        json_data_object = {}\n",
        "                        keyword = self.get_keyword(code)\n",
        "                        # get match score and common words coordinate\n",
        "                        if not highlight_all:\n",
        "                          all_match_keyword_list, all_match_imp_keyword_list = self.get_best_token_match(page_num, \n",
        "                                                                                                        code,\n",
        "                                                                                                        page,\n",
        "                                                                                                        highlight[3],\n",
        "                                                                                                        self.match_threshold)\n",
        "                        else:\n",
        "                          all_match_keyword_list, all_match_imp_keyword_list = self.get_best_all_token_match(page_num, \n",
        "                                                                                                        code,\n",
        "                                                                                                        page,\n",
        "                                                                                                        highlight[3],\n",
        "                                                                                                        self.match_threshold)\n",
        "\n",
        "                        # Step-1: highlight and set color coding dont have common words\n",
        "                        if not all_match_keyword_list:\n",
        "                            page_highlight = page.add_highlight_annot(highlight)\n",
        "                            page_highlight.set_colors(stroke=[0.92, 0.59, 0.48])  # dark salmon\n",
        "                            page_highlight.update()\n",
        "                            # prepare json object\n",
        "                            json_data.append(self.prepare_json_object(num_page, code, highlight, keyword))\n",
        "\n",
        "                        # Step-2: highlight and set color coding that have common words\n",
        "                        for match_keyword_dict in all_match_keyword_list:\n",
        "                            # highlight ICD-10 code\n",
        "                            if not self.is_coord_equal(prev_highlight, highlight):\n",
        "                              self.highlight_code(page, highlight, match_keyword_dict[\"score\"])\n",
        "                              prev_highlight = highlight\n",
        "                            # highlight common words\n",
        "                            highlight_coords_dict = {}\n",
        "                            highlight_coords_dict, highlighted_word_list = self.highlight_common_words(page, match_keyword_dict[\"common_words_coords\"])\n",
        "                            # prepare json object\n",
        "                            json_data.append(self.prepare_json_object(num_page, code, highlight, keyword, match_keyword_dict, highlight_coords_dict))\n",
        "\n",
        "                        # Step-3: highlight and set color coding for impairment keyword common words\n",
        "                        json_imp_data_list = []\n",
        "                        for match_imp_keyword_dict in all_match_imp_keyword_list:\n",
        "                            json_imp_data_object = {}\n",
        "                            # highlight common words\n",
        "                            highlight_coords_dict = {}\n",
        "                            if \"imp_common_keyword_coords\" in match_imp_keyword_dict:\n",
        "                                #print(f\"match_imp_keyword_dict: {match_imp_keyword_dict}\")\n",
        "                                highlight_coords_list = self.highlight_impairment(page, match_imp_keyword_dict[\"imp_common_keyword_coords\"])\n",
        "                                already_done_page_list.append(page_num)\n",
        "                                # prepare json object\n",
        "                                # json_imp_data_object[f\"page-{num_page}\"] = {\"impairment_keyword_coords\": highlight_coords_dict}\n",
        "                                if highlight_coords_list:\n",
        "                                    json_imp_data_list.extend(highlight_coords_list)\n",
        "                        if len(json_imp_data_list) > 0:\n",
        "                            json_imp_data.append({\n",
        "                                f\"page-{num_page}\": json_imp_data_list\n",
        "                            })\n",
        "\n",
        "            # highlight ICD key phrase\n",
        "            if page_num in self.impairment_keyword_dict and page_num not in already_done_page_list:\n",
        "                json_imp_data_list = []\n",
        "                num_page = page_num + 1\n",
        "                key_phrase_sents = self.impairment_keyword_dict[page_num]\n",
        "                for key_phrase_sent in key_phrase_sents:\n",
        "                  coordinates = page.search_for(key_phrase_sent)\n",
        "                  cords_list = []\n",
        "                  for inst in coordinates:\n",
        "                    key_phrase_list = key_phrase_sent.split()\n",
        "                    for key_phrase in key_phrase_list:\n",
        "                      if self.is_exact_match(page, key_phrase, inst, full_match=True, case_sensitive=True) and not self.is_coord_already_highlighted(inst):\n",
        "                        self.draw_keyword_box(page, inst, key_phrase)\n",
        "                        cords_list.append(inst)\n",
        "                        self.highlighted_coord_list.append(inst)\n",
        "                  if len(cords_list) > 0:\n",
        "                      json_imp_data_list.append(self.prepare_imp_json_object(key_phrase_sent, cords_list))\n",
        "                if len(json_imp_data_list) > 0:\n",
        "                    json_imp_data.append({\n",
        "                        f\"page-{num_page}\": json_imp_data_list\n",
        "                    })\n",
        "\n",
        "        # combine both json object and write into json file\n",
        "        json_data_dump_file = None\n",
        "        if highlight_all:\n",
        "            json_data_dump_file = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}_{cfg.json_output['OUT_4']}\"\n",
        "        else:\n",
        "            json_data_dump_file = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}_{cfg.json_output['OUT_1']}\"\n",
        "            \n",
        "        with open(json_data_dump_file, \"w\") as json_out_file:\n",
        "            json.dump({\n",
        "              \"icd10_code_and_description\": json_data,\n",
        "              \"keyword_impairment\": json_imp_data\n",
        "            }, json_out_file)\n",
        "\n",
        "        # create highlighted pdf file\n",
        "        pdf_output_file_name = None\n",
        "        if highlight_all:\n",
        "            pdf_output_file_name = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}_{cfg.pdf_output['OUT_4']}\"\n",
        "        else:\n",
        "            pdf_output_file_name = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}_{cfg.pdf_output['OUT_1']}\"\n",
        "            \n",
        "        pdf_file.save(pdf_output_file_name, garbage=4, deflate=True, clean=True)\n",
        "\n",
        "        return pdf_output_file_name, json_data_dump_file\n",
        "\n",
        "    def highlight_icd_code_and_common_words(self, icd10_code_dict, pdf_file_name=None):\n",
        "        pdf_file = fitz.open(pdf_file_name)\n",
        "        json_data = []\n",
        "\n",
        "        for page_num, page in enumerate(pdf_file):\n",
        "            self.code_y2_coords_list = []\n",
        "            self.already_found_word_list = []\n",
        "            code_highlight_dict = {}\n",
        "\n",
        "            # highlight ICD-10 code\n",
        "            if page_num in icd10_code_dict:\n",
        "                num_page = page_num + 1\n",
        "                # let's store code y2 coords\n",
        "                for code in icd10_code_dict[page_num]:\n",
        "                    highlight_list = page.search_for(code)\n",
        "                    #print(f\"Page-{num_page}-{code}, Coordinate: {highlight_list}\")\n",
        "                    if len(highlight_list) > 0:\n",
        "                        code_highlight_dict[code] = highlight_list\n",
        "                        self.code_y2_coords_list.extend([highlight[3] for highlight in highlight_list])\n",
        "                    else:\n",
        "                        for alt_code in self.get_alternate_code_pattern(code):\n",
        "                            highlight_list = page.search_for(alt_code)\n",
        "                            for highlight in highlight_list:\n",
        "                                if len(highlight) > 0:\n",
        "                                    code_highlight_dict[code] = [highlight]\n",
        "                                    #print(f\"alternate-code:{code}, Coordinate: {highlight}\")\n",
        "                                    self.code_y2_coords_list.extend([highlight[3]])\n",
        "\n",
        "                filtered_code_highlight_dict = filter_duplicate_coordinate(code_highlight_dict)\n",
        "                # now, let highlight every code and its common words\n",
        "                for code, highlight_list in filtered_code_highlight_dict.items():\n",
        "                    prev_highlight = fitz.Rect(0, 0, 0, 0)\n",
        "                    for highlight in highlight_list:\n",
        "                        json_data_object = {}\n",
        "                        keyword = self.get_keyword(code)\n",
        "                        # get match score and common words coordinate\n",
        "                        all_match_keyword_list, all_match_imp_keyword_list = self.get_best_token_match(page_num, code,\n",
        "                                                                                                       page,\n",
        "                                                                                                       highlight[3],\n",
        "                                                                                                       self.match_threshold)\n",
        "\n",
        "                        # Step-1: highlight and set color coding dont have common words\n",
        "                        if not all_match_keyword_list:\n",
        "                            page_highlight = page.add_highlight_annot(highlight)\n",
        "                            page_highlight.set_colors(stroke=[0.92, 0.59, 0.48])  # dark salmon\n",
        "                            page_highlight.update()\n",
        "                            # prepare json object\n",
        "                            json_data.append(self.prepare_json_object(num_page, code, highlight, keyword))\n",
        "\n",
        "                        # Step-2: highlight and set color coding that have common words\n",
        "                        for match_keyword_dict in all_match_keyword_list:\n",
        "                            # highlight ICD-10 code\n",
        "                            if not self.is_coord_equal(prev_highlight, highlight):\n",
        "                              self.highlight_code(page, highlight, match_keyword_dict[\"score\"])\n",
        "                              prev_highlight = highlight\n",
        "                            # highlight common words\n",
        "                            highlight_coords_dict = {}\n",
        "                            highlight_coords_dict, highlighted_word_list = self.highlight_common_words(page, match_keyword_dict[\"common_words_coords\"])\n",
        "\n",
        "                            # prepare json object\n",
        "                            json_data.append(self.prepare_json_object(num_page, code, highlight, keyword, match_keyword_dict, highlight_coords_dict))\n",
        "\n",
        "        # build and write json file\n",
        "        json_data_dump_file = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}_{cfg.json_output['OUT_2']}\"\n",
        "        with open(json_data_dump_file, \"w\") as json_out_file:\n",
        "          json.dump(json_data, json_out_file)\n",
        "\n",
        "        # create highlighted pdf file\n",
        "        pdf_output_file_name = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}_{cfg.pdf_output['OUT_2']}\"\n",
        "        pdf_file.save(pdf_output_file_name, garbage=4, deflate=True, clean=True)\n",
        "\n",
        "        return pdf_output_file_name, json_data_dump_file\n",
        "\n",
        "    def highlight_keyword_impairment(self, pdf_file_name=None):\n",
        "        pdf_file = fitz.open(pdf_file_name)\n",
        "        json_imp_data = []\n",
        "        for page_num, page in enumerate(pdf_file):\n",
        "          self.highlighted_coord_list = []\n",
        "          # highlight ICD key phrase\n",
        "          if page_num in self.impairment_keyword_dict:\n",
        "            json_imp_data_list = []\n",
        "            num_page = page_num + 1\n",
        "            key_phrase_sents = self.impairment_keyword_dict[page_num]\n",
        "            for key_phrase_sent in key_phrase_sents:\n",
        "              coordinates = page.search_for(key_phrase_sent)\n",
        "              cords_list = []\n",
        "              for inst in coordinates:\n",
        "                key_phrase_list = key_phrase_sent.split()\n",
        "                for key_phrase in key_phrase_list:\n",
        "                  if self.is_exact_match(page, key_phrase, inst, full_match=True, case_sensitive=True) and not self.is_coord_already_highlighted(inst):\n",
        "                    self.draw_keyword_box(page, inst, key_phrase)\n",
        "                    cords_list.append(inst)\n",
        "                    self.highlighted_coord_list.append(inst)\n",
        "              if len(cords_list) > 0:\n",
        "                json_imp_data_list.append(self.prepare_imp_json_object(key_phrase_sent, cords_list))\n",
        "            if len(json_imp_data_list) > 0:\n",
        "              json_imp_data.append({\n",
        "                f\"page-{num_page}\": json_imp_data_list\n",
        "              })\n",
        "\n",
        "        json_imp_data_dump_file = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}_{cfg.json_output['OUT_3']}\"\n",
        "        with open(json_imp_data_dump_file, \"w\") as json_out_file:\n",
        "            json.dump(json_imp_data, json_out_file)\n",
        "\n",
        "        # create highlighted pdf file\n",
        "        pdf_output_file_name = f\"{self.OUTPUT_FILES_PATH}/{pdf_file_name.split('/')[1].split('.')[0]}_{cfg.pdf_output['OUT_3']}\"\n",
        "        pdf_file.save(pdf_output_file_name, garbage=4, deflate=True, clean=True)\n",
        "        return pdf_output_file_name, json_imp_data_dump_file\n",
        "\n",
        "    def highlight_code(self, page_obj, p_highlight, match_score=0):\n",
        "        # highlight code if threshold is more than match threshold and common words exists\n",
        "        if match_score >= self.match_threshold:\n",
        "            page_highlight = page_obj.add_highlight_annot(p_highlight)\n",
        "            page_highlight.set_colors(stroke=[0.66, 1, 0.07])  # light green\n",
        "            page_highlight.update()\n",
        "        else:\n",
        "            # highlight and set color coding dont have common words\n",
        "            page_highlight = page_obj.add_highlight_annot(p_highlight)\n",
        "            page_highlight.set_colors(stroke=[0.92, 0.59, 0.48])  # dark salmon\n",
        "            page_highlight.update()\n",
        "\n",
        "    def highlight_alternate_code(self, page_obj, p_code):\n",
        "        for alt_code in self.get_alternate_code_pattern(p_code):\n",
        "            highlight_list = page_obj.search_for(alt_code)\n",
        "            for highlight in highlight_list:\n",
        "                if len(highlight) > 0:\n",
        "                    #print(f\"alternate-code:{p_code}, Coordinate: {highlight}\")\n",
        "                    self.code_y2_coords_list.extend([highlight[3]])\n",
        "                    # highlight pdf for option pattern\n",
        "                    self.highlight_code(page_obj, highlight)\n",
        "\n",
        "    def draw_keyword_box(self, p_page, pinst, p_key_phrase):\n",
        "      rect = fitz.Rect(pinst[0] - 2, pinst[1], pinst[2] + 5, pinst[3] + 2)\n",
        "      textwidth = fitz.get_text_length(p_key_phrase)\n",
        "      pivot = fitz.Point(rect.x0, rect.y0 + rect.height/2) #middle of left side\n",
        "      matrix = fitz.Matrix(textwidth/rect.width, 1.0)\n",
        "      p_page.draw_rect(rect, color=(0.439, 0.160, 0.388)) # red (0.439, 0.160, 0.388)\n",
        "      #p_page.insert_textbox(rect, f'{p_key_phrase}', color=(0.529, 0.807, 0.921)) # sky blue\n",
        "      \n",
        "    def highlight_common_words(self, page_obj, common_words_coord_dict):\n",
        "        highlight_coords_dict = {}\n",
        "        highlighted_word_list = []\n",
        "        prev_common_word = \"\"\n",
        "        for common_word, common_word_coord in common_words_coord_dict.items():\n",
        "            highlight = page_obj.add_highlight_annot(common_word_coord)\n",
        "            highlight.update()\n",
        "            if prev_common_word.lower() != common_word.lower():\n",
        "                highlight_coords_dict[common_word] = common_word_coord\n",
        "                highlighted_word_list.append(common_word)\n",
        "                prev_common_word = common_word\n",
        "        return highlight_coords_dict, highlighted_word_list\n",
        "\n",
        "    def highlight_impairment(self, page_obj, imp_keywords_coord_dict):\n",
        "        highlight_coords_list = []\n",
        "        for keyword_impairment, imp_keywords_coords in imp_keywords_coord_dict.items():\n",
        "          #print(f\"keyword_impairment: {keyword_impairment}\")\n",
        "          cords_list = []\n",
        "          for imp_keywords_coord in imp_keywords_coords:\n",
        "            key_phrase_list = keyword_impairment.split()\n",
        "            for key_phrase in key_phrase_list:\n",
        "              if self.is_exact_match(page_obj, key_phrase, imp_keywords_coord, full_match=True, case_sensitive=True) and not self.is_coord_already_highlighted(imp_keywords_coord):\n",
        "                self.draw_keyword_box(page_obj, imp_keywords_coord, key_phrase)\n",
        "                self.highlighted_coord_list.append(imp_keywords_coord)\n",
        "                cords_list.append(imp_keywords_coord)\n",
        "          if len(cords_list) > 0:\n",
        "            #print(f\"keyword_impairment2: {keyword_impairment}\")\n",
        "            highlight_coords_list.append(self.prepare_imp_json_object(keyword_impairment, cords_list))\n",
        "        return highlight_coords_list\n",
        "\n",
        "    def get_best_token_match(self, page_num, p_code, page_obj, code_y2_coord, match_threshold):\n",
        "        all_match_keyword_list = []\n",
        "        all_match_imp_keyword_list = []\n",
        "\n",
        "        # Step 1: reverse code pattern\n",
        "        reversed_icd_code = reverse_code_pattern(p_code)\n",
        "        # Step 2: fetch keyword based on code\n",
        "        keyword = self.get_keyword(reversed_icd_code)\n",
        "        # Step 3: get code paragraph\n",
        "        code_paragraph_list, non_code_paragraph_list = self.get_paragraph(page_obj, code_y2_coord)\n",
        "        # print(f\"code_paragraph_list: {non_code_paragraph_list}\")\n",
        "\n",
        "        # Step 4: prepare code_paragraph for common words\n",
        "        for code_paragraph in code_paragraph_list:\n",
        "            match_keyword_dict = {}\n",
        "            common_words = self.get_common_words(keyword, code_paragraph)\n",
        "            if len(common_words) > 0:\n",
        "                match_keyword_dict[\"common_words\"] = common_words\n",
        "                # Step 4: get best token match ratio\n",
        "                clean_paragraph = \" \".join(self.clean_text(code_paragraph))\n",
        "                match_keyword_dict[\"paragraph\"] = clean_paragraph\n",
        "                # match_score = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "                # if fuzz.token_set_ratio(keyword, clean_paragraph) >= match_threshold else 0\n",
        "                match_keyword_dict[\"score\"] = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "                # Step 5: build common words coordinate dict\n",
        "                common_words_coord_dict = {}\n",
        "                print(f\"code_y2_coord: {code_y2_coord}\")\n",
        "                for common_word in common_words:\n",
        "                    highlight_list = page_obj.search_for(common_word)\n",
        "                    #print(f\"highlight_list: {highlight_list}\")\n",
        "                    found_coord = False\n",
        "                    for highlight in highlight_list:\n",
        "                        # get common word y2 coord value\n",
        "                        common_word_y2_coords = highlight[3]\n",
        "                        common_word_y1_coords = highlight[1]\n",
        "                        if (code_y2_coord - 2) <= common_word_y2_coords <= (code_y2_coord + 2):\n",
        "                          common_words_coord_dict[common_word] = highlight\n",
        "                          found_coord = True\n",
        "                        if not found_coord:\n",
        "                          if (code_y2_coord - 20) <= common_word_y2_coords <= (code_y2_coord + 20):\n",
        "                            common_words_coord_dict[common_word] = highlight\n",
        "                            # self.already_found_word_list.append(common_word)\n",
        "                match_keyword_dict[\"common_words_coords\"] = common_words_coord_dict\n",
        "                all_match_keyword_list.append(match_keyword_dict)\n",
        "\n",
        "        def get_keyword_impairment_list(p_paragraph):\n",
        "            keyword_impairments = []\n",
        "            for keyword_impairment in self.impairment_keyword_dict[page_num]:\n",
        "                keyword_impairment_found = self.get_common_words(keyword_impairment, p_paragraph)\n",
        "                if len(keyword_impairment_found) > 0:\n",
        "                    keyword_impairments.append(keyword_impairment)\n",
        "            return keyword_impairments\n",
        "\n",
        "        # Step 5: prepare non code_paragraph for common words\n",
        "        for non_code_paragraph in non_code_paragraph_list:\n",
        "            match_imp_keyword_dict = {}\n",
        "            imp_common_words_coord_dict = {}\n",
        "            keyword_impairment_list = get_keyword_impairment_list(non_code_paragraph)\n",
        "            for keyword_impairment in keyword_impairment_list:\n",
        "                highlight_list = page_obj.search_for(keyword_impairment)\n",
        "                highlight_list = [highlight for highlight in highlight_list if highlight[3] not in self.code_y2_coords_list]\n",
        "                if keyword_impairment not in self.already_found_word_list and len(highlight_list) > 0:\n",
        "                    self.already_found_word_list.append(keyword_impairment)\n",
        "                    imp_common_words_coord_dict[f\"{keyword_impairment}\"] = highlight_list\n",
        "                    continue\n",
        "            match_imp_keyword_dict[\"imp_common_keyword_coords\"] = imp_common_words_coord_dict\n",
        "            all_match_imp_keyword_list.append(match_imp_keyword_dict)\n",
        "\n",
        "        return all_match_keyword_list, all_match_imp_keyword_list\n",
        "    \n",
        "    def get_best_all_token_match(self, page_num, p_code, page_obj, code_y2_coord, match_threshold):\n",
        "        all_match_keyword_list = []\n",
        "        all_match_imp_keyword_list = []\n",
        "\n",
        "        # Step 1: reverse code pattern\n",
        "        reversed_icd_code = reverse_code_pattern(p_code)\n",
        "        # Step 2: fetch keyword based on code\n",
        "        keyword = self.get_keyword(reversed_icd_code)\n",
        "        # Step 3: get all paragraph\n",
        "        all_paragraph_list = self.get_all_paragraph(page_obj)\n",
        "        # print(f\"code_paragraph_list: {non_code_paragraph_list}\")\n",
        "\n",
        "        def get_keyword_impairment_list(p_paragraph):\n",
        "          keyword_impairments = []\n",
        "          for keyword_impairment in self.impairment_keyword_dict[page_num]:\n",
        "              keyword_impairment_found = self.get_common_words(keyword_impairment, p_paragraph)\n",
        "              if len(keyword_impairment_found) > 0:\n",
        "                  keyword_impairments.append(keyword_impairment)\n",
        "          return keyword_impairments\n",
        "\n",
        "        # Step 4: prepare all_paragraph for common words and impairment\n",
        "        for all_paragraph in all_paragraph_list:\n",
        "          match_keyword_dict = {}\n",
        "          common_words = self.get_common_words(keyword, all_paragraph)\n",
        "          if len(common_words) > 0:\n",
        "              match_keyword_dict[\"common_words\"] = common_words\n",
        "              # Step 4: get best token match ratio\n",
        "              clean_paragraph = \" \".join(self.clean_text(all_paragraph))\n",
        "              match_keyword_dict[\"paragraph\"] = clean_paragraph\n",
        "              # match_score = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "              # if fuzz.token_set_ratio(keyword, clean_paragraph) >= match_threshold else 0\n",
        "              match_keyword_dict[\"score\"] = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "              # Step 5: build common words coordinate dict\n",
        "              common_words_coord_dict = {}\n",
        "              for common_word in common_words:\n",
        "                  highlight_list = page_obj.search_for(common_word)\n",
        "                  found_coord = False\n",
        "                  for highlight in highlight_list:\n",
        "                      # get common word y2 coord value\n",
        "                      common_word_y2_coords = highlight[3]\n",
        "                      if (code_y2_coord - 2) <= common_word_y2_coords <= (code_y2_coord + 2):\n",
        "                        common_words_coord_dict[common_word] = highlight\n",
        "                        found_coord = True\n",
        "                      if not found_coord:\n",
        "                        if (code_y2_coord - 20) <= common_word_y2_coords <= (code_y2_coord + 20):\n",
        "                          common_words_coord_dict[common_word] = highlight\n",
        "                          # self.already_found_word_list.append(common_word)\n",
        "              match_keyword_dict[\"common_words_coords\"] = common_words_coord_dict\n",
        "              all_match_keyword_list.append(match_keyword_dict)\n",
        "          else:\n",
        "            # Step 5: prepare non code_paragraph for common words\n",
        "            match_imp_keyword_dict = {}\n",
        "            imp_common_words_coord_dict = {}\n",
        "            keyword_impairment_list = get_keyword_impairment_list(all_paragraph)\n",
        "            for keyword_impairment in keyword_impairment_list:\n",
        "              # print(f\"keyword_impairment11: {keyword_impairment}\")\n",
        "              highlight_list = page_obj.search_for(keyword_impairment)\n",
        "              if keyword_impairment not in self.already_found_word_list and len(highlight_list) > 0:\n",
        "                self.already_found_word_list.append(keyword_impairment)\n",
        "                imp_common_words_coord_dict[f\"{keyword_impairment}\"] = highlight_list\n",
        "                # print(f\"keyword_impairment22: {keyword_impairment}, coords: {highlight_list}\")\n",
        "                continue\n",
        "            match_imp_keyword_dict[\"imp_common_keyword_coords\"] = imp_common_words_coord_dict\n",
        "            all_match_imp_keyword_list.append(match_imp_keyword_dict)\n",
        "\n",
        "        return all_match_keyword_list, all_match_imp_keyword_list\n",
        "\n",
        "    def prepare_json_object(self, num_page, code, highlight, keyword, match_keyword_dict=None, highlight_coords_dict=None):\n",
        "        # prepare json object\n",
        "        json_data_object = {}\n",
        "        actual_code = reverse_code_pattern(code)\n",
        "        json_data_object[f\"page-{num_page}\"] = {\n",
        "          \"code_type\": \"ICD-10\",\n",
        "          \"actual_code\": actual_code,\n",
        "          \"code_found\": code,\n",
        "          \"code_coord\": f\"{highlight}\",\n",
        "          \"code_desc\": keyword if keyword else \"UNVALIDATED\",\n",
        "          \"common_keywords\": list(set([w.lower() for w in match_keyword_dict[\"common_words\"]])) if match_keyword_dict is not None else 'UNVALIDATED',\n",
        "          \"keyword_coords\": f\"{highlight_coords_dict if highlight_coords_dict is not None else ''}\",\n",
        "          \"keyword_found\": True if match_keyword_dict is not None else False,\n",
        "          \"match_confidence\": match_keyword_dict['score'] if match_keyword_dict is not None else 0.0,\n",
        "          \"synid\": self.get_code_synid(actual_code),\n",
        "          \"field_target\": self.get_field_target(actual_code)\n",
        "        }\n",
        "        return json_data_object\n",
        "    \n",
        "    def prepare_imp_json_object(self, key_phrase_sent, cords_list):\n",
        "      json_data_object = {\n",
        "        \"keywords\": key_phrase_sent,\n",
        "        \"coordinate\": f\"{cords_list}\",\n",
        "        \"synid\": self.get_synid(key_phrase_sent)\n",
        "      }\n",
        "      return json_data_object\n",
        "\n",
        "    def is_coord_equal(self, highlight1, highlight2):\n",
        "      if highlight1[0] == highlight2[0] and highlight1[1] == highlight2[1] and highlight1[2] == highlight2[2] and highlight1[3] == highlight2[3]:\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "    def is_coord_already_highlighted(self, highlight2):\n",
        "      is_coord_already_highlighted = False\n",
        "      for highlight1 in self.highlighted_coord_list:\n",
        "        if highlight1[0] == highlight2[0] and highlight1[1] == highlight2[1] and highlight1[2] == highlight2[2] and highlight1[3] == highlight2[3]:\n",
        "          is_coord_already_highlighted = True\n",
        "          break\n",
        "      return is_coord_already_highlighted\n",
        "\n",
        "    def search_icd_code(self, txt_list):\n",
        "      pdf_page_vocab = {}\n",
        "      for txt_file in txt_list:\n",
        "        with open(txt_file, \"r\") as f:\n",
        "          page_txt = f.read()\n",
        "          # check the page that have line number instead of code\n",
        "          index_page = False\n",
        "          if re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "            index_page = True\n",
        "\n",
        "          doc = self.nlp_code10(page_txt)\n",
        "          code_list = []\n",
        "          if index_page:\n",
        "            # check the code contain letter \"L\"\n",
        "            code_list = [ent.text for ent in doc.ents if not re.search(\"(L[0-9]+)\", ent.text)]\n",
        "          else:\n",
        "            code_list = [ent.text for ent in doc.ents]\n",
        "        \n",
        "          if len(code_list) != 0:\n",
        "            page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "            pdf_page_vocab[page_number] = list(set(code_list))\n",
        "            # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "      return pdf_page_vocab\n",
        "\n",
        "    def get_keyword(self, p_code):\n",
        "        keyword = \"\"\n",
        "        # reverse code if required\n",
        "        code = reverse_code_pattern(p_code)\n",
        "        # get keyword from dataset\n",
        "        keyword_list = list(self.code_df.loc[self.code_df[\"Code\"] == code][\"Keyword\"])\n",
        "        if len(keyword_list) > 0:\n",
        "            keyword = keyword_list[0]\n",
        "        return keyword\n",
        "\n",
        "    def get_code_synid(self, code):\n",
        "      synid = \"No_SynId\"\n",
        "      # get synid from dataset\n",
        "      synid_list = list(self.code_df.loc[self.code_df[\"Code\"] == code][\"SynId\"])\n",
        "      if len(synid_list) > 0:\n",
        "        synid = \"No_SynId\" if synid_list[0] is None else synid_list[0]\n",
        "      return synid\n",
        "\n",
        "    def get_field_target(self, code):\n",
        "      field_target = \"No_Field_Target\"\n",
        "      # get synid from dataset\n",
        "      field_target_list = list(self.code_df.loc[self.code_df[\"Code\"] == code][\"Field_Target\"])\n",
        "      if len(field_target_list) > 0:\n",
        "        field_target = \"No_Field_Target\" if field_target_list[0] is None else field_target_list[0]\n",
        "      return field_target\n",
        "\n",
        "    def get_synid(self, keyword_impairment):\n",
        "      synid = \"No_SynId\"\n",
        "      # get synid from dataset\n",
        "      synid_list = list(self.synid_df.loc[self.synid_df[\"Short_Description\"].str.lower() == keyword_impairment.lower()][\"SynId\"])\n",
        "      if len(synid_list) > 0:\n",
        "        synid = synid_list[0]\n",
        "      return synid\n",
        "\n",
        "    def get_paragraph(self, page_obj, code_y2_coord):\n",
        "        code_paragraph_list = []\n",
        "        non_code_paragraph_list = []\n",
        "        page_content = page_obj.get_text(\"blocks\", sort=False)\n",
        "        for content in page_content:\n",
        "            # print(f\"content found: {content[4]}\")\n",
        "            if content[1] <= code_y2_coord <= content[3]:\n",
        "                if len(self.clean_text(content[4])) > 0:\n",
        "                    code_paragraph = content[4]\n",
        "                    code_paragraph_list.append(code_paragraph)\n",
        "            else:\n",
        "                if len(self.clean_text(content[4])) > 0:\n",
        "                    non_code_paragraph_list.append(content[4])\n",
        "        return code_paragraph_list, non_code_paragraph_list\n",
        "\n",
        "    def get_all_paragraph(self, page_obj):\n",
        "        code_paragraph_list = []\n",
        "        page_content = page_obj.get_text(\"text\", sort=False)\n",
        "        # for content in page_content:\n",
        "        if len(self.clean_text(page_content)) > 0:\n",
        "          code_paragraph_list.append(page_content)\n",
        "        return code_paragraph_list\n",
        "\n",
        "    def get_common_words(self, sent1, sent2):\n",
        "        clean_token1 = self.clean_text(sent1)\n",
        "        clean_token2 = self.clean_text(sent2)\n",
        "        token_set1 = set(clean_token1)\n",
        "        token_set2 = set(clean_token2)\n",
        "\n",
        "        common_word_set = set()\n",
        "\n",
        "        def get_common(token_set1, token_set2):\n",
        "            for w1 in token_set1:\n",
        "                for w2 in token_set2:\n",
        "                    if w1.lower() == w2.lower():\n",
        "                        common_word_set.add(w1)\n",
        "\n",
        "        get_common(token_set1, token_set2)\n",
        "        get_common(token_set2, token_set1)\n",
        "        return list(common_word_set)\n",
        "\n",
        "    def clean_text(self, sent):\n",
        "        # tokenize sentence\n",
        "        sent1 = word_tokenize(sent)\n",
        "        # filter stop words\n",
        "        filtered_sent = [w for w in sent1 if not w.lower() in self.stop_words]\n",
        "        filtered_sent = [w for w in filtered_sent if re.sub(re.compile('\\W'), '', w)]\n",
        "        clean_tokens = []\n",
        "        for token in filtered_sent:\n",
        "            if token.find(\"-\"):\n",
        "                tokens = token.split(\"-\")\n",
        "                clean_tokens.extend(tokens)\n",
        "            else:\n",
        "                clean_tokens.append(token)\n",
        "        return clean_tokens\n",
        "\n",
        "    def is_exact_match(self, page, term, clip, full_match=False, case_sensitive=False):\n",
        "      # clip is an item from page.search_for(term, quads=True)\n",
        "      termLen = len(term)\n",
        "      termBboxLen = max(clip.height, clip.width)\n",
        "      termfontSize = termBboxLen/termLen\n",
        "      f = termfontSize * 2\n",
        "\n",
        "      validate = page.get_text(\"blocks\", clip = clip + (-f, -f, f, f), flags=0)[0][4]\n",
        "      flag = 0\n",
        "      if not case_sensitive:\n",
        "          flag = re.IGNORECASE\n",
        "\n",
        "      matches = len(re.findall(f'{term}', validate, flags=flag)) > 0\n",
        "      if full_match:\n",
        "          matches = len(re.findall(f'\\\\b{term}\\\\b', validate))>0\n",
        "      return matches\n",
        "\n",
        "    def is_exact_match2(self, page, term, clip):\n",
        "        # clip is an item from page.search_for(term, quads=True)\n",
        "        termLen = len(term)\n",
        "        termBboxLen = max(clip.height, clip.width)\n",
        "        termfontSize = termBboxLen / termLen\n",
        "        f = termfontSize * 2\n",
        "        # clip = clip.rect\n",
        "        text_block = page.get_text(\"blocks\", clip=clip + (-f, -f, f, f), flags=0)[0][4]\n",
        "        # re.sub(r\"[^a-zA-Z\\d\\s:]\", \"\", \"(HIV]\")\n",
        "        # if re.sub(r\"[^a-zA-Z\\d\\s:]\", \"\", text_block.strip()) in [t for t in term.split()]:\n",
        "        if re.sub(r\"[^a-zA-Z\\d\\s:]\", \"\", text_block.lower().strip()) in [t.lower() for t in term.split()]:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def get_alternate_code_pattern(self, p_code):\n",
        "        # create alternate pattern\n",
        "        code_patterns = []\n",
        "        code_arr = p_code.split(\".\")\n",
        "        if len(code_arr) > 1:\n",
        "            code1 = f\"{code_arr[0]}. {code_arr[1]}\"\n",
        "            code2 = f\"{code_arr[0]} .{code_arr[1]}\"\n",
        "            code3 = f\"{code_arr[0]} . {code_arr[1]}\"\n",
        "            code4 = f\"{code_arr[0]} {code_arr[1]}\"\n",
        "            code44 = f\"{code_arr[0]},{code_arr[1]}\"\n",
        "            code45 = f\"{code_arr[0]}, {code_arr[1]}\"\n",
        "            code46 = f\"{code_arr[0]} ,{code_arr[1]}\"\n",
        "            code47 = f\"{code_arr[0]} , {code_arr[1]}\"\n",
        "            code_patterns.extend([code1, code2, code3, code4, code44, code45, code46, code47])\n",
        "            # handle if the first char of code is missing\n",
        "            alphabats = {\"Z\": \"2\", \"B\": \"8\", \"O\": \"0\", \"S\": \"5\", \"l\": \"1\", \"G\": \"6\", \"o\": \"9\", \"i\": \"1\"}\n",
        "            for key, val in alphabats.items():\n",
        "                if p_code.startswith(key):\n",
        "                    code5 = p_code.replace(key, val)\n",
        "                    code_patterns.extend([code5])\n",
        "                # replcae char on 1 index if it is not present in icd9 code dataset\n",
        "                if p_code.find(val) == 1:\n",
        "                    code6 = replacer(p_code, key, 1)\n",
        "                    code_patterns.extend([code6])\n",
        "                    # replcae char on 2 index\n",
        "                    if p_code.find(val) == 2:\n",
        "                        code7 = replacer(code6, key, 2)\n",
        "                        code_patterns.extend([code7])\n",
        "        return code_patterns\n",
        "\n",
        "\n",
        "def find_nearest(array, value):\n",
        "    array = np.asarray(array)\n",
        "    idx = (np.abs(array - value)).argmin()\n",
        "    return array[idx]\n",
        "\n",
        "\n",
        "def reverse_code_pattern(p_code):\n",
        "    orig_code = p_code\n",
        "\n",
        "    # check for code contains space(\" \")\n",
        "    tmp_code = orig_code.split(\" \")\n",
        "    if len(tmp_code) > 1:\n",
        "        orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "\n",
        "    # check for code contains dot(\".\")\n",
        "    tmp_code = p_code.split(\".\")\n",
        "    if len(tmp_code) > 1:\n",
        "        orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "\n",
        "    # check for code contains comma(\",\")\n",
        "    tmp_code = p_code.split(\",\")\n",
        "    if len(tmp_code) == 2:\n",
        "        orig_code = f\"{tmp_code[0].strip()}.{tmp_code[1].strip()}\"\n",
        "    elif len(tmp_code) == 2:\n",
        "        orig_code = f\"{tmp_code[0].strip()}.{tmp_code[2].strip()}\"\n",
        "\n",
        "    # handle if the first char of code is missing\n",
        "    alphabats = {\"Z\": \"2\", \"B\": \"8\", \"O\": \"0\", \"S\": \"5\", \"l\": \"1\", \"G\": \"6\", \"o\": \"9\", \"i\": \"1\"}\n",
        "    for key, val in alphabats.items():\n",
        "        # replcae char on 0 index\n",
        "        if orig_code.find(val) == 0:\n",
        "            # orig_code = orig_code.replace(val, key)\n",
        "            orig_code = replacer(orig_code, key, 0)\n",
        "        # replcae char on 1 index\n",
        "        if orig_code.find(key) == 1:\n",
        "            orig_code = replacer(orig_code, val, 1)\n",
        "            # replcae char on 2 index\n",
        "            if orig_code.find(key) == 2:\n",
        "                orig_code = replacer(orig_code, val, 2)\n",
        "            break\n",
        "\n",
        "    return orig_code\n",
        "\n",
        "\n",
        "def replacer(s, newstring, index, nofail=False):\n",
        "    # raise an error if index is outside of the string\n",
        "    if not nofail and index not in range(len(s)):\n",
        "        raise ValueError(\"index outside given string\")\n",
        "\n",
        "    # if not erroring, but the index is still not in the correct range..\n",
        "    if index < 0:  # add it to the beginning\n",
        "        return newstring + s\n",
        "    if index > len(s):  # add it to the end\n",
        "        return s + newstring\n",
        "\n",
        "    # insert the new string between \"slices\" of the original\n",
        "    return s[:index] + newstring + s[index + 1:]\n",
        "\n",
        "\n",
        "def create_directory(dir_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)"
      ],
      "metadata": {
        "id": "jRYlndgDUtds"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentence Extractor"
      ],
      "metadata": {
        "id": "W5REPlzYjQ96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceExtractor:\n",
        "  def __init__(self):\n",
        "    self.MAX_WORKERS = 20\n",
        "    self.keyword_matcher = KeywordMatcher()\n",
        "\n",
        "  def get_json_array_list(self, text_path):\n",
        "    json_arr = None\n",
        "    try:\n",
        "      # print(f\"Running '{text_path}'\")\n",
        "      json_arr = self.keyword_matcher.call(text_path)\n",
        "      #print(f\"Got json for '{json_arr}'\")\n",
        "    except Exception as err:\n",
        "      print(f\"Error for file[{text_path}] is:\\n{err}\")\n",
        "    return json_arr\n",
        "\n",
        "  def get_wrong_keyword_dict(self, text_files_list, with_thread=False, with_process=False):\n",
        "      def get_sorted_dict(p_json_arr_list):\n",
        "          wrong_keyword_dict = {\n",
        "              idx: set([list(element.values())[0] for element in json_arr if json_arr])\n",
        "              for idx, json_arr in enumerate(p_json_arr_list)\n",
        "          }\n",
        "          return dict(sorted(wrong_keyword_dict.items(), key=lambda item: item[0]))\n",
        "\n",
        "      if with_thread:\n",
        "          # take care so that unnecessary thread should not be created\n",
        "          workers = min(self.MAX_WORKERS, len(text_files_list))\n",
        "          with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
        "              json_arr_list = executor.map(self.get_json_array_list, text_files_list)\n",
        "          return get_sorted_dict(json_arr_list)\n",
        "      if with_process:\n",
        "          with futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
        "              json_arr_list = executor.map(self.get_json_array_list, text_files_list)\n",
        "          return get_sorted_dict(json_arr_list)\n",
        "      else:\n",
        "          json_arr_list = list(map(self.get_json_array_list, text_files_list))\n",
        "          tmp_wrong_keyword_dict = {\n",
        "              idx: set([list(element.keys())[0] for element in json_arr if json_arr is not None])\n",
        "              for idx, json_arr in enumerate(json_arr_list)\n",
        "          }\n",
        "          return tmp_wrong_keyword_dict\n",
        "\n",
        "  def extract_sentence(self, wrong_keyword_list, sample_text_list):\n",
        "      match_keyword_dict = {}\n",
        "      for key, keyword_set in wrong_keyword_list.items():\n",
        "          match_dicts = {}\n",
        "          for key_phrase in keyword_set:\n",
        "              # print(key, key_phrase)\n",
        "              with open(sample_text_list[key], \"r\") as f:\n",
        "                  file_txt = f.read()\n",
        "              # match_list = re.findall(f\"([^\\n]*?(?i){key_phrase}[^.]*\\.)\", file_txt)\n",
        "              match_list = re.findall(f\"([^\\n]*{key_phrase}[^\\n]*\\n)\", file_txt)\n",
        "              if match_list:\n",
        "                  match_dicts[key_phrase] = [match.replace(\"\\n\", \"\") for match in match_list]\n",
        "          match_keyword_dict[key] = match_dicts\n",
        "      return match_keyword_dict"
      ],
      "metadata": {
        "id": "7idpKdOBUR5j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utility"
      ],
      "metadata": {
        "id": "Y-y8N5VfqzFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fifth_value(row):\n",
        "    return row[5]\n",
        "\n",
        "\n",
        "def get_second_value(row):\n",
        "    return row[2]\n",
        "    \n",
        "def check_append(out_dict, item):\n",
        "    key, x1, y1, x2, y2, *rest = item\n",
        "    if key not in out_dict:\n",
        "        out_dict[key] = [fitz.Rect(x1, y1, x2, y2)]\n",
        "    else:\n",
        "        out_dict[key].append(fitz.Rect(x1, y1, x2, y2))\n",
        "\n",
        "\n",
        "def consolidate_and_change_out_format(final_out):\n",
        "    out_dict = dict()\n",
        "    for items in final_out:\n",
        "        if not isinstance(items, (list, array)):\n",
        "            check_append(out_dict, items)\n",
        "        else:\n",
        "            for item in items:\n",
        "                check_append(out_dict, item)\n",
        "\n",
        "    return out_dict\n",
        "\n",
        "\n",
        "def remove_duplicates(sorted_array):\n",
        "    data = {}\n",
        "    final_output = []\n",
        "    for row in sorted_array:\n",
        "        if row[1] not in data:\n",
        "            data[row[1]] = [row]\n",
        "            # If the x1 is already in the dictionary, append the row to the existing value\n",
        "        else:\n",
        "            data[row[1]].append(row)\n",
        "\n",
        "    # print (*sorted_rows,sep='\\n')\n",
        "    # Iterate over the dictionary and print any keys (x1) that have more than one value (rows)\n",
        "    for key, value in data.items():\n",
        "        if len(value) > 1:\n",
        "            y_values = sorted(value, key=get_second_value)\n",
        "            if len(y_values) > 1:\n",
        "                sorted_value = y_values\n",
        "                for i in range(len(y_values)):\n",
        "                    for j in range(i + 1, len(y_values)):\n",
        "                        if y_values[i][2] != y_values[j][2]:\n",
        "                            final_output.append(y_values)\n",
        "                            # print(y_values)\n",
        "                        elif y_values[i][2] == y_values[j][2]:\n",
        "                            sorted_value = sorted(value, key=get_fifth_value)\n",
        "                last_element = sorted_value[-1]\n",
        "                final_output.append(last_element)\n",
        "                            # print ('[',last_element,']')\n",
        "        else:\n",
        "            # print (value)\n",
        "            final_output.append(value)\n",
        "    return consolidate_and_change_out_format(final_output)\n",
        "\n",
        "\n",
        "def filter_duplicate_coordinate(code_dict):\n",
        "    # sample_text = None\n",
        "    sorted_array = []\n",
        "    # with open(r\"33.dict\") as f:\n",
        "    #     sample_text = eval(f.read())\n",
        "\n",
        "    for key in code_dict:\n",
        "        for highlight in code_dict[key]:\n",
        "            diffXvals = float(highlight[2]) - float(highlight[0])\n",
        "            # print(key,\",\",rect.x1,\",\",rect.y1,\",\",rect.x2,\",\",rect.y2,\",\",diffXvals)\n",
        "            str = key, highlight[0], highlight[1], highlight[2], highlight[3], diffXvals\n",
        "            sorted_array.append(str)\n",
        "            # print (sorted_array)\n",
        "\n",
        "    return remove_duplicates(sorted_array)"
      ],
      "metadata": {
        "id": "61v0ZPveq1Ha"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keyword Matching & Highlighting "
      ],
      "metadata": {
        "id": "7s7ZxYMUBAol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf input_files\n",
        "!mkdir -p input_files"
      ],
      "metadata": {
        "id": "wfJwndx9vcTP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf outputs\n",
        "!mkdir -p outputs\n",
        "#!rm -rf output*.zip"
      ],
      "metadata": {
        "id": "3ArIraAD0zx3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def purge(file_path):\n",
        "  for f in glob.glob(file_path):\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "32V_ohLAgvLg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-0: create highlighter instance\n",
        "INPUT_PDF_FILES_PATH = \"input_files\"\n",
        "\n",
        "highlighter = Highlighter(match_threshold=35)\n",
        "sent_extractor = SentenceExtractor()"
      ],
      "metadata": {
        "id": "leBmw505CQUl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```log\n",
        "CPU times: user 1min 7s, sys: 6.54 s, total: 1min 14s\n",
        "Wall time: 1min 15s\n",
        "```"
      ],
      "metadata": {
        "id": "EkP76OMfYUo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_type = 4"
      ],
      "metadata": {
        "id": "uyQTNjI_gXJJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "wrong_keyword_dict1 = None\n",
        "icd10_code_dict1 = None\n",
        "for pdf_file in os.listdir(INPUT_PDF_FILES_PATH):\n",
        "  pdf_file_name = f\"{INPUT_PDF_FILES_PATH}/{pdf_file}\"\n",
        "\n",
        "  # Step-1: splitting pdf file\n",
        "  print(\"Step-1: Splitting pdf file............\")\n",
        "  pdf_list = highlighter.split_pdf(pdf_file_name)\n",
        "\n",
        "  # Step-2: Extracting text from pdf\n",
        "  print(\"Step-2: Extracting text from pdf............\")\n",
        "  txt_list = highlighter.extract_text_from_pdf(pdf_list)\n",
        "\n",
        "  # Step-3: Searching ICD-10 code\n",
        "  print(\"Step-3: Searching ICD-10 code into text file..........\")\n",
        "  icd10_code_dict = highlighter.search_icd_code(txt_list)\n",
        "  icd10_code_dict1 = icd10_code_dict\n",
        "\n",
        "  # Step-4: Get closet match of ICD-10 keyword\n",
        "  print(\"Step-4: Get closet match of ICD-10 keyword..........\")\n",
        "  wrong_keyword_dict = sent_extractor.get_wrong_keyword_dict(txt_list)\n",
        "  wrong_keyword_dict1 = wrong_keyword_dict\n",
        "  highlighter.set_impairment_keyword_dict(wrong_keyword_dict)\n",
        "\n",
        "  if output_type == 1:\n",
        "    # Step-4: Highlighting ICD-10 code, its description and keyword impairment into PDF file\n",
        "    print(\"Step-5: Highlighting ICD-10 code, its description and keyword impairment into PDF file............\")\n",
        "    pdf_output_file, json_code_output_file = highlighter.highlight_all(icd10_code_dict, pdf_file_name=pdf_file_name)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "    print(f\"Highlighted code and impairment coordinates are saved into [{json_code_output_file}] file.\")\n",
        "  elif output_type == 2:\n",
        "    # Step-5: Highlighting ICD-10 code and its description into pdf\n",
        "    print(\"Step-5: Highlighting ICD-10 code and its description into pdf............\")\n",
        "    pdf_output_file, json_output_file = highlighter.highlight_icd_code_and_common_words(icd10_code_dict, pdf_file_name=pdf_file_name)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "    print(f\"Highlighted coordinates are saved into [{json_output_file}] file.\")\n",
        "  elif output_type == 3:\n",
        "    # Step-6: Highlighting keyword impairment into pdf\n",
        "    print(\"Step-5: Highlighting keyword impairment into pdf............\")\n",
        "    pdf_output_file, json_output_file = highlighter.highlight_keyword_impairment(pdf_file_name=pdf_file_name)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting keyword impairment.\")\n",
        "    print(f\"Highlighted coordinates are saved into [{json_output_file}] file.\")\n",
        "  elif output_type == 4:\n",
        "    # Step-7: Highlighting ICD-10 code, its description and all keyword impairment into pdf\n",
        "    print(\"Step-5: Highlighting ICD-10 code, its description and all keyword impairment into pdf............\")\n",
        "    pdf_output_file, json_code_output_file = highlighter.highlight_all(icd10_code_dict, pdf_file_name=pdf_file_name, highlight_all=True)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "    print(f\"Highlighted code and impairment coordinates are saved into [{json_code_output_file}] file.\")\n",
        "  else:\n",
        "    print(\"Please pass value 1, 2 and 3 for output type.\")\n",
        "\n",
        "  # remove all pdf and text files\n",
        "  purge(\"pdf-files/*.pdf\")\n",
        "  purge(\"txt-files/*.txt\")\n",
        "  pdf_list = []\n",
        "  txt_list = []"
      ],
      "metadata": {
        "id": "hiOT-aTfedMT",
        "outputId": "e1a7ec26-faed-4fa3-ac65-03a69d9ad2e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-1: Splitting pdf file............\n",
            "Step-2: Extracting text from pdf............\n",
            "Step-3: Searching ICD-10 code into text file..........\n",
            "Step-4: Get closet match of ICD-10 keyword..........\n",
            "Step-5: Highlighting ICD-10 code, its description and all keyword impairment into pdf............\n",
            "File[outputs/APS_38600000R_final_all.pdf] is saved after highlighting ICD-10 code\n",
            "Highlighted code and impairment coordinates are saved into [outputs/APS_38600000R_final_all.json] file.\n",
            "CPU times: user 1min 40s, sys: 1.44 s, total: 1min 42s\n",
            "Wall time: 1min 42s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "purge(\"pdf-files/*.pdf\")\n",
        "purge(\"txt-files/*.txt\")"
      ],
      "metadata": {
        "id": "JlOdZaO9IL5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip output4.zip outputs/*.*"
      ],
      "metadata": {
        "id": "MeS8LHhjjfzU",
        "outputId": "3001fd84-0dc9-4584-8feb-ad5ccfe32b90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: outputs/APS_38600000R_final_all.json (deflated 83%)\n",
            "  adding: outputs/APS_38600000R_final_all.pdf (deflated 2%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "icd10_code_dict1"
      ],
      "metadata": {
        "id": "g6DPMRvWKz0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_keyword_dict1[6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVboLP-X4REu",
        "outputId": "b4d26429-4a5c-402b-941c-9b6dafc76079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Acute pancreatitis',\n",
              " 'Hearing Disorder',\n",
              " 'Hearing Loss',\n",
              " 'Hearing loss',\n",
              " 'Hyperlipidemia',\n",
              " 'acute pancreatitis',\n",
              " 'headache',\n",
              " 'hearing loss'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_keyword_dict1[60]"
      ],
      "metadata": {
        "id": "HB4M0PpLN5fm",
        "outputId": "0a938327-8c02-458b-af75-c956a55fa63f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Atrial fibrillation',\n",
              " 'Back Pain',\n",
              " 'HTN',\n",
              " 'Headache',\n",
              " 'Hypertension',\n",
              " 'Hypertriglyceridemia',\n",
              " 'Neurological symptoms',\n",
              " 'Overweight',\n",
              " 'Sciatica',\n",
              " 'WAS'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_keyword_dict1[0]"
      ],
      "metadata": {
        "id": "oy3VrsX1vpuQ",
        "outputId": "f49d8fd6-29d3-4591-8314-349fa86a2626",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Atrial Fibrillation',\n",
              " 'Headache',\n",
              " 'Hypertension',\n",
              " 'Hypertriglyceridemia',\n",
              " 'MD',\n",
              " 'MI',\n",
              " 'WAS'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ICD 10 Code"
      ],
      "metadata": {
        "id": "RIRH21XK258d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p pdf-files\n",
        "!mkdir -p txt-files"
      ],
      "metadata": {
        "id": "Ebs7E8X23lri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define directory path after creating it\n",
        "pdf_files_path = \"pdf-files\"\n",
        "txt_files_path = \"txt-files\"\n",
        "\n",
        "# create nlp instance\n",
        "nlp = English()\n",
        "\n",
        "\n",
        "def split_pdf(pdf_path):\n",
        "  pdf_in_file = open(pdf_path, \"rb\")\n",
        "  pdf = PdfReader(pdf_in_file)\n",
        "  pdf_list = []\n",
        "  for page in range(len(pdf.pages)):\n",
        "      inputpdf = PdfReader(pdf_in_file)\n",
        "      output = PdfWriter()\n",
        "      output.add_page(inputpdf.pages[page])\n",
        "      with open(f\"{pdf_files_path}/page-{page}.pdf\", \"wb\") as outputStream:\n",
        "          output.write(outputStream)\n",
        "          pdf_list.append(f\"page-{page}.pdf\")\n",
        "  return pdf_list\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_list):\n",
        "    txt_file_list = []\n",
        "    i = 0\n",
        "    for pdf_file in pdf_list:\n",
        "        with open(os.path.join(pdf_files_path, pdf_file), \"rb\") as f:\n",
        "            pdf = pdftotext.PDF(f)\n",
        "\n",
        "        # Read all the text into one string\n",
        "        pdf_text = \"\\n\\n\".join(pdf)\n",
        "\n",
        "        # write text into file\n",
        "        with open(f\"{txt_files_path}/page-{str(i)}.txt\", \"a\") as f:\n",
        "            f.write(pdf_text)\n",
        "        txt_file_list.append(f\"{txt_files_path}/page-{str(i)}.txt\")\n",
        "        i += 1\n",
        "    return txt_file_list\n",
        "\n",
        "\n",
        "def get_opt_pattern(icd_10_code):\n",
        "  # create alternate pattern\n",
        "  code_arr = icd_10_code.split(\".\")\n",
        "  if len(code_arr) > 1:\n",
        "    code1 = f\"{code_arr[0]}. {code_arr[1]}\"\n",
        "    code2 = f\"{code_arr[0]} .{code_arr[1]}\"\n",
        "    code3 = f\"{code_arr[0]} . {code_arr[1]}\"\n",
        "    return [code1, code2, code3]\n",
        "  else:\n",
        "    return icd_10_code\n",
        "\n",
        "\n",
        "def highlight_icd10_code(pdf_page_dict: dict, pdf_file_name: str):\n",
        "    pdf_file = fitz.open(pdf_file_name)\n",
        "\n",
        "    def highlight_pdf(highlight):\n",
        "        for inst in highlight:\n",
        "          highlight = page.add_highlight_annot(inst)\n",
        "          highlight.update()\n",
        "          highlight = page.search_for(text_to_be_highlighted)\n",
        "          print(f\"Page-{page_num}: \", code, highlight, end='\\n')\n",
        "\n",
        "    for page_num, page in enumerate(pdf_file):\n",
        "        if page_num in pdf_page_dict:\n",
        "          for code in pdf_page_dict[page_num]:\n",
        "            text_to_be_highlighted = code\n",
        "            highlight = page.search_for(text_to_be_highlighted)\n",
        "            print(f\"Page-{page_num}: \", code, highlight, end='\\n')\n",
        "            if len(highlight) == 0:\n",
        "                alternate_code_list = get_opt_pattern(code)\n",
        "                for alt_code in alternate_code_list:\n",
        "                  text_to_be_highlighted = alt_code\n",
        "                  highlight = page.search_for(text_to_be_highlighted)\n",
        "                  # highlight pdf for option pattern\n",
        "                  highlight_pdf(highlight)\n",
        "            # highlight pdf for main pattern\n",
        "            highlight_pdf(highlight)\n",
        "\n",
        "    output_pdf_file_name = f\"{pdf_file_name.split('.')[0]}_output.pdf\"\n",
        "    pdf_file.save(output_pdf_file_name, garbage=4, deflate=True, clean=True)\n",
        "    return output_pdf_file_name\n",
        "\n",
        "\n",
        "def search_icd_10_code(txt_list):\n",
        "  pdf_page_vocab = {}\n",
        "  for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "      page_txt = f.read()\n",
        "      # filter the page that have line number instead of code\n",
        "      if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "        doc = nlp(page_txt)\n",
        "        code_list = [ent.text for ent in doc.ents]\n",
        "        if len(code_list) != 0:\n",
        "          #print(txt_file)\n",
        "          page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "          pdf_page_vocab[page_number] = code_list\n",
        "          # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "  return pdf_page_vocab"
      ],
      "metadata": {
        "id": "snzCZDEN29Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-1: splitting pdf file\n",
        "pdf_file_name = \"28page.pdf\"\n",
        "pdf_list = split_pdf(pdf_file_name)\n",
        "\n",
        "# Step-2: Extracting text from pdf\n",
        "txt_list = extract_text_from_pdf(pdf_list)\n",
        "\n",
        "# Step-3: loading and updating patterns to Spacy\n",
        "nlp.add_pipe(\"entity_ruler\").from_disk(\"./icd10_code_patterns-v1.jsonl\")\n",
        "\n",
        "# Step-4: Searching ICD-10 code\n",
        "#print (txt_list)\n",
        "pdf_page_vocab = search_icd_10_code(txt_list)\n",
        "\n",
        "# Step-5: Highlighting ICD-10 code into pdf\n",
        "output_file_name = highlight_icd10_code(pdf_page_vocab, pdf_file_name)\n",
        "print(f\"File[{output_file_name}] is saved after highlighting ICD-10 code\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPlZD1rj3D0Z",
        "outputId": "e3130f4e-4908-40fa-c3df-f02f29d7cd92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R53.83 [Rect(447.8690185546875, 54.60003662109375, 471.3388671875, 63.60443115234375)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  R03.0 [Rect(287.4623718261719, 223.8399658203125, 305.3106994628906, 231.8438720703125)]\n",
            "Page-0:  E78.3 []\n",
            "Page-0:  E78.3 [Rect(151.10546875, 232.47998046875, 173.5032958984375, 240.48388671875)]\n",
            "File[28page_output.pdf] is saved after highlighting ICD-10 code\n"
          ]
        }
      ]
    }
  ]
}