{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-extracting-embeddings-from-all-encoder-layers-of-BERT.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNqdMR3mPxrLluDtZ/dWXEG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d0b936911f04cfead656d9703c4ed77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_48777134cb314271b15c238965d56c12",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5c598e2e7bf443c2ba2c4f63b6116746",
              "IPY_MODEL_69b6f5fb09254aacb94da5b80a97c970"
            ]
          }
        },
        "48777134cb314271b15c238965d56c12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c598e2e7bf443c2ba2c4f63b6116746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_204a16eda8384e728d1c4f02d5fa51d9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_55e8b6e598ec4f1d8277dcfb5fe8b608"
          }
        },
        "69b6f5fb09254aacb94da5b80a97c970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_91410c64f70342fcb60efbaf08b5d346",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:14&lt;00:00, 30.2B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7194965c3ed44c43b34d56f5c3ba5361"
          }
        },
        "204a16eda8384e728d1c4f02d5fa51d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "55e8b6e598ec4f1d8277dcfb5fe8b608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91410c64f70342fcb60efbaf08b5d346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7194965c3ed44c43b34d56f5c3ba5361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "452d59eadb2a4c51be0bcf9c861831c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_99e2d68ef455456ebdbd4c7fc1c8657a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_72d1683cb4cb4c88b8a1e720cb6fff0e",
              "IPY_MODEL_f705e2581c654f1fa84daab61bb62b94"
            ]
          }
        },
        "99e2d68ef455456ebdbd4c7fc1c8657a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72d1683cb4cb4c88b8a1e720cb6fff0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bcb45bdbd18b4b14a18fe4a05492dc45",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f5c708fcd59342f4bfdc163bfd41365d"
          }
        },
        "f705e2581c654f1fa84daab61bb62b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c638677e6508421db4b5930fe489f13a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:13&lt;00:00, 31.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3697209def3f4bc7b26c3b08f0af0981"
          }
        },
        "bcb45bdbd18b4b14a18fe4a05492dc45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f5c708fcd59342f4bfdc163bfd41365d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c638677e6508421db4b5930fe489f13a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3697209def3f4bc7b26c3b08f0af0981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2750c71c564041049b2a50533a430d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_12effe4aab5c4db4bbd45fcd793794fc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_964c8754f4654e8abe619a72c228c86c",
              "IPY_MODEL_392a6f81f4dd4e339e09bae785195383"
            ]
          }
        },
        "12effe4aab5c4db4bbd45fcd793794fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "964c8754f4654e8abe619a72c228c86c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ff38da67e33043b395a364763d9b170e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07345666a72c4635af4fb1a3ccdc2fe7"
          }
        },
        "392a6f81f4dd4e339e09bae785195383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4cef1e4a17694eeeb190977865bff8dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.24MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7126e18276f84645997e7e7d91dcd817"
          }
        },
        "ff38da67e33043b395a364763d9b170e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07345666a72c4635af4fb1a3ccdc2fe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4cef1e4a17694eeeb190977865bff8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7126e18276f84645997e7e7d91dcd817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/getting-started-with-google-bert/blob/main/3-getting-hands-on-with-BERT/2_extracting_embeddings_from_all_encoder_layers_of_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-01JQRyEyPyK"
      },
      "source": [
        "## Extracting embeddings from all encoder layers of BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNSMwh6xyy5q"
      },
      "source": [
        "Pre-training BERT from scratch is computationally expensive. So, we can download the pre-trained BERT model and use it. Google has open sourced the pre-trained BERT model and we can download it from Google Research's GitHub repository – https://github.com/google-research/bert. They have released the pre-trained BERT model with various configurations.\n",
        "\n",
        "The pre-trained model is also available in the BERT-uncased and BERT-cased formats. In BERT-uncased, all the tokens are lowercased, but in BERT-cased, the tokens are not lowercased and are used directly for training. \n",
        "\n",
        "The BERT-uncased model is the one that is most commonly used, but if we are working on certain tasks such as **Named Entity Recognition (NER)** where we have to preserve the case, then we should use the BERT-cased model. Along with these, Google also released pre-trained BERT models trained using the whole word masking method.\n",
        "\n",
        "We can use the pre-trained model in the following two ways:\n",
        "- As a feature extractor by extracting embeddings\n",
        "- By fine-tuning the pre-trained BERT model on downstream tasks such as text\n",
        "classification, question-answering, and more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWWvPi3M1HOG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x28xzbu61xWn"
      },
      "source": [
        "**Hugging Face transformers**\n",
        "\n",
        "Hugging Face is an organization that is on a path to solve and democratize AI through natural language. Their open-source library 'transformers' is very popular among the NLP community. It is very useful and powerful for several NLP and NLU tasks. It includes thousands of pre-trained models in about 100+ languages. One of the many advantages of the transformer library is that it is compatible with both PyTorch and TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUvximYa1IeA"
      },
      "source": [
        "%%capture\n",
        "!pip install torch==1.4.0\n",
        "!pip install transformers==3.5.1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjC56MC91X7K"
      },
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADJ2dXfx1pln"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYEzpetO2Ge3"
      },
      "source": [
        "Consider a sentence – I love Paris. Say we need to extract the contextual embedding of each word in the sentence. To do this, first, we tokenize the sentence and feed the tokens to the pre-trained BERT model, which will return the embeddings for each of the tokens. Apart from obtaining the token-level (word-level) representation, we can also obtain the sentence-level\n",
        "representation.\n",
        "\n",
        "Let's suppose we want to perform a sentiment analysis task, and say we have the dataset shown in the following figure:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/getting-started-with-google-bert/sample-dataset.png?raw=1' width='800'/>\n",
        "\n",
        "We have sentences and their corresponding labels, where 1 indicates positive sentiment and 0 indicates negative sentiment. We can train a classifier to classify the sentiment of a sentence using the given dataset.\n",
        "\n",
        "But we can't feed the given dataset directly to a classifier, since it has text. So first, we need to vectorize the text. We can vectorize the text using methods such as:-\n",
        "\n",
        "- TF-IDF, \n",
        "- word2vec,\n",
        "- BERT\n",
        "\n",
        "BERT learns the contextual embedding, unlike other context-free embedding models such as word2vec. Now, we will see how to use the pre-trained BERT model to vectorize the sentences in our dataset.\n",
        "\n",
        "Let's take the first sentence in our dataset – `I love Paris`. First, we tokenize the sentence using the WordPiece tokenizer and get the tokens (words).\n",
        "\n",
        "```\n",
        "tokens = [I, love, Paris]\n",
        "```\n",
        "\n",
        "Now, we add the `[CLS]` token at the beginning and the `[SEP]` token at the end.\n",
        "\n",
        "```\n",
        "tokens = [[CLS], I, love, Paris, [SEP]]\n",
        "```\n",
        "\n",
        "Similarly, we can tokenize all the sentences in our training set. But the length of each sentence varies, right? Yes, and so does the length of the tokens. We need to keep the length of all the tokens the same. \n",
        "\n",
        "Say we keep the length of the tokens to 7 for all the sentences in\n",
        "our dataset. If we look at our preceding tokens list, the tokens length is 5. To make the tokens length 7, we add a new token called `[PAD]`.\n",
        "\n",
        "```\n",
        "tokens = [[CLS], I, love, Paris, [SEP], [PAD], [PAD]]\n",
        "```\n",
        "\n",
        "As we can observe, now our tokens length is 7, as we have added two [PAD] tokens. \n",
        "\n",
        "**The next step is to make our model understand that the `[PAD]` token is added only to match the tokens length and it is not part of the actual tokens. To do this, we introduce an attention mask. We set the attention mask value to 1 in all positions and 0 to the position where we have a `[PAD]` token.**\n",
        "\n",
        "```\n",
        "attention_mask = [ 1,1,1,1,1,0,0]\n",
        "```\n",
        "\n",
        "Next, we map all the tokens to a unique token ID. Suppose the following is the mapped token ID:\n",
        "\n",
        "```\n",
        "token_ids = [101, 1045, 2293, 3000, 102, 0, 0]\n",
        "```\n",
        "\n",
        "It implies that ID 101 indicates the token [CLS], 1045 indicates the token I, 2293 indicates the token Love, and so on.\n",
        "\n",
        "**Now, we feed token_ids along with attention_mask as input to the pre-trained BERT model and obtain the vector representation (embedding) of each of the tokens.**\n",
        "\n",
        "As we can see, once we feed the tokens as the input, encoder 1 computes the representation of all the tokens and sends it to the next encoder, which is encoder 2. Encoder 2 takes the representation computed by encoder 1 as input, computes its representation, and sends it to the next encoder, which is encoder 3. In this way, each encoder sends its representation to the next\n",
        "encoder above it. The final encoder, which is encoder 12, returns the\n",
        "final representation (embedding) of all the tokens in our sentence:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/getting-started-with-google-bert/pre-trained-BERT.png?raw=1' width='800'/>\n",
        "\n",
        "**Thus, in this way, we can obtain the representation of each of the tokens. These representations are basically the contextualized word (token) embeddings.** Say we are using the pre-trained BERT-base model; in that case, the representation size of each token is 768.\n",
        "\n",
        "We learned how to extract the embedding from the pre-trained BERT model in the previous notebook. We learned that they are the embeddings obtained from the final encoder layer.\n",
        "\n",
        "Now the question is, should we consider the embeddings obtained only from the final encoder layer (final hidden state), or should we also consider the embeddings obtained from all the encoder layers (all hidden states)?\n",
        "\n",
        "Let's represent the input embedding layer with $h_0$, the first encoder layer (first hidden layer) with $h_1$, the second encoder layer (second hidden layer) with $h_2$, and so on to the final twelfth encoder layer, $h_{12}$.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/getting-started-with-google-bert/pre-trained-BERT2.png?raw=1' width='800'/>\n",
        "\n",
        "Instead of taking the embeddings (representations) only from the final encoder layer, the researchers of BERT have experimented with taking embeddings from different encoder layers.\n",
        "\n",
        "For instance, for NER task, the researchers have used the pre-trained BERT model to extract features. Instead of using the embedding only from the final encoder layer (final hidden layer) as a feature, they have experimented with using embeddings from other encoder layers (other hidden layers) as features and obtained the following F1 score:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/getting-started-with-google-bert/f1-score.png?raw=1' width='800'/>\n",
        "\n",
        "As we can observe from the preceding table, concatenating the embeddings of the last four encoder layers (last four hidden layers) gives us a greater F1 score of 96.1%. Thus, instead of taking embeddings only from the final encoder layer (final hidden layer), we can also use embeddings from the other encoder layers.\n",
        "\n",
        "\n",
        "We learned how to obtain the representation for each word in the sentence `I love Paris`. But how do we obtain the representation of the complete sentence?\n",
        "\n",
        "We learned that we have prepended the `[CLS]` token to the beginning of our sentence. The representation of the `[CLS]` token will hold the aggregate representation of the complete sentence. So, we can ignore the embeddings of all other tokens and take the embedding of the `[CLS]` token and assign it as a representation of our sentence. Thus, the representation of our sentence `I love Paris` is just the representation of the `[CLS]` token $R_{[CLS]}$.\n",
        "\n",
        "In a very similar fashion, we can compute the vector representation of all the sentences in our training set. Once we have the sentence representation of all the sentences in our training set, we can feed those representations as input and train a classifier to perform a sentiment analysis task.\n",
        "\n",
        "**Note that using the representation of the `[CLS]` token as a sentence representation is not always a good idea. The efficient way to obtain the representation of a sentence is either averaging or pooling the representation of all the tokens.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRIFkzvFAJzP"
      },
      "source": [
        "## Generating BERT embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_GOc2HcAKtN"
      },
      "source": [
        "Now, we will learn how to extract the embeddings from all the encoder layers using the transformers library.\n",
        "\n",
        "We use the 'bert-base-uncased' model. As the name suggests, it is the BERT-base model with 12 encoders and it is trained with uncased tokens. Since we are using BERTbase, the representation size will be 768.\n",
        "\n",
        "Now while downloading the pre-trained BERT model, we need to set `output_hidden_states = True`. Setting this to `True` helps us to obtain embeddings from all the encoder layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n0ByK_Z4AyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "1d0b936911f04cfead656d9703c4ed77",
            "48777134cb314271b15c238965d56c12",
            "5c598e2e7bf443c2ba2c4f63b6116746",
            "69b6f5fb09254aacb94da5b80a97c970",
            "204a16eda8384e728d1c4f02d5fa51d9",
            "55e8b6e598ec4f1d8277dcfb5fe8b608",
            "91410c64f70342fcb60efbaf08b5d346",
            "7194965c3ed44c43b34d56f5c3ba5361",
            "452d59eadb2a4c51be0bcf9c861831c9",
            "99e2d68ef455456ebdbd4c7fc1c8657a",
            "72d1683cb4cb4c88b8a1e720cb6fff0e",
            "f705e2581c654f1fa84daab61bb62b94",
            "bcb45bdbd18b4b14a18fe4a05492dc45",
            "f5c708fcd59342f4bfdc163bfd41365d",
            "c638677e6508421db4b5930fe489f13a",
            "3697209def3f4bc7b26c3b08f0af0981"
          ]
        },
        "outputId": "9ecd44ad-276d-4012-d199-7a773cb1d12c"
      },
      "source": [
        "# Download and load the pre-trained bert-base-uncased model\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d0b936911f04cfead656d9703c4ed77",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "452d59eadb2a4c51be0bcf9c861831c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVZC3T2D4T3k"
      },
      "source": [
        "Next, we download and load the tokenizer that was used to pre-train the `bert-baseuncased` model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JznTE32L4TEC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "2750c71c564041049b2a50533a430d43",
            "12effe4aab5c4db4bbd45fcd793794fc",
            "964c8754f4654e8abe619a72c228c86c",
            "392a6f81f4dd4e339e09bae785195383",
            "ff38da67e33043b395a364763d9b170e",
            "07345666a72c4635af4fb1a3ccdc2fe7",
            "4cef1e4a17694eeeb190977865bff8dc",
            "7126e18276f84645997e7e7d91dcd817"
          ]
        },
        "outputId": "0325e11b-662b-4e92-ca9b-16a0011b6635"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2750c71c564041049b2a50533a430d43",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vq7Hsqo5dof"
      },
      "source": [
        "### Preprocessing the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSR0HsRw5ena"
      },
      "source": [
        "Now, let's see how to preprocess the input before feeding it to BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT-dhPyt5P85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a60fd49-caf6-488f-e199-f9a178a62571"
      },
      "source": [
        "# Define the sentence\n",
        "sentence = \"I love Paris\"\n",
        "\n",
        "# Tokenize the sentence and obtain the tokens\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'love', 'paris']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGn8OfiZ5798"
      },
      "source": [
        "Now, we will add the `[CLS]` token at the beginning and the `[SEP]` token at the end of the tokens list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu2FYQXR54Z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d75db0-b25a-4d59-e4da-cd862fe222f7"
      },
      "source": [
        "tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "print(tokens)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'i', 'love', 'paris', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPcEgKfh6kTt"
      },
      "source": [
        "As we can observe, we have a `[CLS]` token at the beginning and an `[SEP]` token at the end of our tokens list. We can also see that length of our tokens list is 5.\n",
        "\n",
        "Say we need to keep the length of our tokens list to 7; in that case, we add two `[PAD]` tokens at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb2kelXX6Lav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c38264a7-7cde-4bd2-a7ce-8965fb379ec5"
      },
      "source": [
        "tokens = tokens + [\"[PAD]\"] + [\"[PAD]\"]\n",
        "print(tokens)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'i', 'love', 'paris', '[SEP]', '[PAD]', '[PAD]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk_-Bysk7B0U"
      },
      "source": [
        "As we can see, now we have the tokens list with `[PAD]` tokens and the length of our tokens list is 7.\n",
        "\n",
        "Next, we create the attention mask. We set the attention mask value to 1 if the token is not a `[PAD]` token, else we set the attention mask to 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXFLzGD469wC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946df68b-2850-4c60-9c59-af02fc8e27d0"
      },
      "source": [
        "attention_mask = [1 if i != '[PAD]' else 0 for i in tokens]\n",
        "print(attention_mask)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn5QGkHJ7e9C"
      },
      "source": [
        "As we can see, we have attention mask values 0 at positions where have a `[PAD]` token and 1 at other positions.\n",
        "\n",
        "Next, we convert all the tokens to their token IDs as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJVkdgT57YnL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5b1a13-d3ef-423a-efa4-6be2c428e9ef"
      },
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(token_ids)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 1045, 2293, 3000, 102, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1swT50Y7zOI"
      },
      "source": [
        "From the output, we can observe that each token is mapped to a unique token ID.\n",
        "\n",
        "Now, we convert token_ids and attention_mask to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kOLKJf67toP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcf08f5-30b7-4b4e-fec3-d794e016100d"
      },
      "source": [
        "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
        "attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
        "print(token_ids)\n",
        "print(attention_mask)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 101, 1045, 2293, 3000,  102,    0,    0]])\n",
            "tensor([[1, 1, 1, 1, 1, 0, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG1AkkzK8aEE"
      },
      "source": [
        "**That's it. Next, we feed token_ids and attention_mask to the pre-trained BERT model and get the embedding.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnEOvW4K8rgi"
      },
      "source": [
        "### Getting the embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8bfFbNN8tCJ"
      },
      "source": [
        "Since we set output_hidden_states = True while defining the model to get the\n",
        "embeddings from all the encoder layers, now the model returns an output tuple with three values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGgMwFbdChOt"
      },
      "source": [
        "last_hidden_state, pooler_output, hidden_states = model(token_ids, attention_mask=attention_mask)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LywA_1sLJbG"
      },
      "source": [
        "Where,\n",
        "\n",
        "- The first value, `last_hidden_state`, contains the representation of all the\n",
        "tokens obtained only from the final encoder layer (encoder 12).\n",
        "- Next, `pooler_output` indicates the representation of the `[CLS]` token from the final encoder layer, which is further processed by a linear and tanh activation function.\n",
        "- `hidden_states` contains the representation of all the tokens obtained from all the encoder layers.\n",
        "\n",
        "First, let's look at `last_hidden_state`. As we learned, it holds the representation of all the tokens obtained only from the final encoder layer (encoder 12)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4Mh15CzMA4C"
      },
      "source": [
        "Now, let's take a look at the cls_head. It contains the representation of the `[CLS]` token. Let's print the shape of cls_head :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9bwVXbSMci1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba32e52d-8066-4b74-d8c8-718c54df6708"
      },
      "source": [
        "print(last_hidden_state.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 7, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns7px0prMjzF"
      },
      "source": [
        "The size `[1,7,768]` indicates `[batch_size, sequence_length, hidden_size]`.\n",
        "\n",
        "Our batch size is 1. The sequence length is the token length. Since we have 7 tokens, the sequence length is 7. The hidden size is the representation (embedding) size and it is 768 for the BERT-base model.\n",
        "\n",
        "We can obtain the representation of each token as:\n",
        "\n",
        "- `hidden_rep[0][0]` gives the representation of the first token which is `[CLS]`\n",
        "- `hidden_rep[0][1]` gives the representation of the second token which is 'I'\n",
        "- `hidden_repo[0][2]` gives the representation of the third token which is 'love'\n",
        "\n",
        "Similarly, we can obtain the representation of all the tokens from the final encoder layer.\n",
        "\n",
        "Next, we have `pooler_output`, which contains the representation of the `[CLS]` token from the final encoder layer, which is further processed by a linear and tanh activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aomr70MEm8ey",
        "outputId": "b8392068-9be2-4b0c-b97c-b18db8b14fb9"
      },
      "source": [
        "print(pooler_output.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jTcWaUlm8v1"
      },
      "source": [
        "The size `[1,768]` indicates `[batch_size, hidden_size]`.\n",
        "\n",
        "We learned that `[CLS]` token holds the aggregate representation of the sentence, so we can use `pooler_output` as the representation of the sentence `I love Paris`.\n",
        "\n",
        "Finally, we have `hidden_states`, which contains the representation of all the tokens obtained from all the encoder layers. It is a tuple containing 13 values holding the representation of all encoder layers (hidden layers), from the input embedding layer $h_0$ to the final encoder layer $h_{12}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6o28yFtnoX9",
        "outputId": "67178cfb-0123-4b22-9452-1fd5c74e0d7f"
      },
      "source": [
        "print(len(hidden_states))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNY4SGj4nt-K"
      },
      "source": [
        "As we can see, it contains 13 values holding the representation of all layers:\n",
        "\n",
        "- `hidden_states[0]` contains the representation of all the tokens obtained from the input embedding layer $h_0$.\n",
        "- `hidden_states[1]` contains the representation of all the tokens obtained from the input embedding layer $h_1$.\n",
        "- `hidden_states[2]` contains the representation of all the tokens obtained from the input embedding layer $h_2$.\n",
        "- `hidden_states[12]` contains the representation of all the tokens obtained from the input embedding layer $h_{12}$.\n",
        "\n",
        "Let's explore this more. First, let's print the shape of `hidden_states[0]`, which contains the representation of all the tokens obtained from the input embedding layer $h_0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDfOBmrxnqhR",
        "outputId": "e04e4b1c-9f70-455a-d2ee-3845a7ba4661"
      },
      "source": [
        "print(hidden_states[0].shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 7, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3a8JAmhoZbP"
      },
      "source": [
        "The size `[1, 7, 768]` indicates `[batch_size, sequence_length, hidden_size]`.\n",
        "\n",
        "Now, let's print the shape of `hidden_states[1]`, which contains the representation of all tokens obtained from the first encoder layer $h_1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxl-NzHuoTbq",
        "outputId": "369e9a01-0b85-45d1-d2aa-3c127e14460b"
      },
      "source": [
        "print(hidden_states[1].shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 7, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJtgpMKao34A",
        "outputId": "c559d7c0-dbe9-4fb1-a893-abbadf931b82"
      },
      "source": [
        "print(hidden_states[2].shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 7, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTmjTDnQo5_o",
        "outputId": "cb14374f-edca-4fdb-e331-225246674e35"
      },
      "source": [
        "print(hidden_states[12].shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 7, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qz6OvSRomfe"
      },
      "source": [
        "Thus, in this way, we can obtain the embedding of tokens from all the encoder layers. We learned how to use the pre-trained BERT model to extract embeddings; **can we also use pretrained BERT for a downstream task such as sentiment analysis? Yes!**"
      ]
    }
  ]
}