{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TycHgKTZJ6a6"
      ],
      "authorship_tag": "ABX9TyMu0zIqottobvnGoLAB63yz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/text-similarity-works/13_code_keyword_sentence_extraction_and_highlight_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "u6vazPC0Ja4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "!pip install pillow\n",
        "\n",
        "!sudo apt install build-essential libpoppler-cpp-dev pkg-config python3-dev\n",
        "!pip install -U pdftotext\n",
        "!pip install PyPDF2\n",
        "!pip install fitz\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "gdwgVvXuJdz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "import difflib\n",
        "import pickle\n",
        "\n",
        "import fitz\n",
        "import pdftotext\n",
        "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from keyword_extractor import call\n",
        "from concurrent import futures"
      ],
      "metadata": {
        "id": "gKbd7WsfyYX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p pdf-files\n",
        "!mkdir -p txt-files\n",
        "!mkdir -p ocr-pdf-files"
      ],
      "metadata": {
        "id": "WyuzUOer0FFK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define directory path after creating it\n",
        "pdf_files_path = \"pdf-files\"\n",
        "txt_files_path = \"txt-files\"\n",
        "ocr_pdf_files_path = \"ocr-pdf-files\"\n",
        "\n",
        "MAX_WORKERS = 20"
      ],
      "metadata": {
        "id": "6RzBfwkuyfv6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Core Functions"
      ],
      "metadata": {
        "id": "0hK4nU35JVaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_pdf(pdf_path):\n",
        "  pdf_in_file = open(pdf_path, \"rb\")\n",
        "  pdf = PdfFileReader(pdf_in_file)\n",
        "  pdf_list = []\n",
        "  for page in range(pdf.numPages):\n",
        "      inputpdf = PdfFileReader(pdf_in_file)\n",
        "      output = PdfFileWriter()\n",
        "      output.addPage(inputpdf.getPage(page))\n",
        "      with open(f\"{pdf_files_path}/page-{page}.pdf\", \"wb\") as outputStream:\n",
        "          output.write(outputStream)\n",
        "          pdf_list.append(f\"page-{page}.pdf\")\n",
        "  return pdf_list\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_list):\n",
        "    txt_file_list = []\n",
        "    i = 0\n",
        "    for pdf_file in pdf_list:\n",
        "        with open(os.path.join(pdf_files_path, pdf_file), \"rb\") as f:\n",
        "            pdf = pdftotext.PDF(f)\n",
        "\n",
        "        # Read all the text into one string\n",
        "        pdf_text = \"\\n\\n\".join(pdf)\n",
        "\n",
        "        # write text into file\n",
        "        with open(f\"{txt_files_path}/page-{str(i)}.txt\", \"a\") as f:\n",
        "            f.write(pdf_text)\n",
        "        txt_file_list.append(f\"{txt_files_path}/page-{str(i)}.txt\")\n",
        "        i += 1\n",
        "    return txt_file_list\n",
        "\n",
        "\n",
        "def get_opt_pattern(icd_10_code):\n",
        "  # create alternate pattern\n",
        "  code_arr = icd_10_code.split(\".\")\n",
        "  if len(code_arr) > 1:\n",
        "    code1 = f\"{code_arr[0]}. {code_arr[1]}\"\n",
        "    code2 = f\"{code_arr[0]} .{code_arr[1]}\"\n",
        "    code3 = f\"{code_arr[0]} . {code_arr[1]}\"\n",
        "    return [code1, code2, code3]\n",
        "  else:\n",
        "    return icd_10_code\n",
        "\n",
        "\n",
        "def isExactMatch(page, term, clip, fullMatch=False, caseSensitive=False):\n",
        "  # clip is an item from page.search_for(term, quads=True)\n",
        "  termLen = len(term)\n",
        "  termBboxLen = max(clip.height, clip.width)\n",
        "  termfontSize = termBboxLen/termLen\n",
        "  f = termfontSize*2\n",
        "\n",
        "  #clip = clip.rect\n",
        "\n",
        "  validate = page.get_text(\"blocks\", clip = clip + (-f, -f, f, f), flags=0)[0][4]\n",
        "  flag = 0\n",
        "  if not caseSensitive:\n",
        "      flag = re.IGNORECASE\n",
        "\n",
        "  matches = len(re.findall(f'{term}', validate, flags=flag)) > 0\n",
        "  if fullMatch:\n",
        "      matches = len(re.findall(f'\\\\b{term}\\\\b', validate))>0\n",
        "  return matches\n",
        "\n",
        "def highlight_icd_code_and_keyword(icd10_code_dict, \n",
        "                                   icd9_code_dict=None, \n",
        "                                   icd_keywords_dict=None, \n",
        "                                   pdf_file_name=None, \n",
        "                                   cords_file_name=None):\n",
        "  pdf_file = fitz.open(pdf_file_name)\n",
        "  already_highlighted_list = []\n",
        "\n",
        "  def highlight_pdf(highlight, icd10_code, code_type):\n",
        "    cords_list = []\n",
        "    for inst in highlight:\n",
        "      highlight = page.add_highlight_annot(inst)\n",
        "      if code_type == \"ICD-9\":\n",
        "        highlight.set_colors(stroke=[1, 0.5, 0.8]) # light red color (r, g, b)\n",
        "      highlight.update()\n",
        "      highlight = page.search_for(icd10_code)\n",
        "      cords_list.append(highlight)\n",
        "\n",
        "    if cords_list:\n",
        "      num_page = page_num + 1\n",
        "      code_cors_output = f\"Page-{num_page} | {icd10_code}\"\n",
        "      txt_output_file_name.write(\"%s\\n\" % code_cors_output)\n",
        "\n",
        "  # create file to write cordinate \n",
        "  txt_output_file_name = open(cords_file_name, \"a\")\n",
        "\n",
        "  for page_num, page in enumerate(pdf_file):\n",
        "    # highlight ICD-10 code\n",
        "    if page_num in icd10_code_dict:\n",
        "      for code in icd10_code_dict[page_num]:\n",
        "        highlight = page.search_for(code)\n",
        "        if len(highlight) == 0:\n",
        "          alternate_code_list = get_opt_pattern(code)\n",
        "          for alt_code in alternate_code_list:\n",
        "            highlight = page.search_for(alt_code)\n",
        "            # highlight pdf for option pattern\n",
        "            highlight_pdf(highlight, alt_code, code_type=\"ICD-10\")\n",
        "        # highlight pdf for main pattern   \n",
        "        highlight_pdf(highlight, code, code_type=\"ICD-10\")\n",
        "\n",
        "    # highlight ICD-9 code\n",
        "    if icd9_code_dict is not None:\n",
        "      if page_num in icd9_code_dict:\n",
        "        for code in icd9_code_dict[page_num]:\n",
        "          highlight = page.search_for(code)\n",
        "          if len(highlight) == 0:\n",
        "            alternate_code_list = get_opt_pattern(code)\n",
        "            for alt_code in alternate_code_list:\n",
        "              highlight = page.search_for(alt_code)\n",
        "              # highlight pdf for option pattern\n",
        "              highlight_pdf(highlight, alt_code, code_type=\"ICD-9\")\n",
        "          # highlight pdf for main pattern   \n",
        "          highlight_pdf(highlight, code, code_type=\"ICD-9\")\n",
        "\n",
        "    # highlight ICD key phrase\n",
        "    if page_num in icd_keywords_dict:\n",
        "      icd_keyword_dict = icd_keywords_dict[page_num]\n",
        "      for key_phrase, key_phrase_sents in icd_keyword_dict.items():\n",
        "\n",
        "        # do not do anything if already highlited\n",
        "        #if key_phrase_sents[0] in already_highlighted_list:\n",
        "        #  continue\n",
        "        #already_highlighted_list.append(key_phrase_sents[0])\n",
        "\n",
        "        cords_list = []\n",
        "        keyword_cors_output = \"\"\n",
        "        for key_phrase_sent in key_phrase_sents:\n",
        "          coordinates = page.search_for(key_phrase_sent)\n",
        "          #print(f\"Keyword: {keyword}, Length: {len(coordinates)}\")\n",
        "          \n",
        "          for inst in coordinates:\n",
        "            #print(f\"Keyword: {keyword}, inst: {inst}\")\n",
        "            # if isExactMatch(page, key_phrase, inst, fullMatch=True, caseSensitive=True):\n",
        "            highlight = page.add_highlight_annot(inst)\n",
        "            highlight.set_colors(stroke=[1, 0.8, 0.8])\n",
        "            highlight.update()\n",
        "            highlight = page.search_for(key_phrase_sent)\n",
        "            cords_list.append(highlight)\n",
        "            num_page = page_num + 1\n",
        "            keyword_cors_output = f\"Page-{num_page} | {key_phrase} | {key_phrase_sent}\"\n",
        "\n",
        "        if cords_list:\n",
        "          txt_output_file_name.write(\"%s\\n\" % keyword_cors_output)\n",
        "          #print(f\"Page-{page_num}: \", highlight, end='\\n')\n",
        "\n",
        "  txt_output_file_name.close()\n",
        "\n",
        "  pdf_output_file_name = f\"{pdf_file_name.split('.')[0]}_output.pdf\"\n",
        "  pdf_file.save(pdf_output_file_name, garbage=4, deflate=True, clean=True)\n",
        "\n",
        "  return pdf_output_file_name, cords_file_name\n",
        "\n",
        "\n",
        "def filter_unwanted_code(code_list, page_text):\n",
        "    filtered_code_list = []\n",
        "    # if re.search(\"ICD\", page_text):\n",
        "    # match_list = re.findall(\"(ICD-[0-9][a-zA-z]*\\-.+)[ ]\", page_text)\n",
        "    match_list = re.findall(\"(IC[(A-z)]-[0-9][a-zA-z]*\\-.+)[ ]\", page_text)\n",
        "    # print(\"Match list:\\n\", match_list)\n",
        "    for found_code in match_list:\n",
        "        for code in code_list:\n",
        "            if code in found_code:\n",
        "                filtered_code_list.append(code)\n",
        "    return filtered_code_list\n",
        "\n",
        "\n",
        "def search_icd_code(txt_list, nlp, code_type):\n",
        "    pdf_page_vocab = {}\n",
        "    for txt_file in txt_list:\n",
        "        with open(txt_file, \"r\") as f:\n",
        "            page_txt = f.read()\n",
        "            # filter the page that have line number instead of code\n",
        "            if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "                doc = nlp(page_txt)\n",
        "                code_list = [ent.text for ent in doc.ents]\n",
        "                page_number = 0\n",
        "                if len(code_list) != 0:\n",
        "                    page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "                    pdf_page_vocab[page_number] = code_list\n",
        "                    # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "\n",
        "                # filter the page that dont have ICD string into it\n",
        "                if code_type == \"ICD-9\":\n",
        "                    filtered_code_list = filter_unwanted_code(code_list, page_txt)\n",
        "                    pdf_page_vocab[page_number] = filtered_code_list\n",
        "                    # print(f\"Page[{txt_file.split('/')[1]}]: {filtered_code_list}\")\n",
        "\n",
        "    return pdf_page_vocab\n",
        "\n",
        "\n",
        "def get_json_array_list(text_path):\n",
        "  json_arr = None\n",
        "  try:\n",
        "    #print(f\"Running '{text_path}'\")\n",
        "    json_arr = call(text_path)\n",
        "    #print(f\"Got json for '{text_path}'\")\n",
        "  except Exception as err:\n",
        "    print(f\"Error for file[{text_path}] is:\\n{err}\")\n",
        "  return json_arr\n",
        "\n",
        "\n",
        "def get_wrong_keyword_dict_with_thread(text_path_list):\n",
        "  wrong_keyword_dict = {}\n",
        "\n",
        "  # take care so that unnecessary thread should not be created\n",
        "  workers = min(MAX_WORKERS, len(text_path_list))\n",
        "  with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
        "    json_arr_list = executor.map(get_json_array_list, sorted(text_path_list))\n",
        "\n",
        "  for idx, json_arr in enumerate(json_arr_list):\n",
        "    wrong_keyword_list = [list(element.values())[0] for element in json_arr]\n",
        "    if wrong_keyword_list: \n",
        "      wrong_keyword_dict[idx] = set(wrong_keyword_list)\n",
        "  return dict(sorted(wrong_keyword_dict.items(), key=lambda item: item[0]))\n",
        "\n",
        "def get_wrong_keyword_dict_with_process(text_path_list):\n",
        "  wrong_keyword_dict = {}\n",
        "\n",
        "  # take care so that unnecessary thread should not be created\n",
        "  workers = min(MAX_WORKERS, len(text_path_list))\n",
        "  with futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
        "    json_arr_list = executor.map(get_json_array_list, sorted(text_path_list))\n",
        "\n",
        "  for idx, json_arr in enumerate(json_arr_list):\n",
        "    wrong_keyword_list = [list(element.values())[0] for element in json_arr]\n",
        "    if wrong_keyword_list: \n",
        "      wrong_keyword_dict[idx] = set(wrong_keyword_list)\n",
        "  return dict(sorted(wrong_keyword_dict.items(), key=lambda item: item[0]))\n",
        "\n",
        "\n",
        "def get_wrong_keyword_dict(text_path_list):\n",
        "  wrong_keyword_dict = {}\n",
        "  for idx, file_path in enumerate(text_path_list):\n",
        "    #print(idx, file_path)\n",
        "    json_arr = get_json_array_list(file_path)\n",
        "    if json_arr is not None:\n",
        "      wrong_keyword_list = [list(element.values())[0] for element in json_arr]\n",
        "    if wrong_keyword_list:\n",
        "      wrong_keyword_dict[idx] = set(wrong_keyword_list)\n",
        "  return wrong_keyword_dict\n",
        "\n",
        "\n",
        "def extract_sentence2(wrong_kerword_list, sample_text_list):\n",
        "  match_dicts = {}\n",
        "  for key, kerword_set in wrong_kerword_list.items():\n",
        "    for key_phrase in kerword_set:\n",
        "      #print(key, key_phrase)\n",
        "      with open(sample_text_list[key], \"r\") as f:\n",
        "        file_txt = f.read()\n",
        "      match_list = re.findall(f\"([^\\n]*?(?i){key_phrase}[^.]*\\.)\", file_txt)\n",
        "      if match_list:\n",
        "        match_dicts[key_phrase] = [match.replace(\"\\n\", \"\") for match in match_list]\n",
        "  return match_dicts\n",
        "\n",
        "\n",
        "def extract_sentence(wrong_kerword_list, sample_text_list):\n",
        "  match_keyword_dict = {}\n",
        "  # create file to write cordinate \n",
        "  icd_keyword_found_filename = open(\"icd_keyword_found.txt\", \"w\")\n",
        "  icd_keyword_found_filename2 = open(\"icd_keyword_match.txt\", \"w\")\n",
        "  for key, kerword_set in wrong_kerword_list.items():\n",
        "    match_dicts = {}\n",
        "    for key_phrase in kerword_set:\n",
        "      #print(key, key_phrase)\n",
        "      keyword_found_output2 = f\"Page-{key} | {key_phrase} |\\n\"\n",
        "      icd_keyword_found_filename2.write(\"%s\\n\" % keyword_found_output2)\n",
        "\n",
        "      with open(sample_text_list[key], \"r\") as f:\n",
        "        file_txt = f.read()\n",
        "      # match_list = re.findall(f\"([^\\n]*?(?i){key_phrase}[^.]*\\.)\", file_txt)\n",
        "      match_list = re.findall(f\"([^\\n]*{key_phrase}[^\\n]*\\n)\", file_txt)\n",
        "      if match_list:\n",
        "        match_dicts[key_phrase] = [match.replace(\"\\n\", \"\") for match in match_list]\n",
        "    match_keyword_dict[key] = match_dicts\n",
        "    keyword_found_output = f\"Page-{key} | {key_phrase} | {match_dicts}|\\n\"\n",
        "    icd_keyword_found_filename.write(\"%s\\n\" % keyword_found_output)\n",
        "\n",
        "  icd_keyword_found_filename.close()\n",
        "  icd_keyword_found_filename2.close()\n",
        "  return match_keyword_dict\n",
        "\n",
        "\n",
        "def purge(file_path):\n",
        "  for f in glob.glob(file_path):\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "qB-XpoweygN4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Single Searching & Highlighting"
      ],
      "metadata": {
        "id": "N4dDTs-rJL_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-0: Load prerequisite instance\n",
        "# create nlp instance\n",
        "nlp_keyword = spacy.load('en_core_web_sm')\n",
        "\n",
        "# loading and updating patterns for ICD-10 code\n",
        "nlp_code10 = English()\n",
        "nlp_code10.add_pipe(\"entity_ruler\").from_disk(\"./icd10_code_patterns-v3.jsonl\")\n",
        "\n",
        "# loading and updating patterns for ICD-9 code\n",
        "#nlp_code9 = English()\n",
        "#nlp_code9.add_pipe(\"entity_ruler\").from_disk(\"./icd9_code_patterns-v1.jsonl\")"
      ],
      "metadata": {
        "id": "y8UcOwHpJnpd",
        "outputId": "01f0110d-89b0-4fc1-d103-4c0ad2a366be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.entityruler.EntityRuler at 0x7f4456fba740>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "purge(\"pdf-files/*.pdf\")\n",
        "purge(\"txt-files/*.txt\")"
      ],
      "metadata": {
        "id": "5i_7mtx2e56B"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-1: spliting pdf file\n",
        "pdf_file_name = \"APS_24680000R_final.pdf\"\n",
        "pdf_list = split_pdf(pdf_file_name)\n",
        "\n",
        "# Step-2: Extracting text from pdf\n",
        "txt_list = extract_text_from_pdf(pdf_list)"
      ],
      "metadata": {
        "id": "qw22evt02aRh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-3: Searching ICD-10 code\n",
        "page_code10_dict = search_icd_code(txt_list, nlp_code10, code_type=\"ICD-10\")"
      ],
      "metadata": {
        "id": "flLgpUfg2ghi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Step-4: Get coloset match of ICD-10 keyword\n",
        "wrong_keyword_dict = get_wrong_keyword_dict(txt_list)"
      ],
      "metadata": {
        "id": "f7C3J8jbnzPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd6bfe8-2a1a-4b97-bd6b-6fe5ddcad3a8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error for file[txt-files/page-48.txt] is:\n",
            "unbalanced parenthesis at position 4\n",
            "CPU times: user 3min 51s, sys: 902 ms, total: 3min 52s\n",
            "Wall time: 3min 52s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_keyword_dict"
      ],
      "metadata": {
        "id": "X9swFK9Jsizl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Step-5: Extract sentence of ICD-10 keyword\n",
        "icd_keywords_dict = extract_sentence(wrong_keyword_dict, txt_list)"
      ],
      "metadata": {
        "id": "5OaO5U4PnR6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4039078-f865-4b3e-d2cb-27be51b4ca1e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10.2 s, sys: 58 ms, total: 10.2 s\n",
            "Wall time: 10.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "icd_keywords_dict[21]"
      ],
      "metadata": {
        "id": "DGnsLSzCmh3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-6: Highlighting ICD-10 code and keyword into pdf\n",
        "pdf_output_file, txt_output_file = highlight_icd_code_and_keyword(page_code10_dict, \n",
        "                                                                  icd9_code_dict=None, \n",
        "                                                                  icd_keywords_dict=icd_keywords_dict,\n",
        "                                                                  pdf_file_name=\"APS_24680000R_final.pdf\", \n",
        "                                                                  cords_file_name=\"APS_24680000R_final_cords.txt\")\n",
        "print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code and keyword\")\n",
        "print(f\"Highlighted coordinates are saved into [{txt_output_file}] file.\")"
      ],
      "metadata": {
        "id": "IsgIgN1hB528",
        "outputId": "b67665a3-73ff-44be-c2d8-228a267d3dd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File[APS_24680000R_final_output.pdf] is saved after highlighting ICD-10 code and keyword\n",
            "Highlighted coordinates are saved into [APS_24680000R_final_cords.txt] file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multiple Searching & Highlighting"
      ],
      "metadata": {
        "id": "TycHgKTZJ6a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-0: Load prerequisite instance\n",
        "# create nlp instance\n",
        "nlp_keyword = spacy.load('en_core_web_sm')\n",
        "\n",
        "# loading and updating patterns for ICD-10 code\n",
        "nlp_code10 = English()\n",
        "nlp_code10.add_pipe(\"entity_ruler\").from_disk(\"./icd10_code_patterns-v3.jsonl\")"
      ],
      "metadata": {
        "id": "TWMPYgIJJ8xG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f5a254-d32d-4fb4-fb18-2149cd6c5f14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.entityruler.EntityRuler at 0x7f93095124b0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for pdf_file in os.listdir(ocr_pdf_files_path):\n",
        "  pdf_file_name = f\"{ocr_pdf_files_path}/{pdf_file}\"\n",
        "  cords_file_name = f\"{pdf_file_name.split('.')[0]}_cords.txt\"\n",
        "\n",
        "  # Step-1: splitting pdf file\n",
        "  pdf_list = split_pdf(pdf_file_name)\n",
        "\n",
        "  # Step-2: Extracting text from pdf\n",
        "  txt_list = extract_text_from_pdf(pdf_list)\n",
        "\n",
        "  # Step-3: Searching ICD-10 code\n",
        "  icd10_code_dict = search_icd_code(txt_list, nlp_code10, code_type=\"ICD-10\")\n",
        "\n",
        "  # Step-4: Get coloset match of ICD-10 keyword\n",
        "  wrong_keyword_dict = get_wrong_keyword_dict(txt_list)\n",
        "\n",
        "  # Step-5: Extract sentence of ICD-10 keyword\n",
        "  icd_keywords_dict = extract_sentence(wrong_keyword_dict, txt_list)\n",
        "\n",
        "  # Step-6: Highlighting ICD-10 code and keyword into pdf\n",
        "  pdf_output_file, txt_output_file = highlight_icd_code_and_keyword(icd10_code_dict, \n",
        "                                                                    icd9_code_dict=None, \n",
        "                                                                    icd_keywords_dict=icd_keywords_dict,\n",
        "                                                                    pdf_file_name=pdf_file_name, \n",
        "                                                                    cords_file_name=cords_file_name)\n",
        "  print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code and keyword\")\n",
        "  print(f\"Highlighted coordinates are saved into [{txt_output_file}] file.\")\n",
        "\n",
        "  # remove all pdf and text files\n",
        "  purge(\"pdf-files/*.pdf\")\n",
        "  purge(\"txt-files/*.txt\")\n",
        "  pdf_list = []\n",
        "  txt_list = []"
      ],
      "metadata": {
        "id": "AA7AygDDy06C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ocr-pdf-files ocr-pdf-files2"
      ],
      "metadata": {
        "id": "mdUVrpzCOsOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ocr-pdf-files\n",
        "!mkdir -p ocr-pdf-files"
      ],
      "metadata": {
        "id": "kbTuptfC1xrl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r ocr-pdf-files2/*.pdf ocr-pdf-files/"
      ],
      "metadata": {
        "id": "Cfh3F3DmOzvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ocr-pdf-files2"
      ],
      "metadata": {
        "id": "gTuKanyTTUjn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "purge(\"ocr-pdf-files/*.txt\")\n",
        "purge(\"ocr-pdf-files/*_output.pdf\")\n",
        "purge(\"pdf-files/*.pdf\")\n",
        "purge(\"txt-files/*.txt\")"
      ],
      "metadata": {
        "id": "l06JNkJQ1yGT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip output.zip ocr-pdf-files/*_cords.txt ocr-pdf-files/*_output.pdf"
      ],
      "metadata": {
        "id": "rZI3lkYT1zxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}