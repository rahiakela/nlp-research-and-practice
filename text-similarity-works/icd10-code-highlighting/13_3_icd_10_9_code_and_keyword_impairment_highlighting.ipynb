{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0hK4nU35JVaA",
        "N4dDTs-rJL_h",
        "TycHgKTZJ6a6"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOStfZfUB9R9t2E7TnGg578",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/nlp-research-and-practice/blob/main/text-similarity-works/icd10-code-highlighting/13_3_icd_10_9_code_and_keyword_impairment_highlighting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "u6vazPC0Ja4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "!pip install pillow\n",
        "\n",
        "#!sudo apt install build-essential libpoppler-cpp-dev pkg-config python3-dev\n",
        "!pip install more-itertools\n",
        "!pip install PyPDF2\n",
        "#!pip install fitz\n",
        "!pip install pymupdf==1.22.0\n",
        "!pip install fuzzywuzzy\n",
        "#!pip install pikepdf"
      ],
      "metadata": {
        "id": "gdwgVvXuJdz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "print(fitz.__doc__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQxK2S849bPW",
        "outputId": "9182cee8-61fc-4dbf-aac2-78faeec16884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PyMuPDF 1.22.0: Python bindings for the MuPDF 1.22.0 library.\n",
            "Version date: 2023-04-14 00:00:01.\n",
            "Built for Python 3.10 on linux (64-bit).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pikepdf\n",
        "#!apt install ocrmypdf"
      ],
      "metadata": {
        "id": "YBXapnZzigSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "X7HN48uMv2O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "4QPcnYT5AKpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0264872-fc32-4577-9d8a-775c6984adaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "YyREdYtyqNMK",
        "outputId": "374341c2-05e0-4d88-f328-04197546e9e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/csv_files/\n",
            "  inflating: data/csv_files/icd_10_code_and_keywords_v2.csv  \n",
            "  inflating: data/csv_files/icd_10_code_and_keywords_v3.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v1.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v2.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v3.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v4.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v5.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v6.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v7.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v8.csv  \n",
            "  inflating: data/csv_files/synid_and_keywords_impairment_v9.csv  \n",
            "   creating: data/pattern_files/\n",
            "  inflating: data/pattern_files/date_regex-v1.json  \n",
            "  inflating: data/pattern_files/generic_matcher_config-v0.json  \n",
            "  inflating: data/pattern_files/icd10_code_patterns-v6.jsonl  \n",
            "   creating: data/pkl_files/\n",
            "  inflating: data/pkl_files/dict_words.pkl  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v0.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v1.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v2.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v3.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v4.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v5.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v6.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v7.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v8.pickle  \n",
            "  inflating: data/pkl_files/knocaps_dict_index_v9.pickle  \n",
            "   creating: data/txt_files/\n",
            "  inflating: data/txt_files/kcaps_v1.txt  \n",
            "  inflating: data/txt_files/kcaps_v2.txt  \n",
            "  inflating: data/txt_files/kcaps_v3.txt  \n",
            "  inflating: data/txt_files/kcaps_v4.txt  \n",
            "  inflating: data/txt_files/kcaps_v5.txt  \n",
            "  inflating: data/txt_files/kcaps_v6.txt  \n",
            "  inflating: data/txt_files/kcaps_v7.txt  \n",
            "  inflating: data/txt_files/kcaps_v8.txt  \n",
            "  inflating: data/txt_files/kcaps_v9.txt  \n",
            "  inflating: data/txt_files/knocaps_v0.txt  \n",
            "  inflating: data/txt_files/knocaps_v1.txt  \n",
            "  inflating: data/txt_files/knocaps_v2.txt  \n",
            "  inflating: data/txt_files/knocaps_v3.txt  \n",
            "  inflating: data/txt_files/knocaps_v4.txt  \n",
            "  inflating: data/txt_files/knocaps_v5.txt  \n",
            "  inflating: data/txt_files/knocaps_v6.txt  \n",
            "  inflating: data/txt_files/knocaps_v7.txt  \n",
            "  inflating: data/txt_files/knocaps_v8.txt  \n",
            "  inflating: data/txt_files/knocaps_v9.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import"
      ],
      "metadata": {
        "id": "L0vdWKmkdeL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import more_itertools\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import fitz\n",
        "\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from array import *\n",
        "\n",
        "from concurrent import futures\n",
        "from keyword_matcher import KeywordMatcher\n",
        "from sentence_extractor import SentenceExtractor\n",
        "from generic_matcher import GenericMatcher\n",
        "import config as cfg\n",
        "from utils import *"
      ],
      "metadata": {
        "id": "gKbd7WsfyYX3",
        "outputId": "6e875656-02e4-4113-9f3f-e0976aa096d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Core Classes"
      ],
      "metadata": {
        "id": "9oNqjgIsRpNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(238/255, 210/255, 2/255)"
      ],
      "metadata": {
        "id": "aQJui1vVhbad",
        "outputId": "acf89b7b-fd9c-431d-bf7e-fab655898f61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9333333333333333, 0.8235294117647058, 0.00784313725490196)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Highlighter:\n",
        "    def __init__(self, match_threshold=30):\n",
        "        # loading and updating patterns for ICD-10 code\n",
        "        self.nlp_code10 = English()\n",
        "        self.nlp_code10.add_pipe(\"entity_ruler\").from_disk(cfg.pattern_files[\"jsonl\"])\n",
        "\n",
        "        self.match_threshold = match_threshold\n",
        "        self.text_list = None\n",
        "\n",
        "        # define icd-10 code dataset\n",
        "        core_df = pd.read_csv(cfg.csv_files[\"CODE_CSV\"])\n",
        "        self.code_df = core_df.where(pd.notnull(core_df), None)\n",
        "        self.synid_df = pd.read_csv(cfg.csv_files[\"IMP_CSV\"])\n",
        "\n",
        "        # define required directory path\n",
        "        self.PDF_FILES_PATH = cfg.file_path[\"TMP_PDF_FILES_PATH\"]\n",
        "        self.TXT_FILES_PATH = cfg.file_path[\"TMP_TXT_FILES_PATH\"]\n",
        "        self.OUTPUT_FILES_PATH = cfg.file_path[\"OUTPUT_PATH\"]\n",
        "\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def set_impairment_keyword_dict(self, imp_keyword_dict):\n",
        "        self.impairment_keyword_dict = imp_keyword_dict\n",
        "\n",
        "    def set_page_matched_date_dict(self, matched_date_dict):\n",
        "        self.page_matched_date_dict = matched_date_dict\n",
        "\n",
        "    def set_match_threshold(self, match_val):\n",
        "        self.match_threshold = match_val\n",
        "\n",
        "    def set_text_list(self, p_text_list):\n",
        "        self.text_list = p_text_list\n",
        "\n",
        "    def highlight_all(self, icd10_code_dict, pdf_file_name=None, highlight_all=False):\n",
        "        pdf_file = fitz.open(pdf_file_name)\n",
        "        json_data = []\n",
        "        json_imp_data = []\n",
        "        json_date_data = []\n",
        "\n",
        "        for page_num, page in enumerate(pdf_file):\n",
        "            already_done_page_list = []\n",
        "            self.code_y2_coords_list = []\n",
        "            self.already_found_word_list = []\n",
        "            self.highlighted_coord_list = []\n",
        "            code_highlight_dict = {}\n",
        "\n",
        "            # highlight ICD-10 code\n",
        "            if page_num in icd10_code_dict:\n",
        "                num_page = page_num + 1\n",
        "\n",
        "                # let's store code y2 coords\n",
        "                for code in icd10_code_dict[page_num]:\n",
        "                    highlight_list = page.search_for(code)\n",
        "                    #print(f\"Page-{num_page}-{code}, Coordinate: {highlight_list}\")\n",
        "                    if len(highlight_list) > 0:\n",
        "                        code_highlight_dict[code] = highlight_list\n",
        "                        self.code_y2_coords_list.extend([highlight[3] for highlight in highlight_list])\n",
        "                    else:\n",
        "                        for alt_code in get_alternate_code_pattern(code):\n",
        "                            highlight_list = page.search_for(alt_code)\n",
        "                            for highlight in highlight_list:\n",
        "                                if len(highlight) > 0:\n",
        "                                    code_highlight_dict[code] = [highlight]\n",
        "                                    # print(f\"alternate-code:{code}, Coordinate: {highlight}\")\n",
        "                                    self.code_y2_coords_list.extend([highlight[3]])\n",
        "                                    # print(f\"[highlight[3]]: {[highlight[3]]}\")\n",
        "\n",
        "                filtered_code_highlight_dict = filter_duplicate_coordinate(code_highlight_dict)\n",
        "                # now, let highlight every code and its common words\n",
        "                for code, highlight_list in filtered_code_highlight_dict.items():\n",
        "                    prev_highlight = fitz.Rect(0, 0, 0, 0)\n",
        "                    for highlight in highlight_list:\n",
        "                        keyword = get_keyword(code, self.code_df)\n",
        "                        # get match score and common words coordinate\n",
        "                        if not highlight_all:\n",
        "                          all_match_keyword_list, all_match_imp_keyword_list = self.get_best_token_match(page_num,\n",
        "                                                                                                        code,\n",
        "                                                                                                        page,\n",
        "                                                                                                        highlight[3],\n",
        "                                                                                                        self.match_threshold)\n",
        "                        else:\n",
        "                          all_match_keyword_list, all_match_imp_keyword_list = self.get_best_all_token_match(page_num,\n",
        "                                                                                                        code,\n",
        "                                                                                                        page,\n",
        "                                                                                                        highlight[3],\n",
        "                                                                                                        self.match_threshold)\n",
        "\n",
        "                        # Step-1: highlight and set color coding dont have common words\n",
        "                        if not all_match_keyword_list:\n",
        "                            highlight_code(page, highlight)\n",
        "                            # prepare json object\n",
        "                            json_data.append(prepare_json_object(num_page, code, self.code_df, highlight, keyword))\n",
        "\n",
        "                        # Step-2: highlight and set color coding that have common words\n",
        "                        for match_keyword_dict in all_match_keyword_list:\n",
        "                            # highlight ICD-10 code\n",
        "                            if not is_coord_equal(prev_highlight, highlight):\n",
        "                              highlight_code(page, highlight)\n",
        "                              prev_highlight = highlight\n",
        "                            # highlight common words\n",
        "                            highlight_coords_dict = {}\n",
        "                            highlight_coords_dict, highlighted_word_list = self.highlight_common_words(page, match_keyword_dict[\"common_words_coords\"])\n",
        "                            # prepare json object\n",
        "                            json_data.append(prepare_json_object(num_page, code, self.code_df, highlight, keyword, match_keyword_dict, highlight_coords_dict))\n",
        "\n",
        "                        # Step-3: highlight and set color coding for impairment keyword common words\n",
        "                        json_imp_data_list = []\n",
        "                        for match_imp_keyword_dict in all_match_imp_keyword_list:\n",
        "                            json_imp_data_object = {}\n",
        "                            # highlight common words\n",
        "                            highlight_coords_dict = {}\n",
        "                            if \"imp_common_keyword_coords\" in match_imp_keyword_dict:\n",
        "                              if match_imp_keyword_dict[\"imp_common_keyword_coords\"]:\n",
        "                                highlight_coords_list = self.highlight_impairment(page, num_page, match_imp_keyword_dict[\"imp_common_keyword_coords\"])\n",
        "                                already_done_page_list.append(page_num)\n",
        "                                # prepare json object\n",
        "                                if highlight_coords_list:\n",
        "                                  json_imp_data_list.extend(highlight_coords_list)\n",
        "                        if len(json_imp_data_list) > 0:\n",
        "                            json_imp_data.extend(json_imp_data_list)\n",
        "\n",
        "            # highlight impairment key phrase\n",
        "            if page_num in self.impairment_keyword_dict and page_num not in already_done_page_list:\n",
        "              json_imp_data_list = self.highlight_keyword_impairment_data(page, page_num)\n",
        "              if len(json_imp_data_list) > 0:\n",
        "                json_imp_data.extend(json_imp_data_list)\n",
        "\n",
        "            # highlight date and time in PDF\n",
        "            if page_num in self.page_matched_date_dict:\n",
        "                json_date_data_list = self.highlight_date_and_time(page, page_num)\n",
        "                if len(json_date_data_list) > 0:\n",
        "                  json_date_data.extend(json_date_data_list)\n",
        "\n",
        "        # save PDF and JSON output file\n",
        "        pdf_output_file_name, json_data_dump_file = save_output_file(pdf_object=pdf_file, cfg=cfg,\n",
        "                                                                     output_type=2 if highlight_all else 1,\n",
        "                                                                     output_path=self.OUTPUT_FILES_PATH,\n",
        "                                                                     output_file_name=pdf_file_name,\n",
        "                                                                     json_data=json_data,\n",
        "                                                                     json_imp_data=json_imp_data,\n",
        "                                                                     json_date_data=json_date_data)\n",
        "        return pdf_output_file_name, json_data_dump_file\n",
        "\n",
        "    def highlight_icd_code_and_common_words(self, icd10_code_dict, pdf_file_name=None):\n",
        "        pdf_file = fitz.open(pdf_file_name)\n",
        "        json_data = []\n",
        "\n",
        "        for page_num, page in enumerate(pdf_file):\n",
        "            self.code_y2_coords_list = []\n",
        "            self.already_found_word_list = []\n",
        "            code_highlight_dict = {}\n",
        "\n",
        "            # highlight ICD-10 code\n",
        "            if page_num in icd10_code_dict:\n",
        "                num_page = page_num + 1\n",
        "                # let's store code y2 coords\n",
        "                for code in icd10_code_dict[page_num]:\n",
        "                    highlight_list = page.search_for(code)\n",
        "                    #print(f\"Page-{num_page}-{code}, Coordinate: {highlight_list}\")\n",
        "                    if len(highlight_list) > 0:\n",
        "                        code_highlight_dict[code] = highlight_list\n",
        "                        self.code_y2_coords_list.extend([highlight[3] for highlight in highlight_list])\n",
        "                    else:\n",
        "                        for alt_code in get_alternate_code_pattern(code):\n",
        "                            highlight_list = page.search_for(alt_code)\n",
        "                            for highlight in highlight_list:\n",
        "                                if len(highlight) > 0:\n",
        "                                    code_highlight_dict[code] = [highlight]\n",
        "                                    #print(f\"alternate-code:{code}, Coordinate: {highlight}\")\n",
        "                                    self.code_y2_coords_list.extend([highlight[3]])\n",
        "\n",
        "                filtered_code_highlight_dict = filter_duplicate_coordinate(code_highlight_dict)\n",
        "                # now, let highlight every code and its common words\n",
        "                for code, highlight_list in filtered_code_highlight_dict.items():\n",
        "                    prev_highlight = fitz.Rect(0, 0, 0, 0)\n",
        "                    for highlight in highlight_list:\n",
        "                        keyword = get_keyword(code, self.code_df)\n",
        "                        # get match score and common words coordinate\n",
        "                        all_match_keyword_list, all_match_imp_keyword_list = self.get_best_token_match(page_num, code,\n",
        "                                                                                                       page,\n",
        "                                                                                                       highlight[3],\n",
        "                                                                                                       self.match_threshold)\n",
        "\n",
        "                        # Step-1: highlight and set color coding dont have common words\n",
        "                        if not all_match_keyword_list:\n",
        "                            page_highlight = page.add_highlight_annot(highlight)\n",
        "                            page_highlight.set_colors(stroke=[0.92, 0.59, 0.48])  # dark salmon\n",
        "                            page_highlight.update()\n",
        "                            # prepare json object\n",
        "                            json_data.append(prepare_json_object(num_page, code, self.code_df, highlight, keyword))\n",
        "\n",
        "                        # Step-2: highlight and set color coding that have common words\n",
        "                        for match_keyword_dict in all_match_keyword_list:\n",
        "                            # highlight ICD-10 code\n",
        "                            if not self.is_coord_equal(prev_highlight, highlight):\n",
        "                              highlight_code(page, highlight)\n",
        "                              prev_highlight = highlight\n",
        "                            # highlight common words\n",
        "                            highlight_coords_dict = {}\n",
        "                            highlight_coords_dict, highlighted_word_list = self.highlight_common_words(page, match_keyword_dict[\"common_words_coords\"])\n",
        "\n",
        "                            # prepare json object\n",
        "                            json_data.append(prepare_json_object(num_page, code, self.code_df, highlight, keyword, match_keyword_dict, highlight_coords_dict))\n",
        "\n",
        "        # save PDF and JSON output file\n",
        "        pdf_output_file_name, json_data_dump_file = save_output_file(pdf_file, cfg, 3, self.OUTPUT_FILES_PATH, pdf_file_name, json_data)\n",
        "        return pdf_output_file_name, json_data_dump_file\n",
        "\n",
        "    def highlight_icd_code_and_keyword_impairment(self, icd10_code_dict, pdf_file_name=None):\n",
        "      pdf_file = fitz.open(pdf_file_name)\n",
        "      json_data = []\n",
        "      json_imp_data = []\n",
        "      json_date_data = []\n",
        "\n",
        "      for page_num, page in enumerate(pdf_file):\n",
        "        already_done_page_list = []\n",
        "        self.code_y2_coords_list = []\n",
        "        self.already_found_word_list = []\n",
        "        self.highlighted_coord_list = []\n",
        "        self.highlighted_term_list = []\n",
        "        code_highlight_dict = {}\n",
        "\n",
        "        # highlight ICD-10 code\n",
        "        if page_num in icd10_code_dict:\n",
        "            num_page = page_num + 1\n",
        "\n",
        "            # let's store code y2 coords\n",
        "            for code in icd10_code_dict[page_num]:\n",
        "                highlight_list = page.search_for(code)\n",
        "                # print(f\"Page-{num_page}-{code}, Coordinate: {highlight_list}\")\n",
        "                if len(highlight_list) > 0:\n",
        "                    code_highlight_dict[code] = highlight_list\n",
        "                    self.code_y2_coords_list.extend([highlight[3] for highlight in highlight_list])\n",
        "                else:\n",
        "                    for alt_code in get_alternate_code_pattern(code):\n",
        "                        highlight_list = page.search_for(alt_code)\n",
        "                        for highlight in highlight_list:\n",
        "                            if len(highlight) > 0:\n",
        "                                code_highlight_dict[code] = [highlight]\n",
        "                                # print(f\"alternate-code:{code}, Coordinate: {highlight}\")\n",
        "                                self.code_y2_coords_list.extend([highlight[3]])\n",
        "                                # print(f\"[highlight[3]]: {[highlight[3]]}\")\n",
        "            # print(f\"code_highlight_dict: {code_highlight_dict}\")\n",
        "            filtered_code_highlight_dict = filter_duplicate_coordinate(code_highlight_dict)\n",
        "            # now, let highlight every code and its common words\n",
        "            for code, highlight_list in filtered_code_highlight_dict.items():\n",
        "                # print(f\"code-{code}, Coordinate: {highlight_list}\")\n",
        "                prev_highlight = fitz.Rect(0, 0, 0, 0)\n",
        "                for highlight in highlight_list:\n",
        "                    # Step-1: highlight ICD-10 code\n",
        "                    #print(f\"prev_highlight: {prev_highlight}\")\n",
        "                    if not is_coord_equal(prev_highlight, highlight):\n",
        "                      #print(f\"highlight: {highlight}\")\n",
        "                      highlight_code(page, highlight)\n",
        "                      prev_highlight = highlight\n",
        "                    # prepare json object\n",
        "                    json_data.append(prepare_code_json_object(num_page, code, highlight))\n",
        "\n",
        "        # highlight ICD key phrase\n",
        "        # if page_num in self.impairment_keyword_dict and page_num not in already_done_page_list:\n",
        "        if page_num in self.impairment_keyword_dict:\n",
        "          json_imp_data_list = self.highlight_keyword_impairment_data(page, page_num)\n",
        "          if len(json_imp_data_list) > 0:\n",
        "            json_imp_data.extend(json_imp_data_list)\n",
        "\n",
        "        # highlight date and time in PDF\n",
        "        if page_num in self.page_matched_date_dict:\n",
        "          json_date_data_list = self.highlight_date_and_time(page, page_num)\n",
        "          if len(json_date_data_list) > 0:\n",
        "            json_date_data.extend(json_date_data_list)\n",
        "\n",
        "      # save PDF and JSON output file\n",
        "      pdf_output_file_name, json_data_dump_file = save_output_file(pdf_object=pdf_file, cfg=cfg,\n",
        "                                                                   output_type=4,\n",
        "                                                                   output_path=self.OUTPUT_FILES_PATH,\n",
        "                                                                   output_file_name=pdf_file_name,\n",
        "                                                                   json_data=json_data,\n",
        "                                                                   json_imp_data=json_imp_data,\n",
        "                                                                   json_date_data=json_date_data)\n",
        "      return pdf_output_file_name, json_data_dump_file\n",
        "\n",
        "    def highlight_keyword_impairment(self, pdf_file_name=None):\n",
        "        pdf_file = fitz.open(pdf_file_name)\n",
        "        json_imp_data = []\n",
        "        for page_num, page in enumerate(pdf_file):\n",
        "          self.highlighted_coord_list = []\n",
        "          # highlight ICD key phrase\n",
        "          if page_num in self.impairment_keyword_dict:\n",
        "            json_imp_data_list = []\n",
        "            num_page = page_num + 1\n",
        "            imp_keywords_coord_dict = {}\n",
        "            key_phrase_sents = self.impairment_keyword_dict[page_num]\n",
        "            for key_phrase_sent in key_phrase_sents:\n",
        "              #print(f\"key_phrase_sent: {key_phrase_sent}\")\n",
        "              coordinates = page.search_for(key_phrase_sent)\n",
        "              imp_keywords_coord_dict[key_phrase_sent] = coordinates\n",
        "            highlight_coords_list = self.highlight_impairment(page, num_page, imp_keywords_coord_dict)\n",
        "            if len(highlight_coords_list) > 0:\n",
        "              json_imp_data_list.extend(highlight_coords_list)\n",
        "            if len(json_imp_data_list) > 0:\n",
        "              json_imp_data.extend(json_imp_data_list)\n",
        "\n",
        "        # save PDF and JSON output file\n",
        "        pdf_output_file_name, json_data_dump_file = save_output_file(pdf_file, cfg, 5, self.OUTPUT_FILES_PATH, pdf_file_name, json_imp_data=json_imp_data)\n",
        "        return pdf_output_file_name, json_data_dump_file\n",
        "\n",
        "    def highlight_keyword_impairment_data(self, p_page, page_num):\n",
        "      json_imp_data_list = []\n",
        "      num_page = page_num + 1\n",
        "      # print(f\"num_page: {num_page}\")\n",
        "      imp_keywords_coord_dict = {}\n",
        "\n",
        "      def highlight_multi_key_phrase(p_keyword_impairment, batch_coord):\n",
        "        phrase_coords = []\n",
        "        filter_coords = []\n",
        "        try:\n",
        "            f_coord = batch_coord[0]\n",
        "            l_coord = batch_coord[-1]\n",
        "            # first coord y1,y2 and second coord y1,y2 must be equal if it is same phrase\n",
        "            if f_coord[1] == l_coord[1] and f_coord[3] == l_coord[3]:\n",
        "              # check first coord x2 and second coord x1 is more than 100(means the same line phrase occurrence)\n",
        "              if (l_coord[0] - f_coord[2]) > 100:\n",
        "                  phrase_coords.extend([\n",
        "                      (f_coord[0], f_coord[1], f_coord[2], f_coord[3]),\n",
        "                      (l_coord[0], l_coord[1], l_coord[2], l_coord[3])\n",
        "                  ])\n",
        "              else:\n",
        "                phrase_coords.extend([(f_coord[0], f_coord[1], l_coord[2], l_coord[3])])\n",
        "            for phrase_coord in phrase_coords:\n",
        "                # print(f\"phrase_coord: {phrase_coord}\")\n",
        "                if is_exact_match(p_page, p_keyword_impairment, phrase_coord, full_match=True, case_sensitive=True) \\\n",
        "                  and not is_coord_already_highlighted(self.highlighted_coord_list, phrase_coord):\n",
        "                  p_highlight = p_page.add_highlight_annot(phrase_coord)\n",
        "                  p_highlight.update()\n",
        "                  filter_coords.append(phrase_coord)\n",
        "                  self.highlighted_coord_list.append(phrase_coord)\n",
        "                # print(f\"key_phrase: {p_keyword_impairment}, phrase_coord: {phrase_coord}\")\n",
        "        except ValueError as ve:\n",
        "            #print(f\"ERROR: coordinate not found>>{ve}\")\n",
        "            self.logger.error(f\"ERROR: coordinate not found>>{ve}\")\n",
        "        return filter_coords\n",
        "\n",
        "      def get_json_data(coords, k_impairment):\n",
        "        return {\n",
        "            \"coords\": coords,\n",
        "            \"synid\": get_synid(k_impairment, self.synid_df),\n",
        "            # \"keyword_field\": get_keyword_field(key_phrase_sent, synid_df)\n",
        "          }\n",
        "\n",
        "      keyword_impairments = self.impairment_keyword_dict[page_num]\n",
        "      for keyword_impairment in keyword_impairments:\n",
        "          # print(f\"key_phrase_sent: {keyword_impairment}\")\n",
        "          coordinates = p_page.search_for(keyword_impairment)\n",
        "          # print(f\"coordinates: {coordinates}\")\n",
        "\n",
        "          phrase_len = len(keyword_impairment.split())\n",
        "          if coordinates:\n",
        "            # highlight word phrase that have equal word phrase and coord length(single phrase occurrence)\n",
        "            if phrase_len == len(coordinates) and phrase_len != 1:\n",
        "                imp_keywords_coord_dict[keyword_impairment] = get_json_data(highlight_multi_key_phrase(keyword_impairment, coordinates), keyword_impairment)\n",
        "            # highlight word phrase that do not have equal word phrase and coord length(multiple phrase occurrence)\n",
        "            elif int(len(coordinates) / phrase_len) * phrase_len == len(coordinates):\n",
        "                phrase_coord_list = []\n",
        "                # batch coord list by dividing its length with phrase length\n",
        "                for batch_coord in more_itertools.batched(coordinates, phrase_len):\n",
        "                    phrase_coord_list.extend(highlight_multi_key_phrase(keyword_impairment, batch_coord))\n",
        "                imp_keywords_coord_dict[keyword_impairment] = get_json_data(phrase_coord_list, keyword_impairment)\n",
        "            else:\n",
        "                coords_list = []\n",
        "                for coord in coordinates:\n",
        "                    if is_exact_match(p_page, keyword_impairment, coord, full_match=True, case_sensitive=True) and not is_coord_already_highlighted(self.highlighted_coord_list, coord):\n",
        "                        page_highlight = p_page.add_highlight_annot(coord)\n",
        "                        page_highlight.update()\n",
        "                        coords_list.extend((coord[0], coord[1], coord[2], coord[3]))\n",
        "                        self.highlighted_coord_list.append(coord)\n",
        "                        # print(f\"key_phrase: {keyword_impairment}, keywords_coord: {coord}\")\n",
        "                # print(f\"coords_list: {coords_list}\")\n",
        "                imp_keywords_coord_dict[keyword_impairment] = get_json_data(coords_list, keyword_impairment)\n",
        "      # print(f\"imp_keywords_coord_dict: {imp_keywords_coord_dict}\")\n",
        "      if imp_keywords_coord_dict:\n",
        "        json_imp_data_list.extend(prepare_imp_json_object(num_page, imp_keywords_coord_dict))\n",
        "      return json_imp_data_list\n",
        "\n",
        "    def highlight_date_and_time(self, p_page, page_num):\n",
        "        num_page = page_num + 1\n",
        "        json_date_data_list = []\n",
        "        highlight_coords_list = []\n",
        "        page_date_coord_dict = {}\n",
        "\n",
        "        def highlight_date(p_coordinate, p_date):\n",
        "          page_highlight = p_page.add_highlight_annot(p_coordinate)\n",
        "          page_highlight.set_colors(stroke=[0.529, 0.807, 0.921])  # sky blue\n",
        "          page_highlight.update()\n",
        "          page_date_coord_dict[p_date] = [p_coordinate[0], p_coordinate[1], p_coordinate[2], p_coordinate[3]]\n",
        "          self.highlighted_term_list.append(p_date)\n",
        "\n",
        "        page_date_list = self.page_matched_date_dict[page_num]\n",
        "        for page_date in page_date_list:\n",
        "            coordinates = p_page.search_for(page_date)\n",
        "            for coordinate in coordinates:\n",
        "              if len(re.findall(r\"^\\d+$\", page_date.strip())) > 0:\n",
        "                if is_exact_year_match(p_page, page_date, coordinate, self.highlighted_term_list):\n",
        "                  highlight_date(coordinate, page_date)\n",
        "              else:\n",
        "                highlight_date(coordinate, page_date)\n",
        "\n",
        "        if page_date_coord_dict:\n",
        "            highlight_coords_list.append(prepare_page_date_json_object(num_page, page_date_coord_dict))\n",
        "        if len(highlight_coords_list) > 0:\n",
        "            json_date_data_list.extend(highlight_coords_list)\n",
        "        return json_date_data_list\n",
        "\n",
        "    def highlight_common_words(self, page_obj, common_words_coord_dict):\n",
        "        highlight_coords_dict = {}\n",
        "        highlighted_word_list = []\n",
        "\n",
        "        prev_common_word = \"\"\n",
        "        for common_word, common_word_coord in common_words_coord_dict.items():\n",
        "            highlight = page_obj.add_highlight_annot(common_word_coord)\n",
        "            highlight.update()\n",
        "            if prev_common_word.lower() != common_word.lower():\n",
        "                highlight_coords_dict[common_word] = common_word_coord\n",
        "                highlighted_word_list.append(common_word)\n",
        "                prev_common_word = common_word\n",
        "        return highlight_coords_dict, highlighted_word_list\n",
        "\n",
        "    def get_best_token_match(self, page_num, p_code, page_obj, code_y2_coord, match_threshold):\n",
        "        all_match_keyword_list = []\n",
        "        all_match_imp_keyword_list = []\n",
        "\n",
        "        # Step 1: reverse code pattern\n",
        "        reversed_icd_code = reverse_code_pattern(p_code)\n",
        "        # Step 2: fetch keyword based on code\n",
        "        keyword = get_keyword(reversed_icd_code, self.code_df)\n",
        "        # Step 3: get code paragraph\n",
        "        code_paragraph_list, non_code_paragraph_list = get_paragraph(page_obj, code_y2_coord)\n",
        "        # print(f\"code_paragraph_list: {non_code_paragraph_list}\")\n",
        "\n",
        "        # Step 4: prepare code_paragraph for common words\n",
        "        for code_paragraph in code_paragraph_list:\n",
        "            match_keyword_dict = {}\n",
        "            common_words = get_common_words(keyword, code_paragraph)\n",
        "            if len(common_words) > 0:\n",
        "                match_keyword_dict[\"common_words\"] = common_words\n",
        "                # Step 4: get best token match ratio\n",
        "                clean_paragraph = \" \".join(clean_text(code_paragraph))\n",
        "                match_keyword_dict[\"paragraph\"] = clean_paragraph\n",
        "                # match_score = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "                # if fuzz.token_set_ratio(keyword, clean_paragraph) >= match_threshold else 0\n",
        "                match_keyword_dict[\"score\"] = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "                # Step 5: build common words coordinate dict\n",
        "                common_words_coord_dict = {}\n",
        "                #print(f\"code_y2_coord: {code_y2_coord}\")\n",
        "                for common_word in common_words:\n",
        "                    highlight_list = page_obj.search_for(common_word)\n",
        "                    #print(f\"highlight_list: {highlight_list}\")\n",
        "                    found_coord = False\n",
        "                    for highlight in highlight_list:\n",
        "                        # get common word y2 coord value\n",
        "                        common_word_y2_coords = highlight[3]\n",
        "                        common_word_y1_coords = highlight[1]\n",
        "                        if (code_y2_coord - 2) <= common_word_y2_coords <= (code_y2_coord + 2):\n",
        "                          common_words_coord_dict[common_word] = highlight\n",
        "                          found_coord = True\n",
        "                        if not found_coord:\n",
        "                          if (code_y2_coord - 20) <= common_word_y2_coords <= (code_y2_coord + 20):\n",
        "                            common_words_coord_dict[common_word] = highlight\n",
        "                            # self.already_found_word_list.append(common_word)\n",
        "                match_keyword_dict[\"common_words_coords\"] = common_words_coord_dict\n",
        "                all_match_keyword_list.append(match_keyword_dict)\n",
        "\n",
        "        def get_keyword_impairment_list(p_paragraph):\n",
        "            keyword_impairments = []\n",
        "            for keyword_impairment in self.impairment_keyword_dict[page_num]:\n",
        "                keyword_impairment_found = get_common_words(keyword_impairment, p_paragraph)\n",
        "                if len(keyword_impairment_found) > 0:\n",
        "                    keyword_impairments.append(keyword_impairment)\n",
        "            return keyword_impairments\n",
        "\n",
        "        # Step 5: prepare non code_paragraph for common words\n",
        "        for non_code_paragraph in non_code_paragraph_list:\n",
        "            match_imp_keyword_dict = {}\n",
        "            imp_common_words_coord_dict = {}\n",
        "            keyword_impairment_list = get_keyword_impairment_list(non_code_paragraph)\n",
        "            for keyword_impairment in keyword_impairment_list:\n",
        "                highlight_list = page_obj.search_for(keyword_impairment)\n",
        "                highlight_list = [highlight for highlight in highlight_list if highlight[3] not in self.code_y2_coords_list]\n",
        "                if keyword_impairment not in self.already_found_word_list and len(highlight_list) > 0:\n",
        "                    self.already_found_word_list.append(keyword_impairment)\n",
        "                    imp_common_words_coord_dict[f\"{keyword_impairment}\"] = highlight_list\n",
        "                    continue\n",
        "            match_imp_keyword_dict[\"imp_common_keyword_coords\"] = imp_common_words_coord_dict\n",
        "            all_match_imp_keyword_list.append(match_imp_keyword_dict)\n",
        "\n",
        "        return all_match_keyword_list, all_match_imp_keyword_list\n",
        "\n",
        "    def get_best_all_token_match(self, page_num, p_code, page_obj, code_y2_coord, match_threshold):\n",
        "        all_match_keyword_list = []\n",
        "        all_match_imp_keyword_list = []\n",
        "\n",
        "        # Step 1: reverse code pattern\n",
        "        reversed_icd_code = reverse_code_pattern(p_code)\n",
        "        # Step 2: fetch keyword based on code\n",
        "        keyword = get_keyword(reversed_icd_code, self.code_df)\n",
        "        # Step 3: get all paragraph\n",
        "        all_paragraph_list = get_all_paragraph(page_obj)\n",
        "        # print(f\"code_paragraph_list: {non_code_paragraph_list}\")\n",
        "\n",
        "        def get_keyword_impairment_list(p_paragraph):\n",
        "          keyword_impairments = []\n",
        "          for keyword_impairment in self.impairment_keyword_dict[page_num]:\n",
        "              keyword_impairment_found = get_common_words(keyword_impairment, p_paragraph)\n",
        "              if len(keyword_impairment_found) > 0:\n",
        "                  keyword_impairments.append(keyword_impairment)\n",
        "          return keyword_impairments\n",
        "\n",
        "        # Step 4: prepare all_paragraph for common words and impairment\n",
        "        for all_paragraph in all_paragraph_list:\n",
        "          match_keyword_dict = {}\n",
        "          common_words = get_common_words(keyword, all_paragraph)\n",
        "          if len(common_words) > 0:\n",
        "              match_keyword_dict[\"common_words\"] = common_words\n",
        "              # Step 4: get best token match ratio\n",
        "              clean_paragraph = \" \".join(clean_text(all_paragraph))\n",
        "              match_keyword_dict[\"paragraph\"] = clean_paragraph\n",
        "              # match_score = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "              # if fuzz.token_set_ratio(keyword, clean_paragraph) >= match_threshold else 0\n",
        "              match_keyword_dict[\"score\"] = fuzz.token_set_ratio(keyword, clean_paragraph)\n",
        "              # Step 5: build common words coordinate dict\n",
        "              common_words_coord_dict = {}\n",
        "              for common_word in common_words:\n",
        "                  highlight_list = page_obj.search_for(common_word)\n",
        "                  found_coord = False\n",
        "                  for highlight in highlight_list:\n",
        "                      # get common word y2 coord value\n",
        "                      common_word_y2_coords = highlight[3]\n",
        "                      if (code_y2_coord - 2) <= common_word_y2_coords <= (code_y2_coord + 2):\n",
        "                        common_words_coord_dict[common_word] = highlight\n",
        "                        found_coord = True\n",
        "                      if not found_coord:\n",
        "                        if (code_y2_coord - 20) <= common_word_y2_coords <= (code_y2_coord + 20):\n",
        "                          common_words_coord_dict[common_word] = highlight\n",
        "                          # self.already_found_word_list.append(common_word)\n",
        "              match_keyword_dict[\"common_words_coords\"] = common_words_coord_dict\n",
        "              all_match_keyword_list.append(match_keyword_dict)\n",
        "          else:\n",
        "            # Step 5: prepare non code_paragraph for common words\n",
        "            match_imp_keyword_dict = {}\n",
        "            imp_common_words_coord_dict = {}\n",
        "            keyword_impairment_list = get_keyword_impairment_list(all_paragraph)\n",
        "            for keyword_impairment in keyword_impairment_list:\n",
        "              #print(f\"keyword_impairment11: {keyword_impairment}\")\n",
        "              highlight_list = page_obj.search_for(keyword_impairment)\n",
        "              if keyword_impairment not in self.already_found_word_list and len(highlight_list) > 0:\n",
        "                self.already_found_word_list.append(keyword_impairment)\n",
        "                imp_common_words_coord_dict[f\"{keyword_impairment}\"] = highlight_list\n",
        "                #print(f\"keyword_impairment22: {keyword_impairment}, coords: {highlight_list}\")\n",
        "                #continue\n",
        "            match_imp_keyword_dict[\"imp_common_keyword_coords\"] = imp_common_words_coord_dict\n",
        "            all_match_imp_keyword_list.append(match_imp_keyword_dict)\n",
        "\n",
        "        return all_match_keyword_list, all_match_imp_keyword_list\n",
        "\n",
        "    def search_icd_code(self, txt_list):\n",
        "      pdf_page_vocab = {}\n",
        "      for txt_file in txt_list:\n",
        "        with open(txt_file, \"r\") as f:\n",
        "          page_txt = f.read()\n",
        "          # check the page that have line number instead of code\n",
        "          index_page = False\n",
        "          if re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "            index_page = True\n",
        "\n",
        "          doc = self.nlp_code10(page_txt)\n",
        "          code_list = []\n",
        "          if index_page:\n",
        "            # check the code contain letter \"L\"\n",
        "            code_list = [ent.text for ent in doc.ents if not re.search(\"(L[0-9]+)\", ent.text)]\n",
        "          else:\n",
        "            code_list = [ent.text for ent in doc.ents]\n",
        "\n",
        "          if len(code_list) != 0:\n",
        "            # page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "            page_number = int(txt_file.split(\"/\")[-1].split(\".\")[0].split(\"-\")[1])\n",
        "            pdf_page_vocab[page_number] = list(set(code_list))\n",
        "            # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "      return pdf_page_vocab"
      ],
      "metadata": {
        "id": "jRYlndgDUtds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PDF Highlighting"
      ],
      "metadata": {
        "id": "7s7ZxYMUBAol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /home/ocreng\n",
        "!mkdir -p /home/ocreng/ocrhigh\n",
        "!mkdir -p /home/ocreng/ocrhigh/input\n",
        "!mkdir -p /home/ocreng/ocrhigh/output\n",
        "!mkdir -p /home/ocreng/ocrhigh/processed\n",
        "!mkdir -p /home/ocreng/ocrhigh/pdf-files\n",
        "!mkdir -p /home/ocreng/ocrhigh/txt-files"
      ],
      "metadata": {
        "id": "ZWvB_40a8SNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def purge(file_path):\n",
        "  for f in glob.glob(file_path):\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "xQe5Xfp9AWLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Step-0: create highlighter instance\n",
        "INPUT_PDF_FILES_PATH = cfg.file_path[\"INPUT_PATH\"]\n",
        "\n",
        "highlighter = Highlighter(match_threshold=35)\n",
        "sent_extractor = SentenceExtractor()\n",
        "generic_matcher = GenericMatcher()"
      ],
      "metadata": {
        "id": "MkoXzNA1AWu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b81e0b6-4bbd-4c4a-a0a6-ef8b19f329f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 13s, sys: 5.16 s, total: 1min 18s\n",
            "Wall time: 1min 24s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```log\n",
        "CPU times: user 1min 7s, sys: 6.54 s, total: 1min 14s\n",
        "Wall time: 1min 15s\n",
        "```"
      ],
      "metadata": {
        "id": "ftvHz73FCCke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test"
      ],
      "metadata": {
        "id": "S-EA40XdAcCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /home/ocreng/ocrhigh/input\n",
        "!mkdir -p /home/ocreng/ocrhigh/input"
      ],
      "metadata": {
        "id": "wfJwndx9vcTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp Practice_Copy_Test_NEWOCR.pdf /home/ocreng/ocrhigh/input\n",
        "#!cp page-10.pdf /home/ocreng/ocrhigh/input"
      ],
      "metadata": {
        "id": "j6YcCHZ_9OMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /home/ocreng/ocrhigh/output\n",
        "!mkdir -p /home/ocreng/ocrhigh/output\n",
        "!rm -rf output*.zip"
      ],
      "metadata": {
        "id": "3ArIraAD0zx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_type = 4"
      ],
      "metadata": {
        "id": "uyQTNjI_gXJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "txt_list = None\n",
        "icd10_code_dict1 = None\n",
        "wrong_keyword_dict = None\n",
        "for pdf_file in os.listdir(INPUT_PDF_FILES_PATH):\n",
        "  pdf_file_name = f\"{INPUT_PDF_FILES_PATH}/{pdf_file}\"\n",
        "\n",
        "  # Step-1: splitting pdf file\n",
        "  print(\"Step-1: Splitting pdf file............\")\n",
        "  pdf_list = split_pdf(highlighter.PDF_FILES_PATH, pdf_file_name)\n",
        "\n",
        "  # Step-2: Extracting text from pdf\n",
        "  print(\"Step-2: Extracting text from pdf............\")\n",
        "  txt_list = extract_text_from_pdf(highlighter.PDF_FILES_PATH, highlighter.TXT_FILES_PATH, pdf_list)\n",
        "  highlighter.set_text_list(txt_list)\n",
        "\n",
        "  # Step-3: Searching ICD-10 code\n",
        "  print(\"Step-3: Searching ICD-10 code into text file..........\")\n",
        "  icd10_code_dict = highlighter.search_icd_code(txt_list)\n",
        "  icd10_code_dict1 = icd10_code_dict\n",
        "\n",
        "  # Step-4: Get closet match of keyword impairment\n",
        "  print(\"Step-4: Get closet match of keyword impairment..........\")\n",
        "  matched_keyword_dict = sent_extractor.get_matched_keyword_dict(txt_list)\n",
        "  highlighter.set_impairment_keyword_dict(matched_keyword_dict)\n",
        "  wrong_keyword_dict = matched_keyword_dict\n",
        "\n",
        "  # Step-5: Get date time list from text file\n",
        "  print(\"Step-5: Get date time list from text file..........\")\n",
        "  matched_date_dict = generic_matcher.get_match_date_dict(txt_list)\n",
        "  highlighter.set_page_matched_date_dict(matched_date_dict)\n",
        "\n",
        "  if output_type == 1:\n",
        "    # Step-6: Highlighting ICD-10 code, its description and keyword impairment into PDF file\n",
        "    print(\"Step-6: Highlighting ICD-10 code, its description and keyword impairment into PDF file............\")\n",
        "    pdf_output_file, json_code_output_file = highlighter.highlight_all(icd10_code_dict, pdf_file_name=pdf_file_name)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "    print(f\"Highlighted code and impairment coordinates are saved into [{json_code_output_file}] file.\")\n",
        "  elif output_type == 2:\n",
        "    # Step-7: Highlighting ICD-10 code, its description and all keyword impairment into pdf\n",
        "    print(\"Step-7: Highlighting ICD-10 code, its description and all keyword impairment into pdf............\")\n",
        "    pdf_output_file, json_code_output_file = highlighter.highlight_all(icd10_code_dict, pdf_file_name=pdf_file_name, highlight_all=True)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "    print(f\"Highlighted code and impairment coordinates are saved into [{json_code_output_file}] file.\")\n",
        "  elif output_type == 3:\n",
        "    # Step-8: Highlighting ICD-10 code and its description into pdf\n",
        "    print(\"Step-8: Highlighting ICD-10 code and its description into pdf............\")\n",
        "    pdf_output_file, json_output_file = highlighter.highlight_icd_code_and_common_words(icd10_code_dict, pdf_file_name=pdf_file_name)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code\")\n",
        "    print(f\"Highlighted coordinates are saved into [{json_output_file}] file.\")\n",
        "  elif output_type == 4:\n",
        "    # Step-9: Highlighting ICD-10 code and all keyword impairment into pdf\n",
        "    print(\"Step-9: Highlighting ICD-10 code and all keyword impairment into pdf............\")\n",
        "    pdf_output_file, json_code_output_file = highlighter.highlight_icd_code_and_keyword_impairment(icd10_code_dict, pdf_file_name=pdf_file_name)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code and all keyword impairment \")\n",
        "    print(f\"Highlighted code and impairment coordinates are saved into [{json_code_output_file}] file.\")\n",
        "  elif output_type == 5:\n",
        "    # Step-10: Highlighting keyword impairment into pdf\n",
        "    print(\"Step-10: Highlighting keyword impairment into pdf............\")\n",
        "    pdf_output_file, json_output_file = highlighter.highlight_keyword_impairment(pdf_file_name=pdf_file_name)\n",
        "    print(f\"File[{pdf_output_file}] is saved after highlighting keyword impairment.\")\n",
        "    print(f\"Highlighted coordinates are saved into [{json_output_file}] file.\")\n",
        "  else:\n",
        "    print(\"Please pass value 1, 2 and 3 for output type.\")\n",
        "\n",
        "  # Step-11: Clean up: move the current file into processed folder\n",
        "  if output_type in [1, 2, 3, 4, 5]:\n",
        "    if Path(f\"{cfg.file_path['PROCESSED_PATH']}/{pdf_file}\").exists():\n",
        "      # take backup of existing file\n",
        "      shutil.move(f\"{cfg.file_path['PROCESSED_PATH']}/{pdf_file}\", f\"{cfg.file_path['PROCESSED_PATH']}/{pdf_file}_bkp\")\n",
        "      # then move it\n",
        "      shutil.move(pdf_file_name, cfg.file_path[\"PROCESSED_PATH\"])\n",
        "    else:\n",
        "      shutil.move(pdf_file_name, cfg.file_path[\"PROCESSED_PATH\"])\n",
        "\n",
        "  # remove all pdf and text files\n",
        "  purge(f\"{cfg.file_path['TMP_PDF_FILES_PATH']}/*.pdf\")\n",
        "  purge(f\"{cfg.file_path['TMP_TXT_FILES_PATH']}/*.txt\")\n",
        "  pdf_list = []\n",
        "  txt_list = []"
      ],
      "metadata": {
        "id": "hiOT-aTfedMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e59d24-5cde-42e1-c3ee-f6b8742c4607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-1: Splitting pdf file............\n",
            "Step-2: Extracting text from pdf............\n",
            "Step-3: Searching ICD-10 code into text file..........\n",
            "Step-4: Get closet match of keyword impairment..........\n",
            "Step-5: Get date time list from text file..........\n",
            "Step-9: Highlighting ICD-10 code and all keyword impairment into pdf............\n",
            "File[/home/ocreng/ocrhigh/output/Practice_Copy_Test_NEWOCR_output_4.pdf] is saved after highlighting ICD-10 code and all keyword impairment \n",
            "Highlighted code and impairment coordinates are saved into [/home/ocreng/ocrhigh/output/Practice_Copy_Test_NEWOCR_output_4.json] file.\n",
            "CPU times: user 1min 7s, sys: 409 ms, total: 1min 8s\n",
            "Wall time: 1min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip output{output_type}.zip /home/ocreng/ocrhigh/output/*.*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Ev-Kf9d0oD",
        "outputId": "09174196-558f-4d79-a3c3-58c3bd9dbacb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: home/ocreng/ocrhigh/output/Practice_Copy_Test_NEWOCR_output_4.json (deflated 81%)\n",
            "  adding: home/ocreng/ocrhigh/output/Practice_Copy_Test_NEWOCR_output_4.pdf (deflated 21%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#matched_date_dict[13]"
      ],
      "metadata": {
        "id": "Bq9v0BAnd1K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wrong_keyword_dict[0]"
      ],
      "metadata": {
        "id": "OVYxrwiid5QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not \"T?\".find(\"?\") > -1 or not \"T)\".find(\")\") > -1:\n",
        "  print(\"yes\")"
      ],
      "metadata": {
        "id": "sM8BT17WGvyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Split TXT/PDF"
      ],
      "metadata": {
        "id": "A0j2GTO4z99I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pdf_file in os.listdir(INPUT_PDF_FILES_PATH):\n",
        "  pdf_file_name = f\"{INPUT_PDF_FILES_PATH}/{pdf_file}\"\n",
        "\n",
        "  # Step-1: splitting pdf file\n",
        "  print(\"Step-1: Splitting pdf file............\")\n",
        "  pdf_list = split_pdf(highlighter.PDF_FILES_PATH, pdf_file_name)\n",
        "\n",
        "  # Step-2: Extracting text from pdf\n",
        "  print(\"Step-2: Extracting text from pdf............\")\n",
        "  txt_list = extract_text_from_pdf(highlighter.PDF_FILES_PATH, highlighter.TXT_FILES_PATH, pdf_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2IbHJR4zqpf",
        "outputId": "37fdc1c7-e15b-4476-f70b-394c47f9bbc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-1: Splitting pdf file............\n",
            "Step-2: Extracting text from pdf............\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip text_files.zip /home/ocreng/ocrhigh/txt-files/*.*\n",
        "!zip pdf_files.zip /home/ocreng/ocrhigh/pdf-files/*.*"
      ],
      "metadata": {
        "id": "z6rYYKDWeAZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Split List"
      ],
      "metadata": {
        "id": "HQBBtc_dYJ0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_list(a_list, divider):\n",
        "  half = len(a_list)//divider\n",
        "  return a_list[:half], a_list[half:]\n",
        "\n",
        "A = [1,2,3,4,5,6]\n",
        "B, C = split_list(A, 3)\n",
        "B, C"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkFlsIHRVsuw",
        "outputId": "a60ae61d-b01a-4d2c-b0d5-a625769838df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 2], [3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "6 // 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EalXDscJXBZB",
        "outputId": "e612778d-db65-4749-f751-b6efb5722607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int(4 / 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INWhqkgNr1Z-",
        "outputId": "8ff3d32c-be22-47a2-9c25-034b6ba47bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install more-itertools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIoOMq6GXw0O",
        "outputId": "9c8d984e-9281-4a0f-a361-0ebb5ab4ef8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (10.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import more_itertools"
      ],
      "metadata": {
        "id": "2-pQgsgUXxX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in more_itertools.batched([1,2,3,4,5,6], 3):\n",
        "  print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN1bIbkfYR1q",
        "outputId": "affc4fc1-4f8a-4d27-ac99-10b9c13ff7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 2, 3)\n",
            "(4, 5, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in more_itertools.batched(\"ABCDEFGHIJ\", 4):\n",
        "  print(batch)"
      ],
      "metadata": {
        "id": "Y9dkrNWlYaTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_list(a_list, divider):\n",
        "  half = len(a_list) // divider\n",
        "  return a_list[:half], a_list[half:]"
      ],
      "metadata": {
        "id": "LGUDZSlTV3hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L =np.array([\n",
        "(63.5, 166.5, 81.5, 175.50439453125),\n",
        "(84.5, 166.5, 133.59002685546875, 175.50439453125),\n",
        "(130.0, 264.0, 147.49998474121094, 273.00439453125),\n",
        "(150.5, 264.0, 199.59002685546875, 273.00439453125),\n",
        "(150.5, 264.0, 199.59002685546875, 273.00439453125),\n",
        "(150.5, 264.0, 199.59002685546875, 273.00439453125)\n",
        "])"
      ],
      "metadata": {
        "id": "a2joGq7oUwQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(63.5, 166.5, 133.59002685546875, 175.50439453125)\n",
        "(130.0, 264.0, 199.59002685546875, 273.00439453125)"
      ],
      "metadata": {
        "id": "ZQfuyU5XkDph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.split(L, 3)"
      ],
      "metadata": {
        "id": "Wo1xtnpPBha8",
        "outputId": "6e4f56cf-2a34-4b6a-92dc-08f94853b03a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 63.5       , 166.5       ,  81.5       , 175.50439453],\n",
              "        [ 84.5       , 166.5       , 133.59002686, 175.50439453]]),\n",
              " array([[130.        , 264.        , 147.49998474, 273.00439453],\n",
              "        [150.5       , 264.        , 199.59002686, 273.00439453]]),\n",
              " array([[150.5       , 264.        , 199.59002686, 273.00439453],\n",
              "        [150.5       , 264.        , 199.59002686, 273.00439453]])]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LL= [\n",
        "(23.5, 443.0, 77.00015258789062, 454.00537109375),\n",
        "(79.5, 443.0, 122.99948120117188, 454.00537109375),\n",
        "(126.5, 443.0, 142.1002197265625, 454.00537109375),\n",
        "(384.5, 478.5, 430.0002136230469, 485.50341796875),\n",
        "(195.49978637695312, 487.0, 240.9998321533203, 494.00341796875),\n",
        "(246.5, 487.0, 260.5, 494.00341796875)]"
      ],
      "metadata": {
        "id": "e8jOWwcSGqyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(LL) // 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YtP_ibNnMkl",
        "outputId": "7c106988-be98-4d30-e273-98965dc57b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_coord in more_itertools.batched(LL, 2):\n",
        "  f_coord = batch_coord[0]\n",
        "  l_coord = batch_coord[-1]\n",
        "  print(f\"f_coord: {f_coord}\")\n",
        "  print(f\"l_coord: {l_coord}\")\n",
        "  phrase_coord = (f_coord[0], f_coord[1], l_coord[2], l_coord[3])\n",
        "  print(f\"key_phrase: CTA DISSECTION, phrase_coord: {phrase_coord}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3eFBmhrYmcv",
        "outputId": "d907861c-4602-49b4-91b8-817b68e220cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f_coord: (23.5, 443.0, 77.00015258789062, 454.00537109375)\n",
            "l_coord: (79.5, 443.0, 122.99948120117188, 454.00537109375)\n",
            "key_phrase: CTA DISSECTION, phrase_coord: (23.5, 443.0, 122.99948120117188, 454.00537109375)\n",
            "f_coord: (126.5, 443.0, 142.1002197265625, 454.00537109375)\n",
            "l_coord: (384.5, 478.5, 430.0002136230469, 485.50341796875)\n",
            "key_phrase: CTA DISSECTION, phrase_coord: (126.5, 443.0, 430.0002136230469, 485.50341796875)\n",
            "f_coord: (195.49978637695312, 487.0, 240.9998321533203, 494.00341796875)\n",
            "l_coord: (246.5, 487.0, 260.5, 494.00341796875)\n",
            "key_phrase: CTA DISSECTION, phrase_coord: (195.49978637695312, 487.0, 260.5, 494.00341796875)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key_phrase_sent = \"plan of care\"\n",
        "coordinates = [\n",
        "    (303.35198974609375, 578.380859375, 350.51043701171875, 589.455078125),\n",
        "    (432.93896484375, 647.3748779296875, 480.2064514160156, 658.4490966796875)]\n",
        "\n",
        "phrase_len = len(key_phrase_sent.split())\n",
        "print((len(coordinates) // phrase_len) )\n",
        "for batch_coord in more_itertools.batched(coordinates, 3):\n",
        "  f_coord = batch_coord[0]\n",
        "  l_coord = batch_coord[-1]\n",
        "  print(f\"f_coord: {f_coord}\")\n",
        "  print(f\"l_coord: {l_coord}\")\n",
        "  phrase_coord = (f_coord[0], f_coord[1], l_coord[2], l_coord[3])\n",
        "  print(f\"key_phrase: {key_phrase_sent}, phrase_coord: {phrase_coord}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klqIUKJ0b8K_",
        "outputId": "da9085de-c6af-4286-b458-535f43a7ef14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "f_coord: (303.35198974609375, 578.380859375, 350.51043701171875, 589.455078125)\n",
            "l_coord: (432.93896484375, 647.3748779296875, 480.2064514160156, 658.4490966796875)\n",
            "key_phrase: plan of care, phrase_coord: (303.35198974609375, 578.380859375, 480.2064514160156, 658.4490966796875)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LL= np.array([\n",
        "(23.5, 443.0, 77.00015258789062, 454.00537109375),\n",
        "(79.5, 443.0, 122.99948120117188, 454.00537109375),\n",
        "(126.5, 443.0, 142.1002197265625, 454.00537109375),\n",
        "(384.5, 478.5, 430.0002136230469, 485.50341796875),\n",
        "(195.49978637695312, 487.0, 240.9998321533203, 494.00341796875),\n",
        "(246.5, 487.0, 260.5, 494.00341796875)])\n",
        "\n",
        "for batch_coord in np.split(LL, 3):\n",
        "  f_coord = batch_coord[0]\n",
        "  l_coord = batch_coord[-1]\n",
        "  print(f\"f_coord: {f_coord}\")\n",
        "  print(f\"l_coord: {l_coord}\")\n",
        "  phrase_coord = (f_coord[0], f_coord[1], l_coord[2], l_coord[3])\n",
        "  print(f\"key_phrase: CTA DISSECTION, phrase_coord: {phrase_coord}\")"
      ],
      "metadata": {
        "id": "c_56oxCoG8Vo",
        "outputId": "32adbc68-58e1-4aef-fbf2-cf03f19b882a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f_coord: [ 23.5        443.          77.00015259 454.00537109]\n",
            "l_coord: [ 79.5        443.         122.9994812  454.00537109]\n",
            "key_phrase: CTA DISSECTION, phrase_coord: (23.5, 443.0, 122.99948120117188, 454.00537109375)\n",
            "f_coord: [126.5        443.         142.10021973 454.00537109]\n",
            "l_coord: [384.5        478.5        430.00021362 485.50341797]\n",
            "key_phrase: CTA DISSECTION, phrase_coord: (126.5, 443.0, 430.0002136230469, 485.50341796875)\n",
            "f_coord: [195.49978638 487.         240.99983215 494.00341797]\n",
            "l_coord: [246.5        487.         260.5        494.00341797]\n",
            "key_phrase: CTA DISSECTION, phrase_coord: (195.49978637695312, 487.0, 260.5, 494.00341796875)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```log\n",
        "f_coord: (63.5, 166.5, 81.5, 175.50439453125)\n",
        "l_coord: (84.5, 166.5, 133.59002685546875, 175.50439453125)\n",
        "key_phrase: CTA DISSECTION, phrase_coord: (63.5, 166.5, 133.59002685546875, 175.50439453125)\n",
        "f_coord: (130.0, 264.0, 147.49998474121094, 273.00439453125)\n",
        "l_coord: (150.5, 264.0, 199.59002685546875, 273.00439453125)\n",
        "key_phrase: CTA DISSECTION, phrase_coord: (130.0, 264.0, 199.59002685546875, 273.00439453125)\n",
        "```"
      ],
      "metadata": {
        "id": "x0AI0G1K4ui4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PDF OCRing"
      ],
      "metadata": {
        "id": "jxVd7v4y4kTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pikepdf\n",
        "\n",
        "def remove_password_from_pdf(input_file, output_file, password=None):\n",
        "    pdf = pikepdf.open(input_file, password=password)\n",
        "    pdf.save(output_file)"
      ],
      "metadata": {
        "id": "JXMHrfp84nVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_password_from_pdf(\"Keyword_Sample_File#4_Redacted_Redacted.pdf\",\n",
        "                         \"Keyword_Sample_File#4_Redacted_Redacted_pw_removed.pdf\", \"synodex\")"
      ],
      "metadata": {
        "id": "a9iOS4i74qqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ocrmypdf --skip-text Sample_File_No3_Redacted_pw_removed.pdf Sample_File_No3_Redacted.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hWihi2ainu8",
        "outputId": "9f1e17bb-c488-4c92-d95f-1c9dd20a52d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rScanning contents:   0% 0/50 [00:00<?, ?page/s]\rScanning contents:  46% 23/50 [00:00<00:00, 222.44page/s]\rScanning contents: 100% 50/50 [00:00<00:00, 326.07page/s]\n",
            "Start processing 2 pages concurrently\n",
            "    1 skipping all processing on this page\n",
            "    2 \u001b[33m[tesseract] lots of diacritics - possibly poor OCR\u001b[0m\n",
            "   12 \u001b[33m[tesseract] lots of diacritics - possibly poor OCR\u001b[0m\n",
            "   20 \u001b[33m[tesseract] lots of diacritics - possibly poor OCR\u001b[0m\n",
            "   19 \u001b[33m[tesseract] lots of diacritics - possibly poor OCR\u001b[0m\n",
            "   21 \u001b[33m[tesseract] lots of diacritics - possibly poor OCR\u001b[0m\n",
            "   36 \u001b[33m[tesseract] lots of diacritics - possibly poor OCR\u001b[0m\n",
            "   46 \u001b[33m[tesseract] lots of diacritics - possibly poor OCR\u001b[0m\n",
            "OCR: 100% 50.0/50.0 [01:42<00:00,  2.05s/page]\n",
            "Postprocessing...\n",
            "PDF/A conversion: 100% 50/50 [00:17<00:00,  2.80page/s]\n",
            "Recompressing JPEGs: 0image [00:00, ?image/s]\n",
            "Deflating JPEGs: 0image [00:00, ?image/s]\n",
            "JBIG2: 0item [00:00, ?item/s]\n",
            "Optimize ratio: 1.00 savings: -0.2%\n",
            "Image optimization did not improve the file - optimizations will not be used\n",
            "Output file is a PDF/A-2B (as expected)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Keyword_Sample_File4_Redacted_Redacted_pw_removed.pdf"
      ],
      "metadata": {
        "id": "Z-Z0qdq0mPW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exact Match"
      ],
      "metadata": {
        "id": "vIep4k_1UBSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mytext = \"\"\"\n",
        "  Encounter #16\n",
        "History & Physical Report\n",
        "1/25/2021: Lab Orders - Hypertriglyceridemia,                 sporadic (E78.3) (Marina Dobricic)\n",
        "   ppointment:    1/25/2021   9:00 AM\n",
        "                                  / Race: White\n",
        "Male\n",
        " The patient is a 41 year old male.\n",
        " Assessment & Plan (Angela Sabbagh; 1/23/2021            11:36 AM)\n",
        " Hypertriglyceridemia, sporadic (E78.3)\n",
        " Impression: stopped will restart and check labs\n",
        "Current Plans\n",
        "       @   Comprehensive Metabolic Panel (CMP) (80053)\n",
        "       @   Complete Blood Count with Differential (CBC) (85025)\n",
        "       @   Lipid Panel (LP) (80061)\n",
        "Signed by Marina Dobricic (1/27/2021        9:54 AM)\n",
        " Comprehensive Metabolic Panel (CMP) (80053) Final, Reviewed (Collected: 01/25/2021)\n",
        " Diagnosis: Hypertriglyceridemia, sporadic (E78.3)\n",
        " Note: Testing done at Silver Pine Medical         Group unless otherwise specified. 43455 Schoenherr Road, Suite 19, Sterling Heights, MI 48313\n",
        "      Sodium                                  141 mmol/L              (Normal Range: 135-145 mmol/L)         Result Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Potassium                               4.1  mmol/L             (Normal Range: 3.5-5.2 mmol/L)        Result    Note:\n",
        "                                                                    Result Annotation:\n",
        "      Chloride                                103   mmol/L            (Normal Range: 98-107 mmol/L)         Result    Note:\n",
        "                                                                    Result Annotation:\n",
        "      Carbon    Dioxide (CO2)                 30 mmol/L               (Normal Range: 21-31    mmol/L)     Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Anion Gap                               8                       (Normal Range: 5-17)      Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Glucose                                 89 mg/dL                (Normal Range: 60-99 mg/dL)        Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Blood Urea Nitrogen (BUN)               17 mg/dLl               (Normal Range: 8-23 mg/dL)        Result    Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Creatinine                              1.03   mg/dL            (Normal Range: 0.80-1.40 mg/dL)        Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      GFR                                     79 mL/min/1.73m2        (Normal Range: >59 mL/min/1.73m2)          Reeult      Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      GFR   African American                  96 mL/min/1.73m2        (Normal Range: >59 mL/min/1.73m2)          Result      Note:\n",
        "                                                                    Result Annotation:\n",
        "      Calcium                                 9.0 mg/dL               (Normal Range: 8.5-11.0 mg/dL)        Result     Note;:\n",
        "                                                                    Result Annotation:\n",
        "      Protein Total                           7.8 g/dL                (Normal Range: 6.4-8.2 g/dL)      Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Albumin                                 4.4 g/dL                (Normal Range: 3.4-5.0 g/dL)      Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Globulin                                3.40                    (Normal Range: 2.20-4.00)       Result    Wote:\n",
        "                                                                    Resutt Annotation:\n",
        "      Album   in/Globulin Ratio               1.3                         Result Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Alkaline Phosphatase (ALP)              64 U/L                  (Normal Range: 50-116 U/L)       Result    Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Aspartate Aminotransferase (AST)        25  U/L                 (Normal Range: 15-37 U/L)      Result    Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Alanine Aminotransferase (ALT)          33 U/L                  (Normal Range: 16-63 U/L)      Reeult    Note:\n",
        "                                                                    Resutt Annotation:\n",
        "      Bilirubin Total                         0.5 mg/dL               (Normal Range:   0.3-1.2 mg/dL)     Result     Note:\n",
        "                                                                    Resutt Annotation:\n",
        " Complete Blood Count with Differential (CBC) (85025) Final, Reviewed (Collected: 01/25/2021)\n",
        " Diagnosis: Hypertriglyceridemia, sporadic (E78.3)\n",
        " Note: Testina done at Silver Pine Medical Group unless otherwise specified. 43455 Schoenherr Road. Suite 19. Sterlina Heiahts. Ml 48313\n",
        "06/28/2022 12:27 pm                                              s                                                                          Page 11/144\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "T6du52kDm4mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term = \"MI\"\n",
        "#matches = len(re.findall(r'(?i)\\bAlbumin\\b', mytext)) > 0\n",
        "# matches = len(re.findall(f'(?i)\\\\b{term}\\\\b', mytext)) > 0\n",
        "#if len(re.findall(f'(?i)\\\\b{term}\\\\b', mytext)) > 0:\n",
        "matches = re.findall(r'\\bMI\\b', \"mi mister miss\")\n",
        "matches"
      ],
      "metadata": {
        "id": "4CxtWaFymzks",
        "outputId": "64302d54-1514-4ffe-fb7b-d4efa0824974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-30.txt\", \"r\") as f:\n",
        "  mytext = f.read()\n",
        "if len(re.findall(f'(?i)\\\\b{term}\\\\b', mytext)) > 0:\n",
        "  print(\"Found\")"
      ],
      "metadata": {
        "id": "reFqGUwgvkJX",
        "outputId": "f503bc29-dcf0-48ef-ec5c-16c3ef2a9a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-29aebb679170>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"page-30.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mmytext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'(?i)\\\\b{term}\\\\b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmytext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'page-30.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_keyword_dict1[0]"
      ],
      "metadata": {
        "id": "oy3VrsX1vpuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "icd10_code_dict1"
      ],
      "metadata": {
        "id": "-PfeXsii2ZGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "text_list = split_pdf(highlighter.PDF_FILES_PATH, \"APS_38600000R_final.pdf\")\n",
        "len(text_list)"
      ],
      "metadata": {
        "id": "CJw-u86V4uF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Highlight Test"
      ],
      "metadata": {
        "id": "4HjWgUOqYgot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file_name = \"page-30.pdf\"\n",
        "pdf_file = fitz.open(pdf_file_name)\n",
        "for page_num, page in enumerate(pdf_file):\n",
        "  for keyword_impairment in wrong_keyword_dict1[0]:\n",
        "    # print(f\"keyword_impairment11: {keyword_impairment}\")\n",
        "    highlight = page.search_for(keyword_impairment)\n",
        "    print(f\"keyword_impairment: {keyword_impairment}, coords: {highlight}\")\n",
        "    highlight = page.add_highlight_annot(highlight)\n",
        "    highlight.update()\n",
        "output_pdf_file_name = f\"{pdf_file_name.split('.')[0]}_output.pdf\"\n",
        "pdf_file.save(output_pdf_file_name, garbage=4, deflate=True, clean=True)"
      ],
      "metadata": {
        "id": "AZKQdR3oAzOx",
        "outputId": "1d508899-4a61-4ec1-ecee-953a755bfba7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keyword_impairment: Urea, coords: [Rect(70.31328582763672, 435.0400390625, 83.75199890136719, 443.0439453125)]\n",
            "keyword_impairment: Comprehensive Metabolic Panel, coords: [Rect(60.47419357299805, 207.52001953125, 118.06912994384766, 215.52392578125), Rect(121.66831970214844, 207.52001953125, 156.94491577148438, 215.52392578125), Rect(159.82464599609375, 207.52001953125, 176.2230682373047, 215.52392578125), Rect(29.997119903564453, 282.8800048828125, 95.99125671386719, 290.8839111328125), Rect(99.83040618896484, 282.8800048828125, 141.58639526367188, 290.8839111328125), Rect(144.46612548828125, 282.8800048828125, 163.6642608642578, 290.8839111328125)]\n",
            "keyword_impairment: ALT, coords: [Rect(148.30589294433594, 665.4400634765625, 158.38491821289062, 673.4439697265625)]\n",
            "keyword_impairment: GFR, coords: [Rect(45.355674743652344, 477.0400390625, 57.77448272705078, 485.0439453125), Rect(45.35564422607422, 498.1600341796875, 53.8148307800293, 506.1639404296875)]\n",
            "keyword_impairment: CBC, coords: [Rect(214.13943481445312, 216.1600341796875, 225.17835998535156, 224.1639404296875), Rect(206.74017333984375, 717.280029296875, 218.61904907226562, 725.283935546875)]\n",
            "keyword_impairment: Encounter, coords: [Rect(31.916934967041016, 27.03997802734375, 74.24887084960938, 35.04388427734375)]\n",
            "keyword_impairment: Anion Gap, coords: [Rect(44.87572479248047, 393.5200500488281, 65.99369812011719, 401.5239562988281), Rect(69.83332824707031, 393.5200500488281, 81.1722412109375, 401.5239562988281)]\n",
            "keyword_impairment: Sodium, coords: [Rect(45.35564422607422, 310.0, 69.01065063476562, 318.00390625)]\n",
            "keyword_impairment: CO2, coords: [Rect(109.46951293945312, 372.6400451660156, 120.38845825195312, 380.6439514160156)]\n",
            "keyword_impairment: CMP, coords: [Rect(186.74208068847656, 207.52001953125, 198.38096618652344, 215.52392578125), Rect(174.7432098388672, 282.8800048828125, 187.82196044921875, 290.8839111328125)]\n",
            "keyword_impairment: Alkaline Phosphatase, coords: [Rect(44.875816345214844, 623.6800537109375, 74.8731689453125, 631.6839599609375), Rect(77.99263000488281, 623.6800537109375, 120.8885269165039, 631.6839599609375)]\n",
            "keyword_impairment: BUN, coords: [Rect(129.18763732910156, 435.0400390625, 140.2265625, 443.0439453125)]\n",
            "keyword_impairment: Complete Blood Count, coords: [Rect(60.47420883178711, 216.1600341796875, 95.99054718017578, 224.1639404296875), Rect(99.35047149658203, 216.1600341796875, 119.26856231689453, 224.1639404296875), Rect(123.10819244384766, 216.1600341796875, 140.70652770996094, 224.1639404296875), Rect(29.997119903564453, 717.280029296875, 70.07352447509766, 725.283935546875), Rect(73.91290283203125, 717.280029296875, 97.67060852050781, 725.283935546875), Rect(101.0302963256836, 717.280029296875, 121.4283447265625, 725.283935546875)]\n",
            "keyword_impairment: Blood Urea Nitrogen, coords: [Rect(45.59565734863281, 435.0400390625, 65.99370574951172, 443.0439453125), Rect(70.31328582763672, 435.0400390625, 87.11167907714844, 443.0439453125), Rect(90.47135162353516, 435.0400390625, 118.41556549072266, 443.0439453125)]\n",
            "keyword_impairment: Glucose, coords: [Rect(45.355674743652344, 414.4000549316406, 71.18318176269531, 422.4039611816406)]\n",
            "keyword_impairment: Lipid Panel, coords: [Rect(60.95414733886719, 223.56005859375, 77.51274108886719, 232.564453125), Rect(81.1122055053711, 223.56005859375, 97.51043701171875, 232.564453125)]\n",
            "keyword_impairment: MI, coords: [Rect(210.2085723876953, 61.5999755859375, 218.56410217285156, 69.6038818359375), Rect(90.52633666992188, 171.280029296875, 97.7038803100586, 179.283935546875), Rect(136.0669708251953, 291.52001953125, 143.2662811279297, 299.52392578125), Rect(521.4700317382812, 299.9200134277344, 526.5894775390625, 307.9239196777344), Rect(207.80300903320312, 477.0400390625, 216.27084350585938, 485.0439453125), Rect(360.8774108886719, 477.0400390625, 369.1646423339844, 485.0439453125), Rect(207.802978515625, 498.1600341796875, 216.27081298828125, 506.1639404296875), Rect(361.0693054199219, 498.1600341796875, 369.3244934082031, 506.1639404296875), Rect(60.2342529296875, 561.0400390625, 67.91350555419922, 569.0439453125), Rect(88.38217163085938, 644.56005859375, 96.2025375366211, 652.56396484375), Rect(79.72894287109375, 665.4400634765625, 87.52114868164062, 673.4439697265625), Rect(136.0669403076172, 725.6799926757812, 143.26625061035156, 733.6838989257812)]\n",
            "keyword_impairment: Albumin, coords: [Rect(44.87572479248047, 561.0400390625, 71.75313568115234, 569.0439453125)]\n",
            "keyword_impairment: Aspartate, coords: [Rect(44.87578582763672, 644.56005859375, 77.48865509033203, 652.56396484375)]\n",
            "keyword_impairment: Urea Nitrogen, coords: [Rect(70.31328582763672, 435.0400390625, 87.11167907714844, 443.0439453125), Rect(90.47135162353516, 435.0400390625, 118.41556549072266, 443.0439453125)]\n",
            "keyword_impairment: Creatinine, coords: [Rect(45.355674743652344, 456.1600341796875, 80.04353332519531, 464.1639404296875)]\n",
            "keyword_impairment: Hypertriglyceridemia, coords: [Rect(139.1866455078125, 61.5999755859375, 222.74185180664062, 69.6038818359375), Rect(29.517169952392578, 171.280029296875, 101.29265594482422, 179.283935546875), Rect(74.87284851074219, 291.52001953125, 146.86593627929688, 299.52392578125), Rect(74.87281799316406, 725.6799926757812, 146.86590576171875, 733.6838989257812)]\n",
            "keyword_impairment: Calcium, coords: [Rect(44.395713806152344, 519.0400390625, 70.8531723022461, 527.0439453125)]\n",
            "keyword_impairment: ppointment, coords: [Rect(34.07672882080078, 79.1199951171875, 70.67321014404297, 87.1239013671875)]\n",
            "keyword_impairment: Chloride, coords: [Rect(45.355674743652344, 351.52001953125, 72.8730239868164, 359.52392578125)]\n",
            "keyword_impairment: AST, coords: [Rect(156.6650390625, 644.56005859375, 167.34400939941406, 652.56396484375)]\n",
            "keyword_impairment: Potassium, coords: [Rect(45.59565734863281, 330.6400146484375, 79.2884292602539, 338.6439208984375)]\n",
            "keyword_impairment: ALP, coords: [Rect(131.42750549316406, 623.6800537109375, 141.98651123046875, 631.6839599609375)]\n",
            "keyword_impairment: Bilirubin Total, coords: [Rect(45.59574890136719, 686.56005859375, 75.59286499023438, 694.56396484375), Rect(78.95254516601562, 686.56005859375, 94.35107421875, 694.56396484375)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ICD 10 Code"
      ],
      "metadata": {
        "id": "RIRH21XK258d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p pdf-files\n",
        "!mkdir -p txt-files"
      ],
      "metadata": {
        "id": "Ebs7E8X23lri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define directory path after creating it\n",
        "pdf_files_path = \"pdf-files\"\n",
        "txt_files_path = \"txt-files\"\n",
        "\n",
        "# create nlp instance\n",
        "nlp = English()\n",
        "\n",
        "\n",
        "def split_pdf(pdf_path):\n",
        "  pdf_in_file = open(pdf_path, \"rb\")\n",
        "  pdf = PdfReader(pdf_in_file)\n",
        "  pdf_list = []\n",
        "  for page in range(len(pdf.pages)):\n",
        "      inputpdf = PdfReader(pdf_in_file)\n",
        "      output = PdfWriter()\n",
        "      output.add_page(inputpdf.pages[page])\n",
        "      with open(f\"{pdf_files_path}/page-{page}.pdf\", \"wb\") as outputStream:\n",
        "          output.write(outputStream)\n",
        "          pdf_list.append(f\"page-{page}.pdf\")\n",
        "  return pdf_list\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_list):\n",
        "    txt_file_list = []\n",
        "    i = 0\n",
        "    for pdf_file in pdf_list:\n",
        "        with open(os.path.join(pdf_files_path, pdf_file), \"rb\") as f:\n",
        "            pdf = pdftotext.PDF(f)\n",
        "\n",
        "        # Read all the text into one string\n",
        "        pdf_text = \"\\n\\n\".join(pdf)\n",
        "\n",
        "        # write text into file\n",
        "        with open(f\"{txt_files_path}/page-{str(i)}.txt\", \"a\") as f:\n",
        "            f.write(pdf_text)\n",
        "        txt_file_list.append(f\"{txt_files_path}/page-{str(i)}.txt\")\n",
        "        i += 1\n",
        "    return txt_file_list\n",
        "\n",
        "\n",
        "def get_opt_pattern(icd_10_code):\n",
        "  # create alternate pattern\n",
        "  code_arr = icd_10_code.split(\".\")\n",
        "  if len(code_arr) > 1:\n",
        "    code1 = f\"{code_arr[0]}. {code_arr[1]}\"\n",
        "    code2 = f\"{code_arr[0]} .{code_arr[1]}\"\n",
        "    code3 = f\"{code_arr[0]} . {code_arr[1]}\"\n",
        "    return [code1, code2, code3]\n",
        "  else:\n",
        "    return icd_10_code\n",
        "\n",
        "\n",
        "def highlight_icd10_code(pdf_page_dict: dict, pdf_file_name: str):\n",
        "    pdf_file = fitz.open(pdf_file_name)\n",
        "\n",
        "    def highlight_pdf(highlight):\n",
        "        for inst in highlight:\n",
        "          highlight = page.add_highlight_annot(inst)\n",
        "          highlight.update()\n",
        "          highlight = page.search_for(text_to_be_highlighted)\n",
        "          print(f\"Page-{page_num}: \", code, highlight, end='\\n')\n",
        "\n",
        "    for page_num, page in enumerate(pdf_file):\n",
        "        if page_num in pdf_page_dict:\n",
        "          for code in pdf_page_dict[page_num]:\n",
        "            text_to_be_highlighted = code\n",
        "            highlight = page.search_for(text_to_be_highlighted)\n",
        "            print(f\"Page-{page_num}: \", code, highlight, end='\\n')\n",
        "            if len(highlight) == 0:\n",
        "                alternate_code_list = get_opt_pattern(code)\n",
        "                for alt_code in alternate_code_list:\n",
        "                  text_to_be_highlighted = alt_code\n",
        "                  highlight = page.search_for(text_to_be_highlighted)\n",
        "                  # highlight pdf for option pattern\n",
        "                  highlight_pdf(highlight)\n",
        "            # highlight pdf for main pattern\n",
        "            highlight_pdf(highlight)\n",
        "\n",
        "    output_pdf_file_name = f\"{pdf_file_name.split('.')[0]}_output.pdf\"\n",
        "    pdf_file.save(output_pdf_file_name, garbage=4, deflate=True, clean=True)\n",
        "    return output_pdf_file_name\n",
        "\n",
        "\n",
        "def search_icd_10_code(txt_list):\n",
        "  pdf_page_vocab = {}\n",
        "  for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "      page_txt = f.read()\n",
        "      # filter the page that have line number instead of code\n",
        "      if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "        doc = nlp(page_txt)\n",
        "        code_list = [ent.text for ent in doc.ents]\n",
        "        if len(code_list) != 0:\n",
        "          #print(txt_file)\n",
        "          page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "          pdf_page_vocab[page_number] = code_list\n",
        "          # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "  return pdf_page_vocab"
      ],
      "metadata": {
        "id": "snzCZDEN29Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-1: splitting pdf file\n",
        "pdf_file_name = \"28page.pdf\"\n",
        "pdf_list = split_pdf(pdf_file_name)\n",
        "\n",
        "# Step-2: Extracting text from pdf\n",
        "txt_list = extract_text_from_pdf(pdf_list)\n",
        "\n",
        "# Step-3: loading and updating patterns to Spacy\n",
        "nlp.add_pipe(\"entity_ruler\").from_disk(\"./icd10_code_patterns-v1.jsonl\")\n",
        "\n",
        "# Step-4: Searching ICD-10 code\n",
        "#print (txt_list)\n",
        "pdf_page_vocab = search_icd_10_code(txt_list)\n",
        "\n",
        "# Step-5: Highlighting ICD-10 code into pdf\n",
        "output_file_name = highlight_icd10_code(pdf_page_vocab, pdf_file_name)\n",
        "print(f\"File[{output_file_name}] is saved after highlighting ICD-10 code\")"
      ],
      "metadata": {
        "id": "JPlZD1rj3D0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Phrase matching"
      ],
      "metadata": {
        "id": "3T0ApWNLUV1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher"
      ],
      "metadata": {
        "id": "ftahgHKBUYC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synid_df = pd.read_csv(cfg.csv_files[\"IMP_CSV\"])\n",
        "synid_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-Wmq-wtxVDUU",
        "outputId": "8a90f772-8cfc-40cc-9ae9-40a06a6f54c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     SynId                          Short_Description\n",
              "0  KW20262                                 US thyroid\n",
              "1  KW20261          no graphic evidence of malignancy\n",
              "2  KW20260                  no evidence of malignancy\n",
              "3  KW20259  scattered areas of fibroglandular density\n",
              "4  KW20258  scattered areas of fibroglandular density"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-9fb8b197-cd4b-4965-8cbe-09f68f09f80a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SynId</th>\n",
              "      <th>Short_Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KW20262</td>\n",
              "      <td>US thyroid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KW20261</td>\n",
              "      <td>no graphic evidence of malignancy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KW20260</td>\n",
              "      <td>no evidence of malignancy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KW20259</td>\n",
              "      <td>scattered areas of fibroglandular density</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KW20258</td>\n",
              "      <td>scattered areas of fibroglandular density</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9fb8b197-cd4b-4965-8cbe-09f68f09f80a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-2a7ea9b6-8911-4b0e-b56b-5a6b6e654afc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a7ea9b6-8911-4b0e-b56b-5a6b6e654afc')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-2a7ea9b6-8911-4b0e-b56b-5a6b6e654afc button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9fb8b197-cd4b-4965-8cbe-09f68f09f80a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9fb8b197-cd4b-4965-8cbe-09f68f09f80a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_icd_10_keyword_pattern(synid_df):\n",
        "  patterns = []\n",
        "  for _, row in synid_df.iterrows():\n",
        "    patterns.append(row[\"Short_Description\"])\n",
        "  return patterns"
      ],
      "metadata": {
        "id": "S25-C5AZWucL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = make_icd_10_keyword_pattern(synid_df)\n",
        "keywords[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STbmzWgwW8hG",
        "outputId": "59d7699f-89a1-40dc-c345-1b90c12c5d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['US thyroid',\n",
              " 'no graphic evidence of malignancy',\n",
              " 'no evidence of malignancy',\n",
              " 'scattered areas of fibroglandular density',\n",
              " 'scattered areas of fibroglandular density',\n",
              " 'BI-RADS 6',\n",
              " 'BI-RADS 5',\n",
              " 'BI-RADS 4C',\n",
              " 'BI-RADS 4B',\n",
              " 'BI-RADS 4A']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
        "patterns = list(nlp.tokenizer.pipe(keywords))\n",
        "phrase_matcher.add('keywords', patterns)"
      ],
      "metadata": {
        "id": "W0a69f63Y-5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-2.txt\", \"r\") as f:\n",
        "  page_txt = f.read()\n",
        "  # filter the page that have line number instead of code\n",
        "  #if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "  doc = nlp(page_txt)\n",
        "  matches = phrase_matcher(doc)\n",
        "\n",
        "  keyword_list = []\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start: end]\n",
        "    keyword_list.append(f\"{span}\")"
      ],
      "metadata": {
        "id": "W21nm7_VXbg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-HCX2ysX3H8",
        "outputId": "0dcb864c-fa0f-49fc-86ad-84a2a89045a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T', 'HHS', 'US']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-25.txt\", \"r\") as f:\n",
        "  page_txt = f.read()\n",
        "  # filter the page that have line number instead of code\n",
        "  #if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "  doc = nlp(page_txt)\n",
        "  matches = phrase_matcher(doc)\n",
        "\n",
        "  keyword_list = []\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start: end]\n",
        "    keyword_list.append(f\"{span}\")"
      ],
      "metadata": {
        "id": "Ga99oXcsYeZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB-D0G4FZ6sS",
        "outputId": "2e42c010-58c1-4973-82e0-97b5d832ac5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hypertriglyceridemia',\n",
              " 'Hypertriglyceridemia',\n",
              " 'EKG',\n",
              " 'Hypertension',\n",
              " 'Atrial Fibrillation',\n",
              " 'Headache']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-36.txt\", \"r\") as f:\n",
        "  page_txt = f.read()\n",
        "  # filter the page that have line number instead of code\n",
        "  #if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "  doc = nlp(page_txt)\n",
        "  matches = phrase_matcher(doc)\n",
        "\n",
        "  keyword_list = []\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start: end]\n",
        "    keyword_list.append(f\"{span}\")"
      ],
      "metadata": {
        "id": "-W4RdswabV3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJgGec4AbYER",
        "outputId": "7e40ee89-461f-487c-fd2e-2efe2c9a78b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hypertriglyceridemia',\n",
              " 'Hypertriglyceridemia',\n",
              " 'EKG',\n",
              " 'Hypertension',\n",
              " 'Atrial Fibrillation',\n",
              " 'Diplopia',\n",
              " 'Headache',\n",
              " 'Visual Loss',\n",
              " 'Gynecomastia',\n",
              " 'Dysphagia',\n",
              " 'Hematuria']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synid_df = pd.read_csv(\"synid_and_keywords_impairment.csv\")\n",
        "keywords = make_icd_10_keyword_pattern(synid_df)"
      ],
      "metadata": {
        "id": "xEqoN5-yidiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
        "patterns = list(nlp.tokenizer.pipe(keywords))\n",
        "phrase_matcher.add('keywords', patterns)"
      ],
      "metadata": {
        "id": "-G151LL_ij0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-36.txt\", \"r\") as f:\n",
        "  page_txt = f.read()\n",
        "  # filter the page that have line number instead of code\n",
        "  #if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "  doc = nlp(page_txt)\n",
        "  matches = phrase_matcher(doc)\n",
        "\n",
        "  keyword_list = []\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start: end]\n",
        "    keyword_list.append(f\"{span}\")"
      ],
      "metadata": {
        "id": "xmNbJ187irjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4toMzolitCG",
        "outputId": "7cdc57c5-fb3e-439a-8fe1-23be3ac2a46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hypertriglyceridemia',\n",
              " 'Hypertriglyceridemia',\n",
              " 'EKG',\n",
              " 'Hypertension',\n",
              " 'Atrial Fibrillation',\n",
              " 'Diplopia',\n",
              " 'Headache',\n",
              " 'Visual Loss',\n",
              " 'Gynecomastia',\n",
              " 'Dysphagia',\n",
              " 'Hematuria',\n",
              " 'Headaches']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_keyword_dict1[30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3bdIYhJgbNz",
        "outputId": "a0c6ff4c-9446-4837-a735-1753ea56e093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ALP',\n",
              " 'ALT',\n",
              " 'AST',\n",
              " 'Albumin',\n",
              " 'Alkaline Phosphatase',\n",
              " 'Anion Gap',\n",
              " 'Aspartate',\n",
              " 'BUN',\n",
              " 'Bilirubin Total',\n",
              " 'Blood Urea Nitrogen',\n",
              " 'CBC',\n",
              " 'CMP',\n",
              " 'CO2',\n",
              " 'Calcium',\n",
              " 'Chloride',\n",
              " 'Complete Blood Count',\n",
              " 'Comprehensive Metabolic Panel',\n",
              " 'Creatinine',\n",
              " 'Encounter',\n",
              " 'GFR',\n",
              " 'Glucose',\n",
              " 'Hypertriglyceridemia',\n",
              " 'Lipid Panel',\n",
              " 'MI',\n",
              " 'Potassium',\n",
              " 'Sodium',\n",
              " 'Urea',\n",
              " 'Urea Nitrogen',\n",
              " 'ppointment'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = [keyword for keyword in wrong_keyword_dict1[30]]\n",
        "len(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jiz1Nlji4t6",
        "outputId": "54ef3417-8146-46fe-d8cd-679a8c13005c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(set(keywords)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACLV0LxtkWji",
        "outputId": "be133bd8-2894-441a-8c07-2ae2281bcca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "fisKIpBvjLdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
        "patterns = list(nlp.tokenizer.pipe(keywords))\n",
        "phrase_matcher.add('keywords', patterns)"
      ],
      "metadata": {
        "id": "LLFgaNg-jJF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"page-30.txt\", \"r\") as f:\n",
        "  page_txt = f.read()\n",
        "  # filter the page that have line number instead of code\n",
        "  #if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "  doc = nlp(page_txt)\n",
        "  matches = phrase_matcher(doc)\n",
        "\n",
        "  keyword_list = []\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start: end]\n",
        "    keyword_list.append(f\"{span}\")\n",
        "\n",
        "print(len(keyword_list))\n",
        "len(list(set(keyword_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJcvrEv1jVtm",
        "outputId": "99084aaa-5ce1-4fd3-9d4c-f30f916dadfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "keywords = [keyword for keyword in wrong_keyword_dict1[30]]\n",
        "pattern1 = [{'ORTH': keywords}]\n",
        "phrase_matcher.add('keywords', None, pattern1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "9fDJwzw3gg2W",
        "outputId": "80526fd2-bf26-4ca9-87cb-5933bf18ae87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-13a45877d65f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwrong_keyword_dict1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpattern1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mphrase_matcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keywords'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/matcher/phrasematcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.phrasematcher.PhraseMatcher.add\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
          ]
        }
      ]
    }
  ]
}