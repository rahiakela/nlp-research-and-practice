{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMs5Abq1n7ihJxkQKt0hX/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/nlp-for-vector-similarity-search/00_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "pIWMQt5kTmgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The respected grandfather of vector similarity search, born back in the 1970s. It consists of two parts, **Term Frequency (TF) and Inverse Document Frequency (IDF)**.\n",
        "\n",
        "The TF component counts the number of times a term appears within a document and divides this by the total number of terms in that same document.\n",
        "\n",
        "<img src='https://d33wubrfki0l68.cloudfront.net/d6e2ea146c9da6e32711b0a925f97d0027e9935c/83b7f/images/semantic-search-10.png' width='600'/>\n",
        "\n",
        ">The term frequency (TF) component of TF-IDF counts the frequency of our query (‘bananas’) and divides by the frequency of all tokens.\n",
        "\n",
        "That is the first half of our calculation, we have the frequency of our query within the current Document `f(q,D)` — over the frequency of all terms within the current Document `f(t,D)`.\n",
        "\n",
        "The Term Frequency is a good measure, but doesn’t allow us to differentiate between common and uncommon words. If we were to search for the word ‘the’ — using TF alone we’d assign this sentence the same relevance as had we searched ‘bananas’.\n",
        "\n",
        "That’s fine until we begin comparing documents, or searching with longer queries. We don’t want words like ‘the’,* ‘is’*, or *‘it’* to be ranked as highly as *‘bananas’* or *‘street’*.\n",
        "\n",
        "Ideally, we want matches between rarer words to score higher. To do this, we can multiply TF by the second term — IDF. The Inverse Document Frequency measures how common a word is across all of our documents.\n",
        "\n",
        "<img src='https://d33wubrfki0l68.cloudfront.net/8b01f75b97e0fb3bac30b88c8ad81fbcff9f2b15/1769f/images/semantic-search-11.png' width='600'/>\n",
        "\n",
        ">The inverse document frequency (IDF) component of TF-IDF counts the number of documents that contain our query.\n",
        "\n",
        "In this example, we have three sentences. When we calculate the IDF for our common word ‘is’, we return a much lower number than that for the rarer word ‘forest’.\n",
        "\n",
        "If we were to then search for both words ‘is’ and ‘forest’ we would merge TF and IDF like so:\n",
        "\n",
        "<img src='https://d33wubrfki0l68.cloudfront.net/270adb0a0e4e7e9e7b1125e4338088525d372b55/b2672/images/semantic-search-12.png' width='600'/>\n",
        "\n",
        "We calculate the TF(‘is’, D) and TF(‘forest’, D) scores for docs a, b, and c. The IDF value is across all docs — so we calculate just IDF(‘is’) and IDF(‘forest’) once. Then, we get TF-IDF values for both words in each doc by multiplying the TF and IDF components. Sentence a scores highest for ‘forest’, and ‘is’ always scores 0 as the IDF(‘is’) score is 0.\n"
      ],
      "metadata": {
        "id": "aOHXOy6jTnfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "oHQXNqkpU4jZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "_qbdVnioU51j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tfidf(word, sentences):\n",
        "  tf = []\n",
        "  num_docs = 0\n",
        "  for sentence in sentences:\n",
        "    # calculate TF\n",
        "    term_count = len([x for x in sentence if word in sentence])\n",
        "    tf.append(term_count / len(sentence))\n",
        "\n",
        "    # count number of docs for IDF\n",
        "    num_docs += 1 if word in sentence else 0\n",
        "  # calculate IDF\n",
        "  idf = np.log10(len(sentences) / num_docs)\n",
        "  return [round(_tf * idf, 2) for _tf in tf]"
      ],
      "metadata": {
        "id": "yJ2WUMKnV9FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculate TF-IDF"
      ],
      "metadata": {
        "id": "0rikmHBtXJ_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"purple is the best city in the forest\".split()\n",
        "b = \"there is an art to getting your way and throwing bananas on to the street is not it\".split()\n",
        "c = \"it is not often you find soggy bananas on the street\".split()"
      ],
      "metadata": {
        "id": "IDiGWjWGXQ7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_a, tfidf_b, tfidf_c = calculate_tfidf(\"forest\", [a, b, c])\n",
        "print(f\"TF-IDF a: {tfidf_a}\\nTF-IDF b: {tfidf_b}\\nTF-IDF c: {tfidf_c}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbWD6yqvXUQ6",
        "outputId": "fffa0a98-a44f-4846-89a8-5bd8df3aa83d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF a: 0.48\n",
            "TF-IDF b: 0.0\n",
            "TF-IDF c: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That’s great, but where does vector similarity search come into this? "
      ],
      "metadata": {
        "id": "GHqBgIXWaw7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectors"
      ],
      "metadata": {
        "id": "3XZKFxD8asEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, we take our vocabulary (a big list of all words in our dataset) — and calculate the TF-IDF for each and every word.\n",
        "\n",
        "<img src='https://d33wubrfki0l68.cloudfront.net/5fd4ff23c48442ae956dc95a6040732793440168/33a4d/images/semantic-search-13.png' width='600'/>\n",
        "\n",
        ">We calculate the TF-IDF value for every word in our vocabulary to create a TF-IDF vector. This process is repeated for each document.\n",
        "\n",
        "We can put all of this together to create our TF-IDF vectors like so:"
      ],
      "metadata": {
        "id": "vtY5mMdhXL_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(a + b + c)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2eErPIbYg8N",
        "outputId": "53434cd0-fb99-4546-9cdf-f46dc839b502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'in', 'you', 'and', 'the', 'your', 'purple', 'bananas', 'often', 'art', 'it', 'an', 'there', 'forest', 'is', 'best', 'throwing', 'getting', 'to', 'way', 'street', 'on', 'not', 'find', 'soggy', 'city'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize vectors\n",
        "vec_a = []\n",
        "vec_b = []\n",
        "vec_c = []\n",
        "\n",
        "for word in vocab:\n",
        "  tfidf_a, tfidf_b, tfidf_c = calculate_tfidf(word, [a, b, c])\n",
        "  vec_a.append(tfidf_a)\n",
        "  vec_b.append(tfidf_b)\n",
        "  vec_c.append(tfidf_c)\n",
        "\n",
        "print(vec_a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JECev5PkYt4j",
        "outputId": "d9a27470-aadd-49dd-b4bd-02af35e2274c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.48, 0.0, 0.0, 0.0, 0.0, 0.48, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48, 0.0, 0.48, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vec_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmeEI_OgZgVT",
        "outputId": "67eef72e-c52c-4160-83f8-0a5739358a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.48, 0.0, 0.48, 0.0, 0.18, 0.0, 0.48, 0.18, 0.48, 0.48, 0.0, 0.0, 0.0, 0.48, 0.48, 0.48, 0.48, 0.18, 0.18, 0.18, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vec_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWhIXFa1ZhoE",
        "outputId": "ccbc0f93-22a1-4873-fee8-8f34a825e641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.48, 0.0, 0.0, 0.0, 0.0, 0.18, 0.48, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18, 0.18, 0.18, 0.48, 0.48, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From there we have our TF-IDF vector. It’s worth noting that vocab sizes can easily be in the 20K+ range, so the vectors produced using this method are incredibly sparse — which means we cannot encode any semantic meaning."
      ],
      "metadata": {
        "id": "iQH0gPpPaisL"
      }
    }
  ]
}