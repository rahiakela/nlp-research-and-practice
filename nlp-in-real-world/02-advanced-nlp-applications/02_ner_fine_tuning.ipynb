{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLiLiThkK2EzP2CRNpwG2S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/nlp-research-and-practice/blob/main/nlp-in-real-world/02-advanced-nlp-applications/02_ner_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "LCbxNJZpcfKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install transformers\n",
        "!pip -q install spacy\n",
        "!pip install spacy-transformers==1.1.5 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "3WhrYAcJcgVG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForTokenClassification\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "mXUnM_u0c059"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://noisy-text.github.io/2017/files/wnut17train.conll"
      ],
      "metadata": {
        "id": "k-sYdNzug3LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##spaCy fine-tuning"
      ],
      "metadata": {
        "id": "JpidC2lIcm26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [\n",
        "    (\n",
        "        \"Chef added some salt and pepper to the rice.\",\n",
        "        {\n",
        "            \"entities\": [\n",
        "            (16, 20, 'SPICE'),\n",
        "            (25, 31, 'SPICE'),\n",
        "            (39, 43, 'INGREDIENT')\n",
        "          ]\n",
        "        }\n",
        "    ),\n",
        "    (\n",
        "        \"The pasta was set to boil with some salt.\",\n",
        "        {\n",
        "            \"entities\": [\n",
        "            (4, 9, 'INGREDIENT'),\n",
        "            (36, 40, 'SPICE')\n",
        "          ]\n",
        "        }\n",
        "    ),\n",
        "    (\n",
        "        \"Adding egg to the rice dish with some pepper.\",\n",
        "        {\n",
        "            \"entities\": [\n",
        "            (7, 10, 'INGREDIENT'),\n",
        "            (18, 22, 'INGREDIENT'),\n",
        "            (38, 44, 'SPICE')\n",
        "          ]\n",
        "        }\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "H4qMPoWamTde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "print(\"Created a blank en model\")\n",
        "\n",
        "nlp.add_pipe(\"ner\", last=True)\n",
        "ner = nlp.get_pipe(\"ner\")\n",
        "print(\"pipe_names\", nlp.pipe_names)\n",
        "\n",
        "for _, annotations in train_data:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])\n",
        "\n",
        "# begin training\n",
        "optimizer = nlp.begin_training()"
      ],
      "metadata": {
        "id": "sOha62hycnfc",
        "outputId": "91a086fd-2a55-42d7-96c1-6da94bd7f58b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a blank en model\n",
            "pipe_names ['ner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_iter = 100\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiece\", \"trf_tok2vec\"]\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "  for _ in range(n_iter):\n",
        "    random.shuffle(train_data)\n",
        "    losses = {}\n",
        "    for batch in spacy.util.minibatch(train_data, size=2):\n",
        "      for text, annots in batch:\n",
        "        doc = nlp.make_doc(text)\n",
        "        nlp.update([Example.from_dict(doc, annots)], drop=0.5, sgd=optimizer, losses=losses)\n",
        "    print(f\"losses: {losses}\")"
      ],
      "metadata": {
        "id": "P02brnERdE57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entities(raw_text):\n",
        "  doc = nlp(raw_text)\n",
        "  result = []\n",
        "  for word in doc.ents:\n",
        "    result.append((word.text, word.label_))\n",
        "  return result"
      ],
      "metadata": {
        "id": "ArxlMZBEh1IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_entities(\"Add water to the spaghetti\"))"
      ],
      "metadata": {
        "id": "EFfn3jFaX3EV",
        "outputId": "e0359dc3-ccd7-47d3-e323-0efbd1f75561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('water', 'SPICE'), ('spaghetti', 'SPICE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_entities(\"Add some paprika on top to your pasta.\"))"
      ],
      "metadata": {
        "id": "faN039OjX7xQ",
        "outputId": "4f2b7fd7-a3f5-497e-9308-da93a52f4130",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('paprika', 'SPICE'), ('pasta', 'INGREDIENT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformers fine-tuning"
      ],
      "metadata": {
        "id": "SocN2Ugna8Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_tokens(raw_text):\n",
        "  raw_docs = re.split(r\"\\n\\t?\\n\", raw_text)\n",
        "  token_docs = []\n",
        "  tag_docs = []\n",
        "\n",
        "  for doc in raw_docs:\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    for line in doc.split(\"\\n\"):\n",
        "      row = line.split(\"\\t\")\n",
        "      if len(row) == 1:\n",
        "        token = row[0]\n",
        "        tag = None\n",
        "      else:\n",
        "        token, tag = line.split(\"\\t\")\n",
        "      tokens.append(token)\n",
        "      tags.append(tag)\n",
        "    token_docs.append(tokens)\n",
        "    tag_docs.append(tags)\n",
        "  return token_docs, tag_docs"
      ],
      "metadata": {
        "id": "ho85PyOlbGJr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_wnut(file_path):\n",
        "  file_path = Path(file_path)\n",
        "\n",
        "  raw_text = file_path.read_text().strip()\n",
        "  token_docs, tag_docs = split_into_tokens(raw_text)\n",
        "  return token_docs, tag_docs"
      ],
      "metadata": {
        "id": "PwD8lv7OstD8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts, tags = read_wnut(\"wnut17train.conll\")"
      ],
      "metadata": {
        "id": "z895Dn8av6gK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0][10:17], tags[0][10:17], sep=\"\\n\")"
      ],
      "metadata": {
        "id": "kJJQs4VkwB6d",
        "outputId": "01b48c9a-996a-436f-de1e-ab5134b248b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['for', 'two', 'weeks', '.', 'Empire', 'State', 'Building']\n",
            "['O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting our data into training and validation set\n",
        "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=0.2)"
      ],
      "metadata": {
        "id": "VSXC1EVywLzt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QnKACCLzxCnX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}