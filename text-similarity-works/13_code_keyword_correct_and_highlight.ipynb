{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+bHXJNZcqXmC0owuknIC/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/text-similarity-works/13_code_keyword_correct_and_highlight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEXPsOYCyUbD"
      },
      "outputs": [],
      "source": [
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "!pip install pillow\n",
        "\n",
        "!sudo apt install build-essential libpoppler-cpp-dev pkg-config python3-dev\n",
        "!pip install -U pdftotext\n",
        "!pip install PyPDF2\n",
        "!pip install fitz\n",
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "import difflib\n",
        "import pickle\n",
        "\n",
        "import fitz\n",
        "import pdftotext\n",
        "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from keyword_extractor import *\n",
        "from concurrent import futures"
      ],
      "metadata": {
        "id": "gKbd7WsfyYX3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p pdf-files\n",
        "!mkdir -p txt-files\n",
        "!mkdir -p ocr-pdf-files"
      ],
      "metadata": {
        "id": "WyuzUOer0FFK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define directory path after creating it\n",
        "pdf_files_path = \"pdf-files\"\n",
        "txt_files_path = \"txt-files\"\n",
        "ocr_pdf_files_path = \"ocr-pdf-files\"\n",
        "\n",
        "MAX_WORKERS = 20"
      ],
      "metadata": {
        "id": "6RzBfwkuyfv6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_pdf(pdf_path):\n",
        "  pdf_in_file = open(pdf_path, \"rb\")\n",
        "  pdf = PdfFileReader(pdf_in_file)\n",
        "  pdf_list = []\n",
        "  for page in range(pdf.numPages):\n",
        "      inputpdf = PdfFileReader(pdf_in_file)\n",
        "      output = PdfFileWriter()\n",
        "      output.addPage(inputpdf.getPage(page))\n",
        "      with open(f\"{pdf_files_path}/page-{page}.pdf\", \"wb\") as outputStream:\n",
        "          output.write(outputStream)\n",
        "          pdf_list.append(f\"page-{page}.pdf\")\n",
        "  return pdf_list\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_list):\n",
        "    txt_file_list = []\n",
        "    i = 0\n",
        "    for pdf_file in pdf_list:\n",
        "        with open(os.path.join(pdf_files_path, pdf_file), \"rb\") as f:\n",
        "            pdf = pdftotext.PDF(f)\n",
        "\n",
        "        # Read all the text into one string\n",
        "        pdf_text = \"\\n\\n\".join(pdf)\n",
        "\n",
        "        # write text into file\n",
        "        with open(f\"{txt_files_path}/page-{str(i)}.txt\", \"a\") as f:\n",
        "            f.write(pdf_text)\n",
        "        txt_file_list.append(f\"{txt_files_path}/page-{str(i)}.txt\")\n",
        "        i += 1\n",
        "    return txt_file_list\n",
        "\n",
        "\n",
        "def get_opt_pattern(icd_10_code):\n",
        "  # create alternate pattern\n",
        "  code_arr = icd_10_code.split(\".\")\n",
        "  if len(code_arr) > 1:\n",
        "    code1 = f\"{code_arr[0]}. {code_arr[1]}\"\n",
        "    code2 = f\"{code_arr[0]} .{code_arr[1]}\"\n",
        "    code3 = f\"{code_arr[0]} . {code_arr[1]}\"\n",
        "    return [code1, code2, code3]\n",
        "  else:\n",
        "    return icd_10_code\n",
        "\n",
        "\n",
        "def isExactMatch(page, term, clip, fullMatch=False, caseSensitive=False):\n",
        "  # clip is an item from page.search_for(term, quads=True)\n",
        "  termLen = len(term)\n",
        "  termBboxLen = max(clip.height, clip.width)\n",
        "  termfontSize = termBboxLen/termLen\n",
        "  f = termfontSize*2\n",
        "\n",
        "  #clip = clip.rect\n",
        "\n",
        "  validate = page.get_text(\"blocks\", clip = clip + (-f, -f, f, f), flags=0)[0][4]\n",
        "  flag = 0\n",
        "  if not caseSensitive:\n",
        "      flag = re.IGNORECASE\n",
        "\n",
        "  matches = len(re.findall(f'{term}', validate, flags=flag)) > 0\n",
        "  if fullMatch:\n",
        "      matches = len(re.findall(f'\\\\b{term}\\\\b', validate))>0\n",
        "  return matches\n",
        "\n",
        "\n",
        "def highlight_icd_code_and_keyword(icd10_code_dict, icd9_code_dict, page_keyword_dict=None, pdf_file_name=None, cords_file_name=None):\n",
        "  pdf_file = fitz.open(pdf_file_name)\n",
        "\n",
        "  def highlight_pdf(highlight, icd10_code, code_type):\n",
        "    cords_list = []\n",
        "    for inst in highlight:\n",
        "      highlight = page.add_highlight_annot(inst)\n",
        "      if code_type == \"ICD-9\":\n",
        "        highlight.set_colors(stroke=[1, 0.5, 0.8]) # light red color (r, g, b)\n",
        "      highlight.update()\n",
        "      highlight = page.search_for(icd10_code)\n",
        "      cords_list.append(highlight)\n",
        "    code_cors_output = f\"Page-{page_num}: {icd10_code} : {cords_list}\"\n",
        "    txt_output_file_name.write(\"%s\\n\" % code_cors_output)\n",
        "\n",
        "  # create file to write cordinate \n",
        "  txt_output_file_name = open(cords_file_name, \"a\")\n",
        "\n",
        "  for page_num, page in enumerate(pdf_file):\n",
        "\n",
        "    # highlight ICD-10 code\n",
        "    if page_num in icd10_code_dict:\n",
        "      for code in icd10_code_dict[page_num]:\n",
        "        highlight = page.search_for(code)\n",
        "        if len(highlight) == 0:\n",
        "          alternate_code_list = get_opt_pattern(code)\n",
        "          for alt_code in alternate_code_list:\n",
        "            highlight = page.search_for(alt_code)\n",
        "            # highlight pdf for option pattern\n",
        "            highlight_pdf(highlight, alt_code, code_type=\"ICD-10\")\n",
        "        # highlight pdf for main pattern   \n",
        "        highlight_pdf(highlight, code, code_type=\"ICD-10\")\n",
        "\n",
        "    # highlight ICD-9 code\n",
        "    if page_num in icd9_code_dict:\n",
        "      for code in icd9_code_dict[page_num]:\n",
        "        highlight = page.search_for(code)\n",
        "        if len(highlight) == 0:\n",
        "          alternate_code_list = get_opt_pattern(code)\n",
        "          for alt_code in alternate_code_list:\n",
        "            highlight = page.search_for(alt_code)\n",
        "            # highlight pdf for option pattern\n",
        "            highlight_pdf(highlight, alt_code, code_type=\"ICD-9\")\n",
        "        # highlight pdf for main pattern   \n",
        "        highlight_pdf(highlight, code, code_type=\"ICD-9\")\n",
        "\n",
        "    # highlight ICD-10 keyword\n",
        "    if page_keyword_dict is not None:\n",
        "      if page_num in page_keyword_dict:\n",
        "        for keyword in page_keyword_dict[page_num]:\n",
        "          coordinates = page.search_for(keyword)\n",
        "          #print(f\"Keyword: {keyword}, Length: {len(coordinates)}\")\n",
        "          cords_list = []\n",
        "          for inst in coordinates:\n",
        "            #print(f\"Keyword: {keyword}, inst: {inst}\")\n",
        "            #if isExactMatch(page, keyword, inst, fullMatch=True, caseSensitive=False):\n",
        "            highlight = page.add_highlight_annot(inst)\n",
        "            highlight.set_colors(stroke=[1, 0.8, 0.8])\n",
        "            highlight.update()\n",
        "            highlight = page.search_for(keyword)\n",
        "            cords_list.append(highlight)\n",
        "          keyword_cors_output = f\"Page-{page_num}: {keyword} : {cords_list}\"\n",
        "          txt_output_file_name.write(\"%s\\n\" % keyword_cors_output)\n",
        "          #print(f\"Page-{page_num}: \", highlight, end='\\n')\n",
        "\n",
        "  txt_output_file_name.close()\n",
        "\n",
        "  pdf_output_file_name = f\"{pdf_file_name.split('.')[0]}_output.pdf\"\n",
        "  pdf_file.save(pdf_output_file_name, garbage=4, deflate=True, clean=True)\n",
        "\n",
        "  return pdf_output_file_name, cords_file_name\n",
        "\n",
        "\n",
        "def filter_unwanted_code(code_list, page_text):\n",
        "    filtered_code_list = []\n",
        "    # if re.search(\"ICD\", page_text):\n",
        "    # match_list = re.findall(\"(ICD-[0-9][a-zA-z]*\\-.+)[ ]\", page_text)\n",
        "    match_list = re.findall(\"(IC[(A-z)]-[0-9][a-zA-z]*\\-.+)[ ]\", page_text)\n",
        "    # print(\"Match list:\\n\", match_list)\n",
        "    for found_code in match_list:\n",
        "        for code in code_list:\n",
        "            if code in found_code:\n",
        "                filtered_code_list.append(code)\n",
        "    return filtered_code_list\n",
        "\n",
        "\n",
        "def search_icd_code(txt_list, nlp, code_type):\n",
        "    pdf_page_vocab = {}\n",
        "    for txt_file in txt_list:\n",
        "        with open(txt_file, \"r\") as f:\n",
        "            page_txt = f.read()\n",
        "            # filter the page that have line number instead of code\n",
        "            if not re.search(\"(P[ ][0-9]+)(,\\s)(L[0-9]+)\", page_txt):\n",
        "                doc = nlp(page_txt)\n",
        "                code_list = [ent.text for ent in doc.ents]\n",
        "                page_number = 0\n",
        "                if len(code_list) != 0:\n",
        "                    page_number = int(txt_file.split(\"/\")[1].split(\".\")[0].split(\"-\")[1])\n",
        "                    pdf_page_vocab[page_number] = code_list\n",
        "                    # print(f\"Page[{txt_file.split('/')[1]}]: {code_list}\")\n",
        "\n",
        "                # filter the page that dont have ICD string into it\n",
        "                if code_type == \"ICD-9\":\n",
        "                    filtered_code_list = filter_unwanted_code(code_list, page_txt)\n",
        "                    pdf_page_vocab[page_number] = filtered_code_list\n",
        "                    # print(f\"Page[{txt_file.split('/')[1]}]: {filtered_code_list}\")\n",
        "\n",
        "    return pdf_page_vocab\n",
        "\n",
        "\n",
        "def get_json_array_list(text_path):\n",
        "  print(f\"Running '{text_path}'\")\n",
        "  json_arr = call(text_path)\n",
        "  print(f\"Got json for '{text_path}'\")\n",
        "  return json_arr\n",
        "\n",
        "\n",
        "def get_wrong_keyword_dict_with_thread(text_path_list):\n",
        "  wrong_keyword_dict = {}\n",
        "\n",
        "  # take care so that unnecessary thread should not be created\n",
        "  workers = min(MAX_WORKERS, len(text_path_list))\n",
        "  with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
        "    json_arr_list = executor.map(get_json_array_list, sorted(text_path_list))\n",
        "\n",
        "  for idx, json_arr in enumerate(json_arr_list):\n",
        "    wrong_keyword_list = [list(element.values())[0] for element in json_arr]\n",
        "    if wrong_keyword_list: \n",
        "      wrong_keyword_dict[idx] = set(wrong_keyword_list)\n",
        "  return dict(sorted(wrong_keyword_dict.items(), key=lambda item: item[0]))\n",
        "\n",
        "def get_wrong_keyword_dict_with_process(text_path_list):\n",
        "  wrong_keyword_dict = {}\n",
        "\n",
        "  # take care so that unnecessary thread should not be created\n",
        "  workers = min(MAX_WORKERS, len(text_path_list))\n",
        "  with futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
        "    json_arr_list = executor.map(get_json_array_list, sorted(text_path_list))\n",
        "\n",
        "  for idx, json_arr in enumerate(json_arr_list):\n",
        "    wrong_keyword_list = [list(element.values())[0] for element in json_arr]\n",
        "    if wrong_keyword_list: \n",
        "      wrong_keyword_dict[idx] = set(wrong_keyword_list)\n",
        "  return dict(sorted(wrong_keyword_dict.items(), key=lambda item: item[0]))\n",
        "\n",
        "\n",
        "def get_wrong_keyword_dict(text_path_list):\n",
        "  wrong_keyword_dict = {}\n",
        "  for idx, file_path in enumerate(text_path_list):\n",
        "    print(idx, file_path)\n",
        "    json_arr = get_json_array_list(file_path)\n",
        "    wrong_keyword_list = [list(element.values())[0] for element in json_arr]\n",
        "    if wrong_keyword_list:\n",
        "      wrong_keyword_dict[idx] = set(wrong_keyword_list)\n",
        "  return wrong_keyword_dict\n",
        "\n",
        "\n",
        "def purge(file_path):\n",
        "  for f in glob.glob(file_path):\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "qB-XpoweygN4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-0: Load prerequisite instance\n",
        "# create nlp instance\n",
        "nlp_keyword = spacy.load('en_core_web_sm')\n",
        "\n",
        "# loading and updating patterns for ICD-10 code\n",
        "nlp_code10 = English()\n",
        "nlp_code10.add_pipe(\"entity_ruler\").from_disk(\"./icd10_code_patterns-v3.jsonl\")\n",
        "\n",
        "# loading and updating patterns for ICD-9 code\n",
        "nlp_code9 = English()\n",
        "nlp_code9.add_pipe(\"entity_ruler\").from_disk(\"./icd9_code_patterns-v1.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zdqev1bymdq",
        "outputId": "abcf65a4-67b8-4728-f5d7-8c2f5635c86f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.entityruler.EntityRuler at 0x7f80a495adc0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for pdf_file in os.listdir(ocr_pdf_files_path):\n",
        "  pdf_file_name = f\"{ocr_pdf_files_path}/{pdf_file}\"\n",
        "  cords_file_name = f\"{pdf_file_name.split('.')[0]}_cords.txt\"\n",
        "\n",
        "  # Step-1: splitting pdf file\n",
        "  pdf_list = split_pdf(pdf_file_name)\n",
        "\n",
        "  # Step-2: Extracting text from pdf\n",
        "  txt_list = extract_text_from_pdf(pdf_list)\n",
        "\n",
        "  # Step-3: Searching ICD-10 code\n",
        "  icd10_code_dict = search_icd_code(txt_list, nlp_code10, code_type=\"ICD-10\")\n",
        "\n",
        "  # Step-4: Searching ICD-9 code\n",
        "  page_code9_dict = search_icd_code(txt_list, nlp_code9, code_type=\"ICD-9\")\n",
        "\n",
        "  # Step-5: Searching ICD-10 keyword\n",
        "  wrong_keyword_dict = get_wrong_keyword_dict(txt_list)\n",
        "  # wrong_keyword_dict = get_wrong_keyword_dict_with_thread(txt_list)\n",
        "  # wrong_keyword_dict = get_wrong_keyword_dict_with_process(txt_list)\n",
        "  # print(\"After sorting:\\n\", wrong_keyword_dict)\n",
        "\n",
        "  # Step-6: Highlighting ICD-10 code and keyword into pdf\n",
        "  pdf_output_file, txt_output_file = highlight_icd_code_and_keyword(icd10_code_dict, page_code9_dict, wrong_keyword_dict,\n",
        "                                                                    pdf_file_name, cords_file_name)\n",
        "  print(f\"File[{pdf_output_file}] is saved after highlighting ICD-10 code and keyword\")\n",
        "  print(f\"Highlighted coordinates are saved into [{txt_output_file}] file.\")\n",
        "\n",
        "  # remove all pdf and text files\n",
        "  purge(\"pdf-files/*.pdf\")\n",
        "  purge(\"txt-files/*.txt\")\n",
        "  pdf_list = []\n",
        "  txt_list = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA7AygDDy06C",
        "outputId": "6a6b86d8-ffa2-479d-ad28-70fa89cdee0e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 txt-files/page-0.txt\n",
            "Running 'txt-files/page-0.txt'\n",
            "Got json for 'txt-files/page-0.txt'\n",
            "1 txt-files/page-1.txt\n",
            "Running 'txt-files/page-1.txt'\n",
            "Got json for 'txt-files/page-1.txt'\n",
            "2 txt-files/page-2.txt\n",
            "Running 'txt-files/page-2.txt'\n",
            "Got json for 'txt-files/page-2.txt'\n",
            "3 txt-files/page-3.txt\n",
            "Running 'txt-files/page-3.txt'\n",
            "Got json for 'txt-files/page-3.txt'\n",
            "4 txt-files/page-4.txt\n",
            "Running 'txt-files/page-4.txt'\n",
            "Got json for 'txt-files/page-4.txt'\n",
            "File[ocr-pdf-files/9929_final_output.pdf] is saved after highlighting ICD-10 code and keyword\n",
            "Highlighted coordinates are saved into [ocr-pdf-files/9929_final_cords.txt] file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ocr-pdf-files\n",
        "!mkdir -p ocr-pdf-files"
      ],
      "metadata": {
        "id": "kbTuptfC1xrl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "purge(\"ocr-pdf-files/*.txt\")\n",
        "purge(\"ocr-pdf-files/*_output.pdf\")\n",
        "purge(\"pdf-files/*.pdf\")\n",
        "purge(\"txt-files/*.txt\")"
      ],
      "metadata": {
        "id": "l06JNkJQ1yGT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip output.zip ocr-pdf-files/*_output_cords.txt ocr-pdf-files/*_output_output.pdf"
      ],
      "metadata": {
        "id": "rZI3lkYT1zxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}