{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-using-pre-trained-word2vec-model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP52dWcwKRKvvEav4kRLv5q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/nlp-research-and-practice/blob/main/practical-natural-language-processing/3-text-representation/5_using_pre_trained_word2vec_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqYICccZYbNE"
      },
      "source": [
        "## Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kehug1ONYcIG"
      },
      "source": [
        "What does it mean when we say a text representation should capture “distributional similarities between words”?\n",
        "\n",
        "Let’s consider some examples. If we’re given the word “USA,” distributionally similar words could be other countries (e.g., Canada, Germany, India, etc.) or cities in the USA. If we’re given the word “beautiful,” words that share some relationship with this word (e.g., synonyms, antonyms) could be considered distributionally similar words. These are words that are likely to occur in similar contexts.\n",
        "\n",
        "In 2013, a seminal work by Mikolov et al. showed that their neural network–based word representation model known as “Word2vec,” based on “distributional similarity,” can capture word analogy relationships such as:\n",
        "\n",
        "`King – Man + Woman ≈ Queen`\n",
        "\n",
        "While learning such semantically rich relationships, Word2vec ensures that the learned word representations are low dimensional (vectors of dimensions 50–500, instead of several thousands) and dense (that is, most values in these vectors are non-zero).\n",
        "\n",
        "Such representations make ML tasks more tractable and efficient. Word2vec led to a lot of work (both pure and applied) in the direction of learning text representations using neural networks. These representations are also called “embeddings.”\n",
        "\n",
        "To “derive” the meaning of the word, Word2vec uses distributional similarity and distributional hypothesis. That is, it derives the meaning of a word from its context: words that appear in its neighborhood in the text. So, if two different words (often) occur in similar context, then it’s highly likely that their meanings are also similar.\n",
        "\n",
        "Word2vec operationalizes this by projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and words with very different meanings are far from one another.\n",
        "\n",
        "Conceptually, Word2vec takes a large corpus of text as input and “learns” to represent the words in a common vector space based on the contexts in which they appear in the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOKzKwpgaDCm"
      },
      "source": [
        "## Pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWZ7ymMXaD6Q"
      },
      "source": [
        "Some of the most popular pre-trained embeddings are Word2vec by Google, GloVe by Stanford, and fasttext embeddings by Facebook, to name a few. Further, they’re available for various dimensions like d = 25, 50, 100, 200, 300, 600.\n",
        "\n",
        "Let us take an example of a pre-trained word2vec model, and how we can use it to look for most similar words. We will use the Google News vectors embeddings. https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI5CJSgSal59"
      },
      "source": [
        "#This module ignores the various types of warnings generated\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#This module provides a way of using operating system dependent functionality\n",
        "import os\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "#This module helps in retrieving information on running processes and system resource utilization\n",
        "import psutil\n",
        "process = psutil.Process(os.getpid())\n",
        "from psutil import virtual_memory\n",
        "mem = virtual_memory()\n",
        "\n",
        "#This module is used to calculate the time\n",
        "import time"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "QFqttpXq46hW",
        "outputId": "81847417-356a-4b73-c65e-8862d442bdeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# content/gdrive/My Drive/Kaggle is the path where kaggle.json is  present in the Google Drive\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/kaggle-keys\""
      ],
      "metadata": {
        "id": "csfkpbot47B8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "# download dataset from kaggle> URL: https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\n",
        "kaggle datasets download -d leadbest/googlenewsvectorsnegative300\n",
        "\n",
        "unzip -qq googlenewsvectorsnegative300.zip\n",
        "rm -rf googlenewsvectorsnegative300.zip"
      ],
      "metadata": {
        "id": "BReyxsVN49uv",
        "outputId": "be56f688-19c0-41b6-c3a4-35b9df7af9e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading googlenewsvectorsnegative300.zip to /content\n",
            "100% 3.17G/3.17G [00:38<00:00, 106MB/s] \n",
            "100% 3.17G/3.17G [00:38<00:00, 88.8MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6LhoxT0a9iS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dfa29cc-cb83-4296-efa2-bae713e7f76d"
      },
      "source": [
        "pretrained_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "\n",
        "#Load W2V model. This will take some time, but it is a one time effort!\n",
        "pre = process.memory_info().rss\n",
        "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
        "print('-'*10)\n",
        "\n",
        "start_time = time.time() #Start the timer\n",
        "ttl = mem.total #Toal memory available\n",
        "\n",
        "#load the model\n",
        "w2v_model = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)\n",
        "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
        "print('-'*10)\n",
        "\n",
        "print('Finished loading Word2Vec')\n",
        "print('-'*10)\n",
        "\n",
        "post = process.memory_info().rss\n",
        "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
        "print('-'*10)\n",
        "\n",
        "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
        "print('-'*10)\n",
        "\n",
        "print(\"Numver of words in vocablulary: \",len(w2v_model.key_to_index)) #Number of words in the vocabulary."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory used in GB before Loading the Model: 4.30\n",
            "----------\n",
            "65.45 seconds taken to load\n",
            "----------\n",
            "Finished loading Word2Vec\n",
            "----------\n",
            "Memory used in GB after Loading the Model: 8.46\n",
            "----------\n",
            "Percentage increase in memory usage: 196.95% \n",
            "----------\n",
            "Numver of words in vocablulary:  3000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-mmhDeLbglI"
      },
      "source": [
        "Here, we find the words that are semantically most similar to the word “beautiful”; the last line returns the embedding vector of the word “beautiful”.\n",
        "\n",
        "Let us examine the model by knowing what the most similar words are, for a given word!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIEzJufYbYuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e5ac1b-194b-4ae9-975b-f08adc97d852"
      },
      "source": [
        "w2v_model.most_similar('beautiful')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gorgeous', 0.8353005051612854),\n",
              " ('lovely', 0.8106936812400818),\n",
              " ('stunningly_beautiful', 0.7329413294792175),\n",
              " ('breathtakingly_beautiful', 0.7231340408325195),\n",
              " ('wonderful', 0.6854086518287659),\n",
              " ('fabulous', 0.6700063943862915),\n",
              " ('loveliest', 0.6612576246261597),\n",
              " ('prettiest', 0.6595001816749573),\n",
              " ('beatiful', 0.6593326330184937),\n",
              " ('magnificent', 0.6591402888298035)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "actJE1Tlb8BV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85cb1f1-e4c7-4029-a6b2-559f2705f461"
      },
      "source": [
        "# Let us try with another word!\n",
        "w2v_model.most_similar('toronto')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('montreal', 0.6984112858772278),\n",
              " ('vancouver', 0.6587257385253906),\n",
              " ('nyc', 0.6248832941055298),\n",
              " ('alberta', 0.6179691553115845),\n",
              " ('boston', 0.611499547958374),\n",
              " ('calgary', 0.61032634973526),\n",
              " ('edmonton', 0.6100260615348816),\n",
              " ('canadian', 0.5944076776504517),\n",
              " ('chicago', 0.5911980271339417),\n",
              " ('springfield', 0.5888351798057556)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKNwhcoRcGqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a00e142-654d-4b41-926e-dba1816e2506"
      },
      "source": [
        "# What is the vector representation for a word?\n",
        "w2v_model['beautiful']"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.01831055,  0.05566406, -0.01153564,  0.07275391,  0.15136719,\n",
              "       -0.06176758,  0.20605469, -0.15332031, -0.05908203,  0.22851562,\n",
              "       -0.06445312, -0.22851562, -0.09472656, -0.03344727,  0.24707031,\n",
              "        0.05541992, -0.00921631,  0.1328125 , -0.15429688,  0.08105469,\n",
              "       -0.07373047,  0.24316406,  0.12353516, -0.09277344,  0.08203125,\n",
              "        0.06494141,  0.15722656,  0.11279297, -0.0612793 , -0.296875  ,\n",
              "       -0.13378906,  0.234375  ,  0.09765625,  0.17773438,  0.06689453,\n",
              "       -0.27539062,  0.06445312, -0.13867188, -0.08886719,  0.171875  ,\n",
              "        0.07861328, -0.10058594,  0.23925781,  0.03808594,  0.18652344,\n",
              "       -0.11279297,  0.22558594,  0.10986328, -0.11865234,  0.02026367,\n",
              "        0.11376953,  0.09570312,  0.29492188,  0.08251953, -0.05444336,\n",
              "       -0.0090332 , -0.0625    , -0.17578125, -0.08154297,  0.01062012,\n",
              "       -0.04736328, -0.08544922, -0.19042969, -0.30273438,  0.07617188,\n",
              "        0.125     , -0.05932617,  0.03833008, -0.03564453,  0.2421875 ,\n",
              "        0.36132812,  0.04760742,  0.00631714, -0.03088379, -0.13964844,\n",
              "        0.22558594, -0.06298828, -0.02636719,  0.1171875 ,  0.33398438,\n",
              "       -0.07666016, -0.06689453,  0.04150391, -0.15136719, -0.22460938,\n",
              "        0.03320312, -0.15332031,  0.07128906,  0.16992188,  0.11572266,\n",
              "       -0.13085938,  0.12451172, -0.20410156,  0.04736328, -0.296875  ,\n",
              "       -0.17480469,  0.00872803, -0.04638672,  0.10791016, -0.203125  ,\n",
              "       -0.27539062,  0.2734375 ,  0.02563477, -0.11035156,  0.0625    ,\n",
              "        0.1953125 ,  0.16015625, -0.13769531, -0.09863281, -0.1953125 ,\n",
              "       -0.22851562,  0.25390625,  0.00915527, -0.03857422,  0.3984375 ,\n",
              "       -0.1796875 ,  0.03833008, -0.24804688,  0.03515625,  0.03881836,\n",
              "        0.03442383, -0.04101562,  0.20214844, -0.03015137, -0.09619141,\n",
              "        0.11669922, -0.06738281,  0.0625    ,  0.10742188,  0.25585938,\n",
              "       -0.21777344,  0.05639648, -0.0065918 ,  0.16113281,  0.11865234,\n",
              "       -0.03088379, -0.11572266,  0.02685547,  0.03100586,  0.09863281,\n",
              "        0.05883789,  0.00634766,  0.11914062,  0.07324219, -0.01586914,\n",
              "        0.18457031,  0.05322266,  0.19824219, -0.22363281, -0.25195312,\n",
              "        0.15039062,  0.22753906,  0.05737305,  0.16992188, -0.22558594,\n",
              "        0.06494141,  0.11914062, -0.06640625, -0.10449219, -0.07226562,\n",
              "       -0.16992188,  0.0625    ,  0.14648438,  0.27148438, -0.02172852,\n",
              "       -0.12695312,  0.18457031, -0.27539062, -0.36523438, -0.03491211,\n",
              "       -0.18554688,  0.23828125, -0.13867188,  0.00296021,  0.04272461,\n",
              "        0.13867188,  0.12207031,  0.05957031, -0.22167969, -0.18945312,\n",
              "       -0.23242188, -0.28710938, -0.00866699, -0.16113281, -0.24316406,\n",
              "        0.05712891, -0.06982422,  0.00053406, -0.10302734, -0.13378906,\n",
              "       -0.16113281,  0.11621094,  0.31640625, -0.02697754, -0.01574707,\n",
              "        0.11425781, -0.04174805,  0.05908203,  0.02661133, -0.08642578,\n",
              "        0.140625  ,  0.09228516, -0.25195312, -0.31445312, -0.05688477,\n",
              "        0.01031494,  0.0234375 , -0.02331543, -0.08056641,  0.01269531,\n",
              "       -0.34179688,  0.17285156, -0.16015625,  0.07763672, -0.03088379,\n",
              "        0.11962891,  0.11767578,  0.20117188, -0.01940918,  0.02172852,\n",
              "        0.23046875,  0.28125   , -0.17675781,  0.02978516,  0.08740234,\n",
              "       -0.06176758,  0.00939941, -0.09277344, -0.203125  ,  0.13085938,\n",
              "       -0.13671875, -0.00500488, -0.04296875,  0.12988281,  0.3515625 ,\n",
              "        0.0402832 , -0.12988281, -0.03173828,  0.28515625,  0.18261719,\n",
              "        0.13867188, -0.16503906, -0.26171875, -0.04345703,  0.0100708 ,\n",
              "        0.08740234,  0.00421143, -0.1328125 , -0.17578125, -0.04321289,\n",
              "       -0.015625  ,  0.16894531,  0.25      ,  0.37109375,  0.19921875,\n",
              "       -0.36132812, -0.10302734, -0.20800781, -0.20117188, -0.01519775,\n",
              "       -0.12207031, -0.12011719, -0.07421875, -0.04345703,  0.14160156,\n",
              "        0.15527344, -0.03027344, -0.09326172, -0.04589844,  0.16796875,\n",
              "       -0.03027344,  0.09179688, -0.10058594,  0.20703125,  0.11376953,\n",
              "       -0.12402344,  0.04003906,  0.06933594, -0.34570312,  0.03881836,\n",
              "        0.16210938,  0.05761719, -0.12792969, -0.05810547,  0.03857422,\n",
              "       -0.11328125, -0.1953125 , -0.28125   , -0.13183594,  0.15722656,\n",
              "       -0.09765625,  0.09619141, -0.09960938, -0.00285339, -0.03637695,\n",
              "        0.15429688,  0.06152344, -0.34570312,  0.11083984,  0.03344727],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IyQq9rEcmU3"
      },
      "source": [
        "Note that if we search for a word that is not present in the Word2vec model, we’ll see a “key not found” error.\n",
        "\n",
        "Hence, as a good coding practice, it’s always advised to first check if the word is present in the model’s vocabulary before attempting to retrieve its vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbx4tZoVcWe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "9f43261f-b0c3-44ac-bef3-fbc040e087c9"
      },
      "source": [
        "# What if I am looking for a word that is not in this vocabulary?\n",
        "w2v_model.most_similar('practicalnlp')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-80d6cf5425de>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# What if I am looking for a word that is not in this vocabulary?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'practicalnlp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'practicalnlp' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNckvoD4c0FJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "ce45ac69-3f4d-4a52-9d64-05088c7529c6"
      },
      "source": [
        "w2v_model['practicalnlp']"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-02f42fa69a36>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'practicalnlp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'practicalnlp' not present\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P-lNMxPc6Ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "1b16f50d-7b70-4f56-bde9-db95047028ad"
      },
      "source": [
        "w2v_model.most_similar('Ryaan')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-1b21fcb6c00c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ryaan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'Ryaan' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJBColx0dHbZ"
      },
      "source": [
        "Two things to note while using pre-trained models:\n",
        "\n",
        "1. Tokens/Words are always lowercased. If a word is not in the vocabulary, the model throws an exception.\n",
        "2. So, it is always a good idea to encapsulate those statements in try/except blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ua3VgPEdhX1"
      },
      "source": [
        "## Getting the embedding representation for full text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSoxkFXtdijy"
      },
      "source": [
        "We have seen how to get embedding vectors for single words. How do we use them to get such a representation for a full text? A simple way is to just sum or average the embeddings for individual words.\n",
        "\n",
        "Let us see a small example using another NLP library Spacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l5fKXjKegud"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2eLoV9Wc-pN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00df87fa-da29-4d27-8b4c-d089e2540b86"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spacy model that we already installed in Chapter 2. This takes a few seconds.\n",
        "%time nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 676 ms, sys: 57.6 ms, total: 733 ms\n",
            "Wall time: 861 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ1tXmE7drv0"
      },
      "source": [
        "# process a sentence using the model\n",
        "mydoc = nlp(\"Canada is a large country\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24x4JOeyfvcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff311b3-7091-4690-8def-4735e0264c62"
      },
      "source": [
        "# Get a vector for individual words\n",
        "print(mydoc[0].vector)  # vector for 'Canada', the first word in the text"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.74234    -0.90920454  0.41536316  0.15736246  1.2859436   0.24543142\n",
            "  1.2570572   0.35663185 -0.8244102  -0.0674134   1.4712349   0.5119143\n",
            " -1.3309681  -0.5264146  -1.0188745  -0.8524463   1.2472408   0.2747297\n",
            " -0.0436547  -0.4842371  -1.2904495   0.42295414 -0.03794765 -0.22511679\n",
            " -0.4816206   0.36949652  1.2843533   1.4024066  -0.6087295   0.7147388\n",
            " -0.14381114 -0.9796721   0.452798    0.7162336  -0.5708136  -0.08537036\n",
            " -0.63481605  0.9896861  -0.474687    3.4676626  -0.9343261   0.29444414\n",
            " -0.02503309  1.285727   -1.7670362   0.39907005 -0.03138383  2.235859\n",
            "  1.233593   -0.06988642 -0.48538476  1.0872145  -0.8912538  -1.4635974\n",
            " -0.76645774 -0.4039675   0.86213416 -0.55711997  0.77631915 -0.13158414\n",
            " -0.3540035  -0.22625872  0.38927513 -0.54100454  0.40940216 -0.5324899\n",
            " -0.55475163 -0.6075223   0.3275603  -1.6374564   0.7500537  -0.6747781\n",
            "  1.2150496  -0.35457557 -0.85388327 -0.69132215 -0.6772988  -1.405904\n",
            " -0.5053379  -0.21676248 -0.219181    0.7379973  -0.24607135 -0.960969\n",
            "  0.54404056  0.05432597  0.36030546 -0.17227349 -0.02405006  0.44561887\n",
            " -0.39934576 -0.57765657  2.6596062  -0.5772871  -0.41377008  0.89937127]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC66XBu9f8QW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb27e534-389f-4d11-d93d-bbff42e37e5c"
      },
      "source": [
        "# Averaged vector for the entire sentence\n",
        "print(mydoc.vector)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.44864374e-01 -1.56845257e-01 -5.19747622e-02  5.86494267e-01\n",
            "  8.10811967e-02 -1.65754989e-01  7.57052720e-01  2.63185889e-01\n",
            "  1.40734492e-02  2.51211464e-01  2.43307427e-01 -2.79111534e-01\n",
            " -3.70179832e-01  5.22314429e-01 -5.23915410e-01  4.84695425e-03\n",
            "  4.30857569e-01 -2.19760254e-01 -3.72532457e-01  1.71566337e-01\n",
            " -2.67529279e-01  2.24802848e-02 -3.03287357e-01 -1.04288436e-01\n",
            "  1.51315406e-01 -5.31261384e-01  4.36048269e-01  2.97305524e-01\n",
            "  4.72418487e-01  3.90211403e-01  2.69951403e-01  2.36672014e-01\n",
            "  4.59462464e-01 -4.97865111e-01 -1.82451054e-01 -1.67997599e-01\n",
            "  1.93978697e-01  5.16766071e-01 -2.88335413e-01  3.74710053e-01\n",
            " -1.11499667e-01  3.33659947e-01  5.49611822e-02  2.53970414e-01\n",
            " -5.02043903e-01  3.85194987e-01 -1.86397389e-01  8.60191345e-01\n",
            "  2.11835742e-01 -1.24764726e-01 -7.09948778e-01  7.70933092e-01\n",
            " -1.79754198e-01 -6.63751960e-01 -4.01271343e-01  1.83464423e-01\n",
            " -2.96254933e-01  7.63848484e-01 -3.35624158e-01 -1.81755573e-01\n",
            "  5.62856086e-02 -5.20981967e-01  2.32470542e-01  7.93596357e-02\n",
            "  1.17028512e-01 -1.91331297e-01  2.83491552e-01  3.67665291e-03\n",
            "  3.98660034e-01 -5.55250108e-01  2.48089619e-02  2.39709094e-01\n",
            "  3.95606980e-02 -3.22076678e-01 -6.04971290e-01 -7.20350385e-01\n",
            " -9.31409597e-02 -8.10149968e-01 -5.13003230e-01 -3.73546213e-01\n",
            " -2.60708272e-01  1.43927217e-01 -2.70993322e-01 -6.63320005e-01\n",
            "  6.42718398e-04  1.37684375e-01  2.20302776e-01  4.54716794e-02\n",
            " -2.09064916e-01  3.36762726e-01 -1.24832727e-01 -4.11891073e-01\n",
            "  1.14090490e+00 -2.19411142e-02 -4.46765989e-01  4.73975614e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT2PckIvgBQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1358327-f7a8-4bf0-ee58-f857478cdcb2"
      },
      "source": [
        "# What happens when I give a sentence with strange words (and stop words), and try to get its word vector in Spacy?\n",
        "temp = nlp(\"practicalnlp is a newword\")\n",
        "temp[0].vector"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.7808644 , -0.13927388,  1.1979539 ,  0.02954794,  0.10888022,\n",
              "       -0.08408853,  1.0671581 ,  1.0224844 , -0.21108732, -0.873439  ,\n",
              "        1.2589307 , -0.03803551, -0.5621327 , -0.68604934, -0.9219377 ,\n",
              "       -0.34106416,  0.41339433, -0.55588746,  0.01959878,  0.9607241 ,\n",
              "       -0.7844026 , -1.2117573 ,  0.10303535, -0.35093412, -1.3106965 ,\n",
              "        0.82981586,  0.53083956,  0.7383765 ,  0.29475784,  0.32589513,\n",
              "        0.12443371, -1.0673028 ,  0.62938726, -0.62013155,  0.33982378,\n",
              "       -0.74396217, -0.37885252,  0.27242422, -0.8541051 ,  1.5684996 ,\n",
              "       -1.3312532 ,  0.22952707, -0.1200439 ,  0.77720463, -0.79096997,\n",
              "        0.82063174,  0.4348258 ,  0.50177073,  1.6081035 , -0.23500574,\n",
              "       -0.66881514,  0.604759  , -0.36700648, -0.46789208, -0.05904178,\n",
              "       -0.12296978,  0.5446122 , -0.21588881, -0.57844615, -0.32954615,\n",
              "       -0.19385432,  0.09608626,  0.08408612, -0.18028721,  0.54459274,\n",
              "        0.20719814,  0.07226813, -0.14609273,  1.1348839 , -1.1861701 ,\n",
              "        1.2974576 , -0.11792937,  0.5255938 ,  0.16170746,  0.19669515,\n",
              "       -0.6895393 ,  0.4875468 , -1.9661396 , -0.37774968, -0.3332744 ,\n",
              "        0.70457137,  0.07189578, -1.7906668 , -0.7090548 , -0.1808691 ,\n",
              "        0.06832196, -0.73891675, -1.0059226 , -0.1793716 ,  0.2113894 ,\n",
              "        0.05209869, -0.00720589,  0.95191145,  0.97276247,  0.37795842,\n",
              "        0.49267176], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBQlQjqigPH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40100265-a0f2-4a11-dc0b-c25f253e721e"
      },
      "source": [
        "temp = nlp(\"Ryaan is a king\")\n",
        "temp[0].vector"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.49338603e+00, -1.76442647e+00,  6.31928861e-01,  4.76052493e-01,\n",
              "       -2.92037308e-01,  9.35610294e-01,  1.47564816e+00,  4.49107736e-01,\n",
              "       -1.45390689e+00, -9.73136008e-01,  1.00833142e+00,  3.00820947e-01,\n",
              "       -1.19312632e+00, -6.35515332e-01, -1.21202350e+00, -2.44572610e-01,\n",
              "        9.66186762e-01, -4.70576018e-01, -4.17391241e-01,  1.19598836e-01,\n",
              "       -1.52088606e+00,  3.90184253e-01, -2.57461518e-02, -5.72054982e-01,\n",
              "        5.73398411e-01, -2.02877939e-01,  1.43802655e+00,  1.48042428e+00,\n",
              "       -4.09596801e-01,  5.95115066e-01, -2.40366697e-01, -8.03723097e-01,\n",
              "        4.20367390e-01, -3.01447839e-01, -1.63274455e+00,  1.11928654e+00,\n",
              "       -1.14143562e+00,  6.56367779e-01,  2.25021333e-01,  2.86314917e+00,\n",
              "       -1.07065856e+00, -5.20799041e-01, -3.20751011e-01,  1.17073393e+00,\n",
              "       -1.43994677e+00,  3.85589719e-01, -3.21990907e-01,  2.65340900e+00,\n",
              "        1.14150953e+00,  9.87993479e-02, -2.21533269e-01,  1.54868090e+00,\n",
              "       -3.14203709e-01, -1.53358471e+00,  1.33565456e-01, -3.98026615e-01,\n",
              "        8.78153801e-01, -1.32631436e-01,  1.15112686e+00, -6.72016919e-01,\n",
              "       -3.26423407e-01, -2.30339170e-03,  2.09373683e-01, -2.73227692e-01,\n",
              "        3.77981544e-01, -5.97369194e-01, -6.16310716e-01, -7.97889709e-01,\n",
              "        1.47018015e-01, -2.05057526e+00,  4.80070144e-01,  2.84858406e-01,\n",
              "        1.47116542e+00, -8.49619210e-01, -3.39627475e-01, -1.43122345e-01,\n",
              "       -1.17612493e+00, -1.24647129e+00, -5.55922687e-02, -8.98445070e-01,\n",
              "       -4.96095717e-02,  5.17408252e-01, -1.02052964e-01, -7.07940042e-01,\n",
              "        1.25052929e+00,  5.63304007e-01,  5.99078476e-01, -1.50417462e-01,\n",
              "        1.06768377e-01,  6.78698421e-01, -5.00269413e-01,  3.13145667e-02,\n",
              "        2.61479974e+00, -3.16546530e-01, -4.97440964e-01,  1.21720088e+00],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}