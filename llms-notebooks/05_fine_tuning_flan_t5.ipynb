{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOj8gyFHen1GigfvgsYjRRK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/nlp-research-and-practice/blob/main/llms-notebooks/05_fine_tuning_flan_t5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "F4ID_EmZI77b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reference**\n",
        "\n",
        "[Efficient Large Language Model training with LoRA and Hugging Face](https://www.philschmid.de/fine-tune-flan-t5-peft)"
      ],
      "metadata": {
        "id": "ILMTj_R_Ntlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install Hugging Face Libraries\n",
        "!pip install \"peft==0.2.0\"\n",
        "!pip install \"transformers==4.27.2\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n",
        "# install additional dependencies needed for training\n",
        "!pip install rouge-score tensorboard py7zr"
      ],
      "metadata": {
        "id": "iztbX09ZI8yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
        "\n",
        "from random import randrange\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
        "from peft import PeftModel, PeftConfig"
      ],
      "metadata": {
        "id": "H8In3ovoJGPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "azXdeL76JCwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from the hub\n",
        "dataset = load_dataset(\"samsum\")\n",
        "\n",
        "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
        "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
        "# Train dataset size: 14732\n",
        "# Test dataset size: 819"
      ],
      "metadata": {
        "id": "7GQP62HBJDz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "xV-a47b8JSbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"google/flan-t5-xxl\"\n",
        "\n",
        "# Load tokenizer of FLAN-t5-XL\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "8bu2HxLDJUOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum total input sequence length after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
        "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
        "    lambda x: tokenizer(x[\"dialogue\"], truncation=True),\n",
        "    batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
        "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
        "# take 85 percentile of max length for better utilization\n",
        "max_source_length = int(np.percentile(input_lenghts, 85))\n",
        "print(f\"Max source length: {max_source_length}\")\n",
        "\n",
        "# The maximum total sequence length for target text after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
        "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
        "    lambda x: tokenizer(x[\"summary\"], truncation=True),\n",
        "    batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
        "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
        "# take 90 percentile of max length for better utilization\n",
        "max_target_length = int(np.percentile(target_lenghts, 90))\n",
        "print(f\"Max target length: {max_target_length}\")"
      ],
      "metadata": {
        "id": "phSRLOCVJrA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(sample,padding=\"max_length\"):\n",
        "    # add prefix to the input for t5\n",
        "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
        "\n",
        "    # tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Tokenize targets with the `text_target` keyword argument\n",
        "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "    # padding in the loss.\n",
        "    if padding == \"max_length\":\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "3fVKF1WwKJAD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
        "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
        "\n",
        "# save datasets to disk for later easy loading\n",
        "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
        "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
      ],
      "metadata": {
        "id": "o-yuRN-QKMKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-tuning"
      ],
      "metadata": {
        "id": "n9JkKXtVKXci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface hub model id\n",
        "model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
        "\n",
        "# load model from the hub\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "EgZPNegiKY7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LoRA Config\n",
        "lora_config = LoraConfig(\n",
        " r=16,\n",
        " lora_alpha=32,\n",
        " target_modules=[\"q\", \"v\"],\n",
        " lora_dropout=0.05,\n",
        " bias=\"none\",\n",
        " task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "# prepare int-8 model for training\n",
        "model = prepare_model_for_int8_training(model)\n",
        "\n",
        "# add LoRA adaptor\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "# trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817"
      ],
      "metadata": {
        "id": "_w5AJ9PGKmwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we want to ignore tokenizer pad token in the loss\n",
        "label_pad_token_id = -100\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8\n",
        ")"
      ],
      "metadata": {
        "id": "-ucdJI6UK8Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir=\"lora-flan-t5-xxl\"\n",
        "\n",
        "# Define training args\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "\t  auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # higher learning rate\n",
        "    num_train_epochs=5,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=500,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "id": "wQObS_wyLNN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "KN7ya4roLX2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save our LoRA model & tokenizer results\n",
        "peft_model_id=\"results\"\n",
        "trainer.model.save_pretrained(peft_model_id)\n",
        "tokenizer.save_pretrained(peft_model_id)\n",
        "# if you want to save the base model to call\n",
        "# trainer.model.base_model.save_pretrained(peft_model_id)"
      ],
      "metadata": {
        "id": "zfLWMLjvLkEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference"
      ],
      "metadata": {
        "id": "MV1IoJ06Lqo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load peft config for pre-trained checkpoint etc.\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "# load base LLM model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
        "model.eval()\n",
        "\n",
        "print(\"Peft model loaded\")"
      ],
      "metadata": {
        "id": "oY8aegDQLrP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from the hub and get a sample\n",
        "dataset = load_dataset(\"samsum\")\n",
        "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
        "\n",
        "input_ids = tokenizer(sample[\"dialogue\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "# with torch.inference_mode():\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=True, top_p=0.9)\n",
        "print(f\"input sentence: {sample['dialogue']}\\n{'---'* 20}\")\n",
        "\n",
        "print(f\"summary:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"
      ],
      "metadata": {
        "id": "127nY8uwMrO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric\n",
        "metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def evaluate_peft_model(sample,max_target_length=50):\n",
        "    # generate summary\n",
        "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)\n",
        "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
        "    # decode eval sample\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
        "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    return prediction, labels\n",
        "\n",
        "# load test dataset from distk\n",
        "test_dataset = load_from_disk(\"data/eval/\").with_format(\"torch\")\n",
        "\n",
        "# run predictions\n",
        "# this can take ~45 minutes\n",
        "predictions, references = [] , []\n",
        "for sample in tqdm(test_dataset):\n",
        "    p,l = evaluate_peft_model(sample)\n",
        "    predictions.append(p)\n",
        "    references.append(l)\n",
        "\n",
        "# compute metric\n",
        "rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "\n",
        "# print results\n",
        "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
        "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
        "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
        "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n",
        "\n",
        "# Rogue1: 50.386161%\n",
        "# rouge2: 24.842412%\n",
        "# rougeL: 41.370130%\n",
        "# rougeLsum: 41.394230%"
      ],
      "metadata": {
        "id": "BfzAQfpPM1kR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}