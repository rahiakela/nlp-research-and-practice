{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPeDA5/t1kx1pgk4VcqbUQV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/applied-text-analysis-with-python/4_text_vectorization_and_transformation_pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Vectorization and Transformation Pipeline"
      ],
      "metadata": {
        "id": "WivVR4njQ_RN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will demonstrate how to use the vectorization process to combine\n",
        "linguistic techniques from NLTK with machine learning techniques in Scikit-Learn\n",
        "and Gensim, creating custom transformers that can be used inside repeatable and\n",
        "reusable pipelines.\n",
        "\n",
        "In order\n",
        "to perform machine learning on text, we need to transform our documents into vector\n",
        "representations such that we can apply numeric machine learning. This process is\n",
        "called feature extraction or more simply, vectorization, and is an essential first step\n",
        "toward language-aware analysis.\n",
        "\n",
        "Representing documents numerically gives us the ability to perform meaningful ana\n",
        "lytics\n",
        "and also creates the instances on which machine learning algorithms operate\n",
        "\n",
        "For this reason, we must now make a critical shift in how we think about language—\n",
        "from a sequence of words to points that occupy a high-dimensional semantic space.\n",
        "Points in space can be close together or far apart, tightly clustered or evenly distributed.\n",
        "\n",
        "By\n",
        "encoding similarity as distance, we can begin to derive the primary components of\n",
        "documents and draw decision boundaries in our semantic space.\n",
        "\n",
        "The simplest encoding of semantic space is the bag-of-words model, whose primary\n",
        "insight is that meaning and similarity are encoded in vocabulary."
      ],
      "metadata": {
        "id": "4Agr70PyRK5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "sy8djaWNR-63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import Binarizer\n",
        "import gensim"
      ],
      "metadata": {
        "id": "OeCMS-sVSAtv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "J7qtV9dzZ_oQ",
        "outputId": "0e262337-a8ed-4292-bfac-0a6aee656d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "  \"The elephant sneezed at the sight of potatoes.\",\n",
        "  \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
        "  \"Wondering, she opened the door to the studio.\",\n",
        "]"
      ],
      "metadata": {
        "id": "fK6gRo94dcVj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Words in Space"
      ],
      "metadata": {
        "id": "NjcyUc6LSA4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will look at four types of vector encoding—frequency,\n",
        "one-hot, TF–IDF, and distributed representations—and discuss their implementations\n",
        "in Scikit-Learn, Gensim, and NLTK.\n",
        "\n",
        "To set this up, let’s create a list of our documents and tokenize them for the proceeding\n",
        "vectorization examples."
      ],
      "metadata": {
        "id": "0rEPgZmISQbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  stem = nltk.stem.SnowballStemmer(\"english\")\n",
        "  text = text.lower()\n",
        "\n",
        "  for token in nltk.word_tokenize(text):\n",
        "    if token in string.punctuation:\n",
        "      continue\n",
        "    yield stem.stem(token)"
      ],
      "metadata": {
        "id": "rYKuC_oASBsb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Frequency Vectors"
      ],
      "metadata": {
        "id": "wU7iCK11dY0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With NLTK\n",
        "def vectorize(doc):\n",
        "  features = defaultdict(int)\n",
        "  for token in tokenize(doc):\n",
        "    features[token] += 1\n",
        "  return features\n",
        "\n",
        "vectors = map(vectorize, corpus)\n",
        "for vector in vectors:\n",
        "  print(dict(vector))"
      ],
      "metadata": {
        "id": "DS8kDzI3daC6",
        "outputId": "c20279c9-8ebc-4d55-f1a6-37d6d3c3e40d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 2, 'eleph': 1, 'sneez': 1, 'at': 1, 'sight': 1, 'of': 1, 'potato': 1}\n",
            "{'bat': 2, 'can': 1, 'see': 2, 'via': 1, 'echoloc': 1, 'the': 1, 'sight': 1, 'sneez': 1}\n",
            "{'wonder': 1, 'she': 1, 'open': 1, 'the': 2, 'door': 1, 'to': 1, 'studio': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In Scikit-Learn\n",
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(corpus)\n",
        "for vector in vectors:\n",
        "  print(vector)"
      ],
      "metadata": {
        "id": "st_TaTzWsPjn",
        "outputId": "72dd020f-8f40-4ca3-9025-d0237d3d455d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 16)\t2\n",
            "  (0, 6)\t1\n",
            "  (0, 14)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 12)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 16)\t1\n",
            "  (0, 12)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 10)\t2\n",
            "  (0, 18)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 16)\t2\n",
            "  (0, 19)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 15)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Gensim way\n",
        "tokenized_corpus = [list(tokenize(doc)) for doc in corpus]\n",
        "id2word = gensim.corpora.Dictionary(tokenized_corpus)\n",
        "vectors = [id2word.doc2bow(doc) for doc in tokenized_corpus]\n",
        "vectors"
      ],
      "metadata": {
        "id": "KXkZjnz1s-ax",
        "outputId": "b4f621b3-732f-4449-d9f5-dfd572ff88c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2)],\n",
              " [(4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 2), (11, 1)],\n",
              " [(6, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###One-Hot Encoding"
      ],
      "metadata": {
        "id": "2sK1h240ubIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With NLTK\n",
        "def vectorize(doc):\n",
        "  return {token: True for token in tokenize(doc)}\n",
        "\n",
        "vectors = map(vectorize, corpus)\n",
        "\n",
        "for vector in vectors:\n",
        "  print(vector)"
      ],
      "metadata": {
        "id": "E-LRg_OqucaW",
        "outputId": "939e3416-783f-4c1b-f5f8-d59f01759873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': True, 'eleph': True, 'sneez': True, 'at': True, 'sight': True, 'of': True, 'potato': True}\n",
            "{'bat': True, 'can': True, 'see': True, 'via': True, 'echoloc': True, 'the': True, 'sight': True, 'sneez': True}\n",
            "{'wonder': True, 'she': True, 'open': True, 'the': True, 'door': True, 'to': True, 'studio': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In Scikit-Learn\n",
        "freq = CountVectorizer()\n",
        "tokenized_corpus = freq.fit_transform(corpus)\n",
        "\n",
        "print(len(tokenized_corpus.toarray()[0]))\n",
        "\n",
        "one_hot = Binarizer()\n",
        "vectors = one_hot.fit_transform(tokenized_corpus.toarray())\n",
        "for vector in vectors:\n",
        "  print(vector)"
      ],
      "metadata": {
        "id": "WJKgJsbEXWg-",
        "outputId": "7cf44d5d-dce4-4649-cb6b-21264bbe82d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "[1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0]\n",
            "[0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0]\n",
            "[0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In Scikit-Learn alternate way\n",
        "freq = CountVectorizer(binary=True)\n",
        "tokenized_corpus = freq.fit_transform(corpus)\n",
        "\n",
        "print(len(tokenized_corpus.toarray()[0]))\n",
        "\n",
        "for vector in tokenized_corpus.toarray():\n",
        "  print(vector)"
      ],
      "metadata": {
        "id": "VXNHTG93b-wu",
        "outputId": "c71c2279-9430-4f7f-bf1f-94f73f624d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "[1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0]\n",
            "[0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0]\n",
            "[0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Gensim way\n",
        "tokenized_corpus = [list(tokenize(doc)) for doc in corpus]\n",
        "id2word = gensim.corpora.Dictionary(tokenized_corpus)\n",
        "\n",
        "vectors = np.array([[(token[0], 1) for token in id2word.doc2bow(doc)] for doc in tokenized_corpus])\n",
        "\n",
        "for vector in vectors:\n",
        "  print(vector)"
      ],
      "metadata": {
        "id": "rKUU4kN3eEin",
        "outputId": "6cb0daab-9c1a-4f09-abe4-8fdcb2230c18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n",
            "[(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n",
            "[(6, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF-IDF"
      ],
      "metadata": {
        "id": "NmdNZJjxgdse"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4sJZKH7gfiq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}